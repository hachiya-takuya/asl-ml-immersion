{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26453007-e94e-4bfc-b0a0-ae6bfb74620e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4784a9-086f-4d0e-99da-8cb70af3ae09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "import evaluate\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils_preproc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc18d3-5e94-457c-969d-f14eeab168b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c90f16b-ce73-4e34-be74-82eb019517e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "MODEL_PATH = \"translate_models/baseline\"\n",
    "DATA_URL = (\n",
    "    \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    ")\n",
    "LOAD_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83b71e7-5d37-4055-b64b-ac4915fcaf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ab8dde-489f-42a3-b143-edd49c39fad9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2638744/2638744 [==============================] - 0s 0us/step\n",
      "Translation data stored at: /home/jupyter/.keras/datasets/spa-eng/spa.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    \"spa-eng.zip\", origin=DATA_URL, extract=True\n",
    ")\n",
    "\n",
    "path_to_file = os.path.join(os.path.dirname(path_to_zip), \"spa-eng/spa.txt\")\n",
    "print(\"Translation data stored at:\", path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a671d2ec-44f3-4048-9d9b-f15dab3e720e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2EWTmjh48kw",
    "outputId": "4a6bcd0b-6c00-42b3-da90-f8f22b799381",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\n",
      "282386239/282386239 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 4096  # Limits parameters in model\n",
    "MIN_TRAINING_SEQ_LEN = 46\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "data_dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# Load simplebooks-92 train set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/train.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# Load simplebooks-92 validation set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0079fa3e-d328-4e5e-a446-6db91eb5a599",
   "metadata": {
    "id": "cF4Unid048kx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the word piece tokenizer. This will take 5-10 mins...\n",
      "Training is complete!!\n"
     ]
    }
   ],
   "source": [
    "# # Train tokenizer vocabulary\n",
    "# print(\"Training the word piece tokenizer. This will take 5-10 mins...\")\n",
    "# vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "#     raw_train_ds,\n",
    "#     vocabulary_size=VOCAB_SIZE,\n",
    "#     lowercase=True,\n",
    "#     reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    "# )\n",
    "# print(\"Training is complete!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb7bb67-4974-4dba-b72d-ca55eb05eedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for token in vocab:\n",
    "#         f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15fbade6-8729-432f-9eab-75bfc79fca37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"vocab.txt\", encoding=\"utf-8\") as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3e6d39e-b48e-4480-8068-1ef3cff8bd56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089aa7e-d65b-4270-8cba-a1cb94962c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95b076aa-bd60-4622-9a89-1622ef282c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    path_to_file, sep=\"\\t\", header=None, names=[\"english\", \"spanish\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "072f48df-e9eb-47b5-ad07-dc212ee2a1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(path, num_examples):\n",
    "    with open(path_to_file) as fp:\n",
    "        lines = fp.read().strip().split(\"\\n\")\n",
    "\n",
    "    sentence_pairs = [\n",
    "        [utils_preproc.preprocess_sentence(sent) for sent in line.split(\"\\t\")]\n",
    "        for line in lines[:num_examples]\n",
    "    ]\n",
    "\n",
    "    return zip(*sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e4945f5-007a-4b87-b4bf-4143e689e82c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_integerize(path, num_examples=None):\n",
    "    targ_lang, inp_lang = load_and_preprocess(path, num_examples)\n",
    "\n",
    "    # TODO 1b\n",
    "    input_tensor = tokenizer(inp_lang)\n",
    "    target_tensor = tokenizer(targ_lang)\n",
    "\n",
    "    return (\n",
    "        input_tensor,\n",
    "        target_tensor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526194b8-2bd4-4372-8bcd-bd04bc8a9286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_PROP = 0.2\n",
    "NUM_EXAMPLES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a105a4b-96eb-4364-8246-e21c183241f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor, target_tensor = load_and_integerize(path_to_file, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab3ac4dc-c3b6-4257-a0cf-a2b0740b4045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118964, 128)\n",
      "(118964, 128)\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)\n",
    "print(target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7c3f4c-7df1-4e2f-8952-9c5c19910d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "max_length_targ = target_tensor.shape[1]\n",
    "print(max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f05477-bd38-424f-a6ae-d2bc1db4f3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = train_test_split(\n",
    "    input_tensor.numpy(),\n",
    "    target_tensor.numpy(),\n",
    "    test_size=TEST_PROP,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "input_tensor_train = splits[0]\n",
    "input_tensor_val = splits[1]\n",
    "\n",
    "target_tensor_train = splits[2]\n",
    "target_tensor_val = splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cde823ea-dd4b-4985-b247-7d3e22f61ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95171, 95171, 23793, 23793)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    len(input_tensor_train),\n",
    "    len(target_tensor_train),\n",
    "    len(input_tensor_val),\n",
    "    len(target_tensor_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f9e3065-7b34-41e5-852b-6b0c5b526be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(_, decoder_input):\n",
    "\n",
    "    # shift ahead by 1\n",
    "    target = tf.roll(decoder_input, -1, 1)\n",
    "\n",
    "    # replace last column with 0s\n",
    "    zeros = tf.zeros([target.shape[0], 1], dtype=tf.int32)\n",
    "    target = tf.concat((target[:, :-1], zeros), axis=-1)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((decoder_input, target))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1ee460d-7556-4b7b-89b6-c7add09b2da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(target_tensor_train)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6485d0c4-f3c9-4838-9ce3-de62035a5137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    create_dataset(input_tensor_train, target_tensor_train)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .repeat()\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset = create_dataset(input_tensor_val, target_tensor_val).batch(\n",
    "    BATCH_SIZE, drop_remainder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1bc0ea4-09ca-4cb7-9d04-ee57a26a48cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for d in train_dataset:\n",
    "#     print(d[0])\n",
    "#     print(d[1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71678b-2564-4cb3-9842-0975d5e542b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505619ca-2910-4f21-881f-fb04b67e2315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef9c485-81d6-4101-a6ab-6012f0fc4899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de35f9-cef7-497d-8bbd-e8a171b25ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a127cc9d-03b8-4d74-9fc6-288b812110a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# this should output \"Num GPUs Available: 1\" if you have one GPU attached\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56e5028-58e5-4baf-84eb-5908bf712ded",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2EWTmjh48kw",
    "outputId": "4a6bcd0b-6c00-42b3-da90-f8f22b799381",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "# MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "# # Model\n",
    "# EMBED_DIM = 256\n",
    "# FEED_FORWARD_DIM = 256\n",
    "# NUM_HEADS = 3\n",
    "# NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab345b-9184-403c-8fe8-e08d9d85388e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf674383-ea72-44a2-a690-94615829a087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be933eaa-07cc-4a58-80e7-84ba3e21de09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b211df6b-c136-4121-918d-30e73b9cff48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"here it is\"\"\"\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "\n",
    "class TimestampedModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    \"\"\"timestamp check point call back\"\"\"\n",
    "\n",
    "    def __init__(self, save_dir):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.saved_models = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"on epoch end\"\"\"\n",
    "        if epoch // 12 == 0:\n",
    "            _ = logs\n",
    "            timestamp = datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "            safe_timestamp = timestamp.replace(\":\", \"_\")\n",
    "            filename = f\"model_{safe_timestamp}_epoch{epoch}\"\n",
    "            filepath = os.path.join(self.save_dir, filename)\n",
    "            self.model.save(filepath)\n",
    "            print(f\">>> Saved model to {filepath}\")\n",
    "            self.saved_models.append(filepath)\n",
    "\n",
    "            while len(self.saved_models) > 2:\n",
    "                to_delete = self.saved_models.pop(0)\n",
    "                print(f\">>> Deleting old model: {to_delete}\")\n",
    "                tf.io.gfile.rmtree(to_delete)\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    \"\"\"transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                Dense(ff_dim, activation=\"relu\"),\n",
    "                Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"call\"\"\"\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    \"\"\"class\"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"call\"\"\"\n",
    "        seq_len = tf.shape(x)[-1]\n",
    "        pad_len = self.maxlen - seq_len\n",
    "\n",
    "        x = tf.cond(\n",
    "            pad_len > 0,\n",
    "            lambda: tf.pad(\n",
    "                x, paddings=[[0, 0], [0, pad_len]], constant_values=0\n",
    "            ),\n",
    "            lambda: x[:, : self.maxlen],\n",
    "        )\n",
    "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    \"\"\"transformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 32,  # Embedding size for each token\n",
    "        num_heads: int = 2,  # Number of attention heads\n",
    "        ff_dim: int = 32,  # Hidden layer size in feed forward network inside transformer\n",
    "        maxlen: int = 2048,\n",
    "        loop_n: int = 12,\n",
    "        vocab_size: int = 32000,\n",
    "        tokenizer=None,\n",
    "    ):\n",
    "        self.history = None\n",
    "        self.maxlen = maxlen\n",
    "        inputs = Input(shape=(maxlen,))\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "            maxlen, vocab_size, embed_dim\n",
    "        )\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for _ in range(loop_n):\n",
    "            transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            x = transformer_block(x)\n",
    "\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        if self.tokenizer:\n",
    "            self.start_packer = keras_nlp.layers.StartEndPacker(\n",
    "                sequence_length=self.maxlen,\n",
    "                start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "            )\n",
    "\n",
    "    def train_tokenizer(self, data, vocab_size=4096):\n",
    "        \"\"\"train_tokenizer\"\"\"\n",
    "        vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "            data,\n",
    "            vocabulary_size=vocab_size,\n",
    "            lowercase=True,\n",
    "            reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    "        )\n",
    "        tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "            vocabulary=vocab,\n",
    "            sequence_length=self.maxlen,\n",
    "            lowercase=True,\n",
    "        )\n",
    "        self.model.tokenizer = tokenizer\n",
    "        self.start_packer = keras_nlp.layers.StartEndPacker(\n",
    "            sequence_length=self.maxlen,\n",
    "            start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "        )\n",
    "\n",
    "    def train(self, *, train_dataset, validation_data, steps_per_epoch, epochs):\n",
    "        \"\"\"train\"\"\"\n",
    "        self.history = self.model.fit(\n",
    "            train_dataset,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "            callbacks=[TimestampedModelCheckpoint(save_dir=\"./variables\")],\n",
    "        )\n",
    "\n",
    "    def generate(self, text: str, p: float = 0.2):\n",
    "        \"\"\"generate\"\"\"\n",
    "        input_tokens = self.tokenizer([text])\n",
    "        packed_tokens = self.start_packer(input_tokens)\n",
    "        token_length = tf.where(packed_tokens != 0)[-1, 1]\n",
    "        initial_sequence_length = token_length + 1\n",
    "        gen_ittr = self._generate_step(\n",
    "            tokens=packed_tokens,\n",
    "            p=p,\n",
    "            start_index=int(initial_sequence_length.numpy()),\n",
    "        )\n",
    "        generated_text_parts = [text]\n",
    "        for word in gen_ittr:\n",
    "            generated_text_parts.append(word)\n",
    "            print(word, end=\" \")\n",
    "\n",
    "        return \" \".join(generated_text_parts)\n",
    "\n",
    "    def _generate_step(self, tokens, p=0.2, start_index=1):\n",
    "        tokens = tokens.numpy()\n",
    "        for i in range(start_index, self.maxlen):\n",
    "            sampled_token = len(self.tokenizer.vocabulary)\n",
    "            while sampled_token > len(self.tokenizer.vocabulary) - 1:\n",
    "                logits = self.model.predict([tokens], verbose=0)[:, i - 1, :]\n",
    "                logits = tf.constant(logits)\n",
    "                sampled_token = top_p_sample(logits[0], p)\n",
    "\n",
    "            tokens[0][i] = sampled_token\n",
    "            next_word = (\n",
    "                self.tokenizer.detokenize([sampled_token]).numpy().decode()\n",
    "            )\n",
    "            yield next_word\n",
    "            if sampled_token == 2:  # EOS token\n",
    "                raise StopIteration\n",
    "\n",
    "\n",
    "def _build_token_dataset():\n",
    "    \"\"\"\n",
    "    for create dataset to train tokenizer\n",
    "    if you want to train tokenizer local,\n",
    "\n",
    "    ds = _build_token_dataset()\n",
    "    Run Transformer.train_tokenizer(ds)\n",
    "    \"\"\"\n",
    "    # Data\n",
    "    BATCH_SIZE = 64\n",
    "    MIN_TRAINING_SEQ_LEN = 512\n",
    "\n",
    "    keras.utils.get_file(\n",
    "        origin=\"https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\",\n",
    "        extract=True,\n",
    "    )\n",
    "    data_dir = os.path.expanduser(\"./data/\")\n",
    "\n",
    "    # Load simplebooks-92 train set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "    raw_train_ds = (\n",
    "        tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/train.txt\")\n",
    "        .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .shuffle(buffer_size=256)\n",
    "    )\n",
    "\n",
    "    # Load simplebooks-92 validation set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "    raw_val_ds = (\n",
    "        tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/valid.txt\")\n",
    "        .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )\n",
    "    return raw_train_ds, raw_val_ds\n",
    "\n",
    "\n",
    "def top_p_sample(logits, p=0.2):\n",
    "    \"\"\"top sample\"\"\"\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    sorted_probs, sorted_indices = tf.sort(\n",
    "        probs, direction=\"DESCENDING\"\n",
    "    ), tf.argsort(probs, direction=\"DESCENDING\")\n",
    "    cumulative_probs = tf.cumsum(sorted_probs)\n",
    "\n",
    "    cutoff_index = tf.reduce_min(tf.where(cumulative_probs > p))\n",
    "    cutoff_index = tf.maximum(cutoff_index, 1)\n",
    "    top_p_indices = sorted_indices[:cutoff_index]\n",
    "    top_p_logits = tf.gather(logits, top_p_indices)\n",
    "    sampled_relative = tf.random.categorical([top_p_logits], num_samples=1)[\n",
    "        0, 0\n",
    "    ]\n",
    "    sampled_token = top_p_indices[sampled_relative]\n",
    "\n",
    "    return sampled_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26d916af-8e26-43cd-93e2-a37acd97bab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    num_heads=16,\n",
    "    ff_dim=EMBEDDING_DIM * 2,\n",
    "    maxlen=max_length_targ,\n",
    "    loop_n=8,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861bf10-9c00-4489-bc04-e96cfea2fe7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867252f-3813-44f0-96c4-6eeaf03778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    train_dataset=train_dataset,\n",
    "    validation_data=validation_data,\n",
    "    steps_per_epoch=128,\n",
    "    epochs=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48217d-f769-4e02-a938-8e53345708e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3da421-6c0d-4220-8827-4ffdb16c5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\"hello, Tom. Today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d4597-24c5-4eec-babf-8df166accea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
