{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26453007-e94e-4bfc-b0a0-ae6bfb74620e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b4784a9-086f-4d0e-99da-8cb70af3ae09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "import evaluate\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import utils_preproc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc18d3-5e94-457c-969d-f14eeab168b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c90f16b-ce73-4e34-be74-82eb019517e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "MODEL_PATH = \"translate_models/baseline\"\n",
    "DATA_URL = (\n",
    "    \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    ")\n",
    "LOAD_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e83b71e7-5d37-4055-b64b-ac4915fcaf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ab8dde-489f-42a3-b143-edd49c39fad9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation data stored at: /home/jupyter/.keras/datasets/spa-eng/spa.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    \"spa-eng.zip\", origin=DATA_URL, extract=True\n",
    ")\n",
    "\n",
    "path_to_file = os.path.join(os.path.dirname(path_to_zip), \"spa-eng/spa.txt\")\n",
    "print(\"Translation data stored at:\", path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b076aa-bd60-4622-9a89-1622ef282c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    path_to_file, sep=\"\\t\", header=None, names=[\"english\", \"spanish\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072f48df-e9eb-47b5-ad07-dc212ee2a1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(path, num_examples):\n",
    "    with open(path_to_file) as fp:\n",
    "        lines = fp.read().strip().split(\"\\n\")\n",
    "\n",
    "    # TODO 1a\n",
    "    sentence_pairs = [\n",
    "        [utils_preproc.preprocess_sentence(sent) for sent in line.split(\"\\t\")]\n",
    "        for line in lines[:num_examples]\n",
    "    ]\n",
    "\n",
    "    return zip(*sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4945f5-007a-4b87-b4bf-4143e689e82c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_integerize(path, num_examples=None):\n",
    "    targ_lang, inp_lang = load_and_preprocess(path, num_examples)\n",
    "\n",
    "    # TODO 1b\n",
    "    input_tensor, inp_lang_tokenizer = utils_preproc.tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = utils_preproc.tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526194b8-2bd4-4372-8bcd-bd04bc8a9286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_PROP = 0.2\n",
    "NUM_EXAMPLES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a105a4b-96eb-4364-8246-e21c183241f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_and_integerize(\n",
    "    path_to_file, NUM_EXAMPLES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3ac4dc-c3b6-4257-a0cf-a2b0740b4045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118964, 53)\n",
      "(118964, 51)\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)\n",
    "print(target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7c3f4c-7df1-4e2f-8952-9c5c19910d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "max_length_targ = target_tensor.shape[1]\n",
    "print(max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f05477-bd38-424f-a6ae-d2bc1db4f3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = train_test_split(\n",
    "    input_tensor, target_tensor, test_size=TEST_PROP, random_state=SEED\n",
    ")\n",
    "\n",
    "input_tensor_train = splits[0]\n",
    "input_tensor_val = splits[1]\n",
    "\n",
    "target_tensor_train = splits[2]\n",
    "target_tensor_val = splits[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde823ea-dd4b-4985-b247-7d3e22f61ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95171, 95171, 23793, 23793)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    len(input_tensor_train),\n",
    "    len(target_tensor_train),\n",
    "    len(input_tensor_val),\n",
    "    len(target_tensor_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f9e3065-7b34-41e5-852b-6b0c5b526be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(encoder_input, decoder_input):\n",
    "\n",
    "    # shift ahead by 1\n",
    "    target = tf.roll(decoder_input, -1, 1)\n",
    "\n",
    "    # replace last column with 0s\n",
    "    zeros = tf.zeros([target.shape[0], 1], dtype=tf.int32)\n",
    "    target = tf.concat((target[:, :-1], zeros), axis=-1)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((decoder_input, target))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1ee460d-7556-4b7b-89b6-c7add09b2da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(target_tensor_train)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6485d0c4-f3c9-4838-9ce3-de62035a5137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    create_dataset(input_tensor_train, target_tensor_train)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .repeat()\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset = create_dataset(input_tensor_val, target_tensor_val).batch(\n",
    "    BATCH_SIZE, drop_remainder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1bc0ea4-09ca-4cb7-9d04-ee57a26a48cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for d in train_dataset:\n",
    "#     print(d[0])\n",
    "#     print(d[1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71678b-2564-4cb3-9842-0975d5e542b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "505619ca-2910-4f21-881f-fb04b67e2315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_UNITS = 256\n",
    "\n",
    "INPUT_VOCAB_SIZE = len(inp_lang.word_index) + 1\n",
    "TARGET_VOCAB_SIZE = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ef9c485-81d6-4101-a6ab-6012f0fc4899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de35f9-cef7-497d-8bbd-e8a171b25ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a127cc9d-03b8-4d74-9fc6-288b812110a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# this should output \"Num GPUs Available: 1\" if you have one GPU attached\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb01e5d9-5b6d-437e-9fb3-51aac6e946cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2EWTmjh48kw",
    "outputId": "4a6bcd0b-6c00-42b3-da90-f8f22b799381"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 128\n",
    "MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000  # Limits parameters in model\n",
    "\n",
    "keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "data_dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# Load simplebooks-92 train set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/train.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# Load simplebooks-92 validation set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0079fa3e-d328-4e5e-a446-6db91eb5a599",
   "metadata": {
    "id": "cF4Unid048kx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the word piece tokenizer. This will take 5-10 mins...\n",
      "Training is complete!!\n"
     ]
    }
   ],
   "source": [
    "# Train tokenizer vocabulary\n",
    "print(\"Training the word piece tokenizer. This will take 5-10 mins...\")\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")\n",
    "print(\"Training is complete!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3e6d39e-b48e-4480-8068-1ef3cff8bd56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab345b-9184-403c-8fe8-e08d9d85388e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf674383-ea72-44a2-a690-94615829a087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be933eaa-07cc-4a58-80e7-84ba3e21de09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b211df6b-c136-4121-918d-30e73b9cff48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"here it is\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    \"\"\"transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                Dense(ff_dim, activation=\"relu\"),\n",
    "                Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"call\"\"\"\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    \"\"\"class\"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"call\"\"\"\n",
    "        seq_len = tf.shape(x)[-1]\n",
    "        pad_len = self.maxlen - seq_len\n",
    "\n",
    "        x = tf.cond(\n",
    "            pad_len > 0,\n",
    "            lambda: tf.pad(\n",
    "                x, paddings=[[0, 0], [0, pad_len]], constant_values=0\n",
    "            ),\n",
    "            lambda: x[:, : self.maxlen],\n",
    "        )\n",
    "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    \"\"\"transformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 32,  # Embedding size for each token\n",
    "        num_heads: int = 2,  # Number of attention heads\n",
    "        ff_dim: int = 32,  # Hidden layer size in feed forward network inside transformer\n",
    "        maxlen: int = 2048,\n",
    "        loop_n: int = 12,\n",
    "        vocab_size: int = 32000,\n",
    "        tokenizer=None,\n",
    "    ):\n",
    "        self.history = None\n",
    "        self.maxlen = maxlen\n",
    "        inputs = Input(shape=(maxlen,))\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "            maxlen, vocab_size, embed_dim\n",
    "        )\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for _ in range(loop_n):\n",
    "            transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            x = transformer_block(x)\n",
    "\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        if self.tokenizer:\n",
    "            self.start_packer = keras_nlp.layers.StartEndPacker(\n",
    "                sequence_length=self.maxlen,\n",
    "                start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "            )\n",
    "\n",
    "    def train_tokenizer(self, data, vocab_size=4096):\n",
    "        \"\"\"train_tokenizer\"\"\"\n",
    "        vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "            data,\n",
    "            vocabulary_size=vocab_size,\n",
    "            lowercase=True,\n",
    "            reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    "        )\n",
    "        tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "            vocabulary=vocab,\n",
    "            sequence_length=self.maxlen,\n",
    "            lowercase=True,\n",
    "        )\n",
    "        self.model.tokenizer = tokenizer\n",
    "        self.start_packer = keras_nlp.layers.StartEndPacker(\n",
    "            sequence_length=self.maxlen,\n",
    "            start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "        )\n",
    "\n",
    "    def train(self, *, train_dataset, validation_data, steps_per_epoch, epochs):\n",
    "        \"\"\"train\"\"\"\n",
    "        self.history = self.model.fit(\n",
    "            train_dataset,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "\n",
    "    def generate(self, text: str, p: float = 0.2):\n",
    "        \"\"\"generate\"\"\"\n",
    "        input_tokens = self.tokenizer([text])\n",
    "        packed_tokens = self.start_packer(input_tokens)\n",
    "        token_length = tf.where(packed_tokens != 0)[-1, 1]\n",
    "        initial_sequence_length = token_length + 1\n",
    "        gen_ittr = self._generate_step(\n",
    "            tokens=packed_tokens,\n",
    "            p=p,\n",
    "            start_index=initial_sequence_length,  # 次に予測する位置\n",
    "        )\n",
    "        generated_text_parts = []\n",
    "        for word in gen_ittr:\n",
    "            generated_text_parts.append(word)\n",
    "            print(word, end=\" \")\n",
    "\n",
    "        return \"\".join(generated_text_parts)  # より自然な表示のため\n",
    "\n",
    "    def _generate_step(self, tokens, p=0.2, start_index=1):\n",
    "        tokens = tokens.numpy()\n",
    "        for i in range(start_index, self.maxlen):\n",
    "            logits = self.model.predict([tokens], verbose=0)[:, i - 1, :]\n",
    "            logits = tf.constant(logits)[0]\n",
    "            sampled_token = top_p_sample(logits, p).numpy()\n",
    "            tokens[0][i] = sampled_token\n",
    "            next_word = (\n",
    "                self.tokenizer.detokenize([sampled_token])\n",
    "                .numpy()\n",
    "                .decode(\"utf-8\")\n",
    "            )\n",
    "            print(next_word)\n",
    "            yield next_word\n",
    "            if sampled_token == 2:  # EOS token\n",
    "                raise StopIteration\n",
    "\n",
    "\n",
    "def _build_token_dataset():\n",
    "    \"\"\"\n",
    "    for create dataset to train tokenizer\n",
    "    if you want to train tokenizer local,\n",
    "\n",
    "    ds = _build_token_dataset()\n",
    "    Run Transformer.train_tokenizer(ds)\n",
    "    \"\"\"\n",
    "    # Data\n",
    "    BATCH_SIZE = 64\n",
    "    MIN_TRAINING_SEQ_LEN = 512\n",
    "\n",
    "    keras.utils.get_file(\n",
    "        origin=\"https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\",\n",
    "        extract=True,\n",
    "    )\n",
    "    data_dir = os.path.expanduser(\"./data/\")\n",
    "\n",
    "    # Load simplebooks-92 train set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "    raw_train_ds = (\n",
    "        tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/train.txt\")\n",
    "        .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .shuffle(buffer_size=256)\n",
    "    )\n",
    "\n",
    "    # Load simplebooks-92 validation set and filter out short lines using MIN_TRAINING_SEQ_LEN\n",
    "    raw_val_ds = (\n",
    "        tf.data.TextLineDataset(data_dir + \"simplebooks-92-raw/valid.txt\")\n",
    "        .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )\n",
    "    return raw_train_ds, raw_val_ds\n",
    "\n",
    "\n",
    "def top_p_sample(logits, p=0.2):\n",
    "    \"\"\"top sample\"\"\"\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    sorted_probs, sorted_indices = tf.sort(\n",
    "        probs, direction=\"DESCENDING\"\n",
    "    ), tf.argsort(probs, direction=\"DESCENDING\")\n",
    "    cumulative_probs = tf.cumsum(sorted_probs)\n",
    "\n",
    "    cutoff_index = tf.reduce_min(tf.where(cumulative_probs > p))\n",
    "    cutoff_index = tf.maximum(cutoff_index, 1)\n",
    "    top_p_indices = sorted_indices[:cutoff_index]\n",
    "    top_p_logits = tf.gather(logits, top_p_indices)\n",
    "    sampled_relative = tf.random.categorical([top_p_logits], num_samples=1)[\n",
    "        0, 0\n",
    "    ]\n",
    "    sampled_token = top_p_indices[sampled_relative]\n",
    "\n",
    "    return sampled_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26d916af-8e26-43cd-93e2-a37acd97bab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    num_heads=2,\n",
    "    ff_dim=EMBEDDING_DIM * 2,\n",
    "    maxlen=max_length_targ,\n",
    "    loop_n=4,\n",
    "    vocab_size=TARGET_VOCAB_SIZE,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28717c22-1685-4db2-a0ea-290c29fad2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "text = \"hello\"\n",
    "input_tokens = self.tokenizer([text])\n",
    "packed_tokens = self.start_packer(input_tokens)\n",
    "token_length = tf.where(packed_tokens != 0)[-1, 1]\n",
    "initial_sequence_length = token_length + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c14bfb59-0bed-4e77-825a-415b43ab20b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tokens = packed_tokens\n",
    "p = 0.2\n",
    "start_index = initial_sequence_length\n",
    "print(start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "943eee06-88fc-48dc-ab01-38f618bfd98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b31c341-4f3c-4a5e-9763-ff11bda6704c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e842123c-9e90-4dec-9c85-92748a91d3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'range_op' from 'tensorflow.python.data.ops' (/home/jupyter/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[:, i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:1014\u001b[0m, in \u001b[0;36mDatasetV2.range\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a `Dataset` of a step-separated range of values.\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m>>> list(Dataset.range(5).as_numpy_iterator())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;124;03m  ValueError: if len(args) == 0.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> range_op ->\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# -> dataset_ops).\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m range_op\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m range_op\u001b[38;5;241m.\u001b[39m_range(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'range_op' from 'tensorflow.python.data.ops' (/home/jupyter/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/__init__.py)"
     ]
    }
   ],
   "source": [
    "logits = self.model.predict([tokens], verbose=0)[:, i - 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dbc04-7b0c-4926-844a-fc8614d4515c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = tf.constant(logits)\n",
    "sampled_token = top_p_sample(logits[0], p).numpy()\n",
    "tokens[0][i] = sampled_token\n",
    "next_word = self.tokenizer.detokenize([sampled_token]).numpy().decode(\"utf-8\")\n",
    "print(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1116a5-bb6c-4d83-8cbf-9fea111cf19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d3dab-4a6e-4be0-a8c4-a5f9b07145a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1cb43-9d1d-4e53-a09b-9c16c06cda04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e66d3-3e0b-4406-8a48-091049531ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3ec46-717f-4bb4-a259-0ac5bc9f83d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79965f22-431e-4ca6-a220-7e90ff1ad906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a25d3b-baca-495b-9a2c-bb292c402204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861bf10-9c00-4489-bc04-e96cfea2fe7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48217d-f769-4e02-a938-8e53345708e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3da421-6c0d-4220-8827-4ffdb16c5614",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "337954c4-e318-4a37-a09d-30d1e8ad3cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 51), dtype=int32, numpy=\n",
       "array([[  2,  45, 301,  47, 408,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens = model.start_packer(tokenizer([\"I am ken\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "43c1c331-b044-4b98-8d98-4eb77ea9a52a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[   1, 1050,   30,  103, 1520,  291,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = model.tokenizer([text])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0404e271-73be-4887-8022-d70763000e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e7bd421-0742-4460-aeed-46a9dada99cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c0ecb0d-0255-449e-b8bd-140f2fb85f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[ 103, 1520,  291,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = self.tokenizer([\"hello\"])\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62a53508-b0ca-4473-a0f8-59a88a004e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "packed_tokens = self.start_packer(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3b1cc39-b134-48ff-bcf4-41de9a764f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_sequence_length = tf.shape(input_tokens)[1].numpy() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9633f235-5831-4ed5-ac66-d402a8caac57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = packed_tokens\n",
    "start_index = initial_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "045e1d43-10c6-4c8d-880a-a5c3c2a61e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 51), dtype=int32, numpy=\n",
       "array([[   2,  103, 1520,  291,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "176ff9e2-7561-4db2-9572-15ea238a849f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 51), dtype=int32, numpy=\n",
       " array([[   2,  103, 1520,  291,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=4>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = self.tokenizer([\"hello\"])\n",
    "packed_tokens = self.start_packer(input_tokens)\n",
    "\n",
    "token_length = tf.where(packed_tokens != 0)[-1, 1] + 1\n",
    "packed_tokens, token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1dad9b06-a504-4090-bd02-17ffdc7ef487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampled_token = sampler(\n",
    "    next=self._next,\n",
    "    prompt=packed_tokens,\n",
    "    index=token_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "580c8121-3ca6-4aea-b185-7b0ae56c378c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 51), dtype=int32, numpy=\n",
       "array([[    2,   103,  1520,   291,  3361, 12322,  1980,  4483, 10543,\n",
       "         7325,  2010,  9468,   742,  6957, 12163, 10350,  9290,  5562,\n",
       "         7873,  6501,  1096,  9582, 11866,  5290,  1643,  5283,  6303,\n",
       "         8531,  9628,  1973,  9206, 11515,  2485,  6839, 12639,  9397,\n",
       "        12708,   834,  1371,  4270, 12626,  8227,  6501,  7335,  3883,\n",
       "         7740,  8348,   869,  6511, 11874,  1223]], dtype=int32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c59a6921-d4cf-484d-8818-322ba465e908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=3>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d4597-24c5-4eec-babf-8df166accea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
