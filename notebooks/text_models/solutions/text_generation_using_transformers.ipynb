{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFoVN0Fd48kp"
   },
   "source": [
    "# Generating Text Using a Transformer Decoder-Only Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNbexFYv48kt"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this example, we will use KerasNLP to build a scaled down Generative model using a Transformer Decoder. A generative model allows you to generate sophisticated text from a prompt.\n",
    "\n",
    "We will train the model on the [simplebooks-92](https://arxiv.org/abs/1911.12391) corpus,which is a dataset made from several novels. It is a good dataset for this example since it has a small vocabulary and high word frequency, which is beneficial when training a generative model with few parameters.\n",
    "\n",
    "This demonstrates how to use KerasNLP tokenization, layers and metrics to simplify the training process, and then show how to generate output text using the KerasNLP sampling utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbtWRiXS48kt"
   },
   "source": [
    "## Setup\n",
    "\n",
    "In order to run this notebook, you will need `keras_nlp`. KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Keras NLP offers transformer later which is extremely helpful to build the generative model in this notebook.\n",
    "\n",
    "Uncomment the cell below if you don't have keras_nlp already installed. You may need to restart the kernel once it has been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QxR27aY5Ghe",
    "outputId": "a31a6553-979c-47ec-aadc-4d829f99d80b"
   },
   "outputs": [],
   "source": [
    "#!pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPz25USa48ku",
    "outputId": "aa8bfff2-3da8-45da-eed5-70034e1172d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you start\n",
    "Please ensure you have a GPU (1 x NVIDIA Tesla T4 should be enough) attached to your Notebook instance to ensure that the training doesn't take too long.\n",
    "\n",
    "To check if you have a GPU attached you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# this should output \"Num GPUs Available: 1\" if you have one GPU attached\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqWg-fxm48kv"
   },
   "source": [
    "## Settings & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9Wwy7GtU48kv"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 128\n",
    "MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000  # Limits parameters in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYcb6lTq48kw"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will be using the SimpleBooks dataset for this notebook. The SimpleBooks dataset consists of 1,573 Gutenberg books and a small vocabulary size to word-level tokens ratio. It has a vocabulary size of ~98k. This size makes it easier to fit a small transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2EWTmjh48kw",
    "outputId": "4a6bcd0b-6c00-42b3-da90-f8f22b799381"
   },
   "outputs": [],
   "source": [
    "keras.utils.get_file(\n",
    "    origin=\"https://storage.googleapis.com/asl-public/text/data/simplebooks.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
    "\n",
    "# Load simplebooks-92 train set and filter out short lines.\n",
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "# Load simplebooks-92 validation set and filter out short lines.\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN8zvDM_48kx"
   },
   "source": [
    "## Train the tokenizer\n",
    "\n",
    "We train the tokenizer from the training dataset for a vocabulary size of `VOCAB_SIZE`, which is a tuned hyperparameter. We want to limit the vocabulary as much as possible, as we will see later on that it has a large effect on the number of model parameters. We also don't want to include *too few* vocabulary terms, or there would be too many out-of-vocabulary (OOV) sub-words. In addition, three tokens are reserved in the vocabulary:\n",
    "\n",
    "- `\"[PAD]\"` for padding sequences to `SEQ_LEN`. This token has index 0 in both `reserved_tokens` and `vocab`, since `WordPieceTokenizer` (and other layers) consider `0`/`vocab[0]` as the default padding.\n",
    "- `\"[UNK]\"` for OOV sub-words, which should match the default `oov_token=\"[UNK]\"` in\n",
    "`WordPieceTokenizer`.\n",
    "- `\"[BOS]\"` stands for beginning of sentence, but here technically it is a token representing the beginning of each line of training data.\n",
    "\n",
    "This cell takes ~5-10 mins to execute because it is computing the word piece vocabulary on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cF4Unid048kx"
   },
   "outputs": [],
   "source": [
    "# Train tokenizer vocabulary\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhlKbkvt48kx"
   },
   "source": [
    "## Load tokenizer\n",
    "\n",
    "We use the vocabulary data to initialize `keras_nlp.tokenizers.WordPieceTokenizer`. WordPieceTokenizer is an efficient implementation of the WordPiece algorithm used by BERT and other models. It will strip, lower-case and do other irreversible preprocessing operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kTz3xEmh48kx"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INSvSBnB48ky"
   },
   "source": [
    "## Tokenize data\n",
    "\n",
    "We preprocess the dataset by tokenizing and splitting it into `features` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U77FMjaY48ky"
   },
   "outputs": [],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Tokenize and split into train and label sequences.\n",
    "train_ds = raw_train_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = raw_val_ds.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0XiTjui48ky"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "We create our scaled down transformer decoder based generative text model with the following layers:\n",
    "\n",
    "- One `keras_nlp.layers.TokenAndPositionEmbedding` layer, which combines the embedding\n",
    "for the token and its position.\n",
    "- Multiple `keras_nlp.layers.TransformerDecoder` layers, with the default causal masking.\n",
    "The layer has no cross-attention when run with decoder sequence only.\n",
    "- One final dense linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uKHCA6ej48ky"
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "# Embedding layer\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "# Transformer decoder layers\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention\n",
    "# Output layer\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# set up the loss metric\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwtZicZo48ky"
   },
   "source": [
    "Let's take a look at our model summary - a large majority of the\n",
    "parameters are in the `token_and_position_embedding` and the output `dense` layer!\n",
    "This means that the vocabulary size (`VOCAB_SIZE`) has a large effect on the size of the model,\n",
    "while the number of Transformer decoder layers (`NUM_LAYERS`) doesn't affect it as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BJg1dbyZ48ky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, None, 256)        1312768   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_decoder (Transf  (None, None, 256)        394749    \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tran  (None, None, 256)        394749    \n",
      " sformerDecoder)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,387,266\n",
      "Trainable params: 3,387,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2F9s5tyz48kz"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, let's train it with the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "StJs0vL048kz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3169/3169 - 360s - loss: 4.5626 - perplexity: 96.2036 - val_loss: 4.1386 - val_perplexity: 63.3348 - 360s/epoch - 113ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7df04f0580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 1  # increase the number of epochs for better results\n",
    "model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reWRkSNf48kz"
   },
   "source": [
    "## Inference\n",
    "\n",
    "With our trained model, we can test it out to gauge its performance. To do this\n",
    "we can seed our model with an input sequence starting with the `\"[BOS]\"` token,\n",
    "and progressively sample the model by making predictions for each subsequent\n",
    "token in a loop.\n",
    "\n",
    "To start lets build a prompt with the same shape as our model inputs, containing\n",
    "only the `\"[BOS]\"` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NL15KPUZ48kz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"packer\" layers adds the [BOS] token for us.\n",
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWu0fC2H48kz"
   },
   "source": [
    "We will use the `keras_nlp.samplers` module for inference, which requires a\n",
    "callback function wrapping the model we just trained. This wrapper calls\n",
    "the model and returns the logit predictions for the current token we are\n",
    "generating.\n",
    "\n",
    "Note: There are two pieces of more advanced functionality available when\n",
    "defining your callback. The first is the ability to take in a `cache` of states\n",
    "computed in previous generation steps, which can be used to speed up generation.\n",
    "The second is the ability to output the final dense \"hidden state\" of each\n",
    "generated token. This is used by `keras_nlp.samplers.ContrastiveSampler`, which\n",
    "avoids repetition by penalizing repeated hidden states. Both are optional, and\n",
    "we will ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uPLJM88348kz"
   },
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JZHgird48k0"
   },
   "source": [
    "Creating the wrapper function is the most complex part of using these functions. Now that\n",
    "it's done, let's test out the different utilities, starting with greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMUdU93R48k0"
   },
   "source": [
    "### Greedy search\n",
    "\n",
    "We greedily pick the most probable token at each timestep. In other words, we get the\n",
    "argmax of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kY-WPGTL48k0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search generated text: \n",
      "[b'[BOS] \" i \\' m not going to be a good deal of trouble , \" said the doctor . \" i \\' m going to be a good deal of time , and i \\' ll tell you . i \\' ll tell you what i \\' ll be doing . i \\' ll tell you about the first thing that i \\' ll be a good time . i \\' ll be a good time , and then i \\' ll be a good time . i \\' ll be a good time , and then , \" said the doctor , \" he said , \" i \\' ll be a good deal of time , \" he said . \" i \\' m going']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak1-pDMr48k0"
   },
   "source": [
    "As you can see, greedy search starts out making some sense, but quickly starts repeating\n",
    "itself. This is a common problem with text generation that can be fixed by some of the\n",
    "probabilistic text generation utilities shown later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrLyz92l48k0"
   },
   "source": [
    "### Beam search\n",
    "\n",
    "At a high-level, beam search keeps track of the `num_beams` most probable sequences at\n",
    "each timestep, and predicts the best next token from all sequences. It is an improvement\n",
    "over greedy search since it stores more possibilities. However, it is less efficient than\n",
    "greedy search since it has to compute and store multiple potential sequences.\n",
    "\n",
    "**Note:** beam search with `num_beams=1` is identical to greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oI7AQ9-h48k0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search generated text: \n",
      "[b'[BOS] \" i don \\' t know , \" he said , as he said , as he said , as he said , as he said , that he would not be able to do so , that he would not be able to do so , that he would not be able to do so , but he would not be able to do so , but he would be able to do so , that he would not be likely to be able to do so , if he should be able to do so , he would be able to do so , that he would not be able to do so . [PAD] , if he would not be able to do so']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-azZoy0b48k0"
   },
   "source": [
    "Similar to greedy search, beam search quickly starts repeating itself, since it is still\n",
    "a deterministic method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYA14uMc48k0"
   },
   "source": [
    "### Random search\n",
    "\n",
    "Random search is our first probabilistic method. At each time step, it samples the next\n",
    "token using the softmax probabilities provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eDpsTuRm48k0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search generated text: \n",
      "[b'[BOS] \" is the present , this is the result that we should be ordered to ride alone with the boys who have heard of , and rowed back to the mountains of besiege , were indeed the airened . the darkness of roughlows to tidings itself ; the possibility of destruction , others would soon pass without any passage or something of the way of rams . the earth is mountains atdvishing , all . the national voice must have met us , and the northern southeast of them . we have stayed here long organ . his brothers \\' attacks , and caused joy to start through . \" [PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWSg0ON648k1"
   },
   "source": [
    "Voilà, no repetitions! However, with random search, we may see some nonsensical words\n",
    "appearing since any word in the vocabulary has a chance of appearing with this sampling\n",
    "method. This is fixed by our next search utility, top-k search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHxKUOWn48k1"
   },
   "source": [
    "### Top-K search\n",
    "\n",
    "Similar to random search, we sample the next token from the probability distribution\n",
    "provided by the model. The only difference is that here, we select out the top `k` most\n",
    "probable tokens, and distribute the probability mass over them before sampling. This way,\n",
    "we won't be sampling from low probability tokens, and hence we would have less\n",
    "nonsensical words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hna2w_e448k1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "[b'[BOS] \" the next day the prince , \" the queen said , and the lord replied : \" my brothers ; \" you \\' re the prince , i shall be here and you . it is that the best , for that is the best of you to do so , and i have to be killed in the castle . you should not be able to do this , sir . your husband is as well as the queen of the courtyard , and that you , and the queen and her daughter of the queen and daughter of france . it is so many times as you have been a knight and a servant girl who has been brought with him in her']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHesxfg748k1"
   },
   "source": [
    "### Top-P search\n",
    "\n",
    "Even with the top-k search, there is something to improve upon. With top-k search, the\n",
    "number `k` is fixed, which means it selects the same number of tokens for any probability\n",
    "distribution. Consider two scenarios, one where the probability mass is concentrated over\n",
    "2 words and another where the probability mass is evenly concentrated across 10. Should\n",
    "we choose `k=2` or `k=10`? There is no one size that fits all `k` here.\n",
    "\n",
    "This is where top-p search comes in! Instead of choosing a `k`, we choose a probability\n",
    "`p` that we want the probabilities of the top tokens to sum up to. This way, we can\n",
    "dynamically adjust the `k` based on the probability distribution. By setting `p=0.9`, if\n",
    "90% of the probability mass is concentrated on the top 2 tokens, we can filter out the\n",
    "top 2 tokens to sample from. If instead the 90% is distributed over 10 tokens, it will\n",
    "similarly filter out the top 10 tokens to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3gwfY-YB48k1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-P search generated text: \n",
      "[b\"[BOS] the third day came to him and then , to him . he was a tall man , and he was the most of the tall , old woman who was very angry , and of course , and so well , and how he would have been to have been the only one of the most beautiful ones who had not gone . there was nothing for him to be of a world , though he did not go , he would have taken the money for his own . he had not yet come to his uncle ' s house . he had no difficulty in doing so much of the dearest of a little house and in the morning . [PAD] , if\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnss68kt48k1"
   },
   "source": [
    "### Using callbacks for text generation\n",
    "\n",
    "We can also wrap the utilities in a callback, which allows you to print out a prediction\n",
    "sequence for every epoch of the model! Here is an example of a callback for top-k search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lMw0jBN148k6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Top-K search generated text: \n",
      "[b'[BOS] \" you \\' d like a chum - - and you \\' d better go away in the world , but you know . if your father is a good - tempered boy and you may know . \" but in a great many places have been in the country . \" he said to him , \" and he is a great - - the young man , who is here . and it is a good thing that , if we were to be sure that we might go to town . he will be in the country , but he is a country in town . we \\' re going to meet the country in the village , who can \\'']\n",
      "\n",
      "1/1 - 13s - loss: 4.2460 - perplexity: 70.0904 - 13s/epoch - 13s/step\n",
      "Epoch 2/2\n",
      "Top-K search generated text: \n",
      "[b\"[BOS] but there were two young men , at the end of the island . there were several of the people of the countrymen who , who , with them , were to be killed to the country , and to be able to find a large , powerful men who were indulge , and advisable , to make an easy resistance , to be carried out of this point for their own , and towed their lives , and , as the english troops were at all times , and their enemies . the men who were at least two or four o ' clocks , who were now driven to their homes , and then , as\"]\n",
      "\n",
      "1/1 - 13s - loss: 4.0664 - perplexity: 58.5320 - 13s/epoch - 13s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7df0647520>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "\n",
    "\n",
    "text_generation_callback = TopKTextGenerator(k=10)\n",
    "# Dummy training loop to demonstrate callback.\n",
    "model.fit(\n",
    "    train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVdJAlz348k6"
   },
   "source": [
    "## Acknowledgment\n",
    "This notebook is based on a [Keras tutorial by Jesse Chan](https://keras.io/examples/generative/text_generation_gpt/#train-the-tokenizer). The transformer decoder layer is based on the the research paper by Google, [Attention Is All You Need, Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "text_generation_gpt",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
