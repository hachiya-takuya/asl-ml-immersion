{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom containers with Vertex AI Training\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Learn how to create a train and a validation split with BigQuery\n",
    "1. Learn how to wrap a machine learning model into a Docker container and train in on Vertex AI\n",
    "1. Learn how to use the hyperparameter tuning engine on Vertex AI to find the best hyperparameters\n",
    "1. Learn how to deploy a trained machine learning model on Vertex AI as a REST API and query it\n",
    "\n",
    "In this lab, you develop, package as a docker image, and run on **Vertex AI Training** a training application that trains a multi-class classification model that predicts the type of forest cover from cartographic data. The [dataset](../../../datasets/covertype/README.md) used in the lab is based on **Covertype Data Set** from UCI Machine Learning Repository.\n",
    "\n",
    "The training code uses `Pytorch` for data pre-processing and modeling. The code has been instrumented using the `hypertune` package so it can be used with **Vertex AI** hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install\n",
    "```\n",
    "pip install --upgrade torch==1.11.0 torchtext==0.12.0 --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "pip install --upgrade torch-model-archiver\n",
    "pip install --upgrade google-cloud-aiplatform[prediction]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for Vertex AI Training and Prediction\n",
    "- `ARTIFACT_STORE` - A GCS bucket in the created in the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"REGION\"\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the `ARTIFACT_STORE` bucket if it's not there. Note that this bucket should be created in the region specified in the variable `REGION` (if you have already a bucket with this name in a different region than `REGION`, you may want to change the `ARTIFACT_STORE` name so that you can recreate a bucket in `REGION` with the command in the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://asl-public/data/covertype/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Covertype dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `covertype_dataset.covertype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation splits\n",
    "\n",
    "Use BigQuery to sample training and validation splits and save them to GCS storage\n",
    "### Create a training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (8)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a training application\n",
    "Now let's define the preprocessing and training application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Our data includes both numerical and categorical features. Numerical features are values that can be measured or counted, such as height or weight. Categorical features are values that can be sorted into categories, such as gender or color.\n",
    "\n",
    "To prepare our data for analysis, we need to standardize the numerical features and one-hot encode the categorical features. Standardizing numerical features means that we adjust their values so they have a mean of zero and a standard deviation of one. One-hot encoding categorical features means that we create a separate column for each category and assign a value of 1 or 0 to indicate which category the feature belongs to.\n",
    "\n",
    "To do these preprocessing steps, we will put the numerical and categorical features in separate lists and apply the appropriate logic to each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "]\n",
    "LABEL_COLUMN = \"Cover_Type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the actual preprocessing logic.<br>\n",
    "Standardization and One-hot encoding have two steps: fitting to the entire dataset and computing states (e.g., mean and standard deviation for standardization and vocabulary lists for one-hot encoding) and transforming actual values.<br>\n",
    "Also, we need to serialize and save the states to reuse the same values in the inference phase.\n",
    "\n",
    "So, we define a base class named `FeatureTransformerBase` that has methods of `fit(),` `transform()` (additionally `fit_transform()` to run both), and `serializa_constans,` and implement `OneHotEncoder` and `StandardScaler` respectively by subclassing the `FeatureTransformerBase.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformerBase:\n",
    "    def fit(self, series: pd.Series):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def transform(self, feature: list) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit_transform(self, series: pd.Series) -> torch.Tensor:\n",
    "        self.fit(series)\n",
    "        return self.transform(series)\n",
    "\n",
    "    def seriarize_constants(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class OneHotEncoder(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        categories = series.unique().tolist()\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in categories]))\n",
    "        self.dictionary = dictionary\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, dictionary=None):\n",
    "        if dictionary:\n",
    "            assert type(dictionary) == list\n",
    "            self.dictionary = vocab(\n",
    "                OrderedDict([(cat, 1) for cat in dictionary])\n",
    "            )\n",
    "\n",
    "        indices = self.dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(\n",
    "            indices, num_classes=len(self.dictionary.get_itos())\n",
    "        )\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def seriarize_constants(self):\n",
    "        return {\"dictionary\": self.dictionary.get_itos()}\n",
    "\n",
    "\n",
    "class StandardScaler(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        self.mean = np.float64(series.mean())\n",
    "        self.std = np.float64(series.std())\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, mean=None, std=None):\n",
    "        if mean:\n",
    "            self.mean = np.float64(mean)\n",
    "        if std:\n",
    "            self.std = np.float64(std)\n",
    "\n",
    "        standardized = (feature - self.mean) / self.std\n",
    "        return torch.tensor(standardized)[:, None].float()\n",
    "\n",
    "    def seriarize_constants(self):\n",
    "        return {\"mean\": self.mean, \"std\": self.std}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a utility function to transform all the features by calling `transform` function, and concatenate all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, transformers):\n",
    "    transformed_features = [\n",
    "        transformers[c].transform(df[c].to_list())\n",
    "        for c in df.columns\n",
    "        if c != LABEL_COLUMN\n",
    "    ]\n",
    "    features = torch.cat(transformed_features, 1)\n",
    "\n",
    "    label = df[LABEL_COLUMN].to_list()\n",
    "    label = torch.LongTensor(label)\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the `fit()` function for all the columns separately and transform them in the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproc Categorical columns\n",
    "transformers = {\n",
    "    c_feature: OneHotEncoder().fit(df_train[c_feature])\n",
    "    for c_feature in CATEGORICAL_FEATURES\n",
    "}\n",
    "transformers.update(\n",
    "    {\n",
    "        n_feature: StandardScaler().fit(df_train[n_feature])\n",
    "        for n_feature in NUMERICAL_FEATURES\n",
    "    }\n",
    ")\n",
    "\n",
    "training_transformed = preprocess(df_train, transformers)\n",
    "validation_transformed = preprocess(df_validation, transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export preprocessing states file for prediction\n",
    "Our training and validation data are transformed successfully.<br>\n",
    "Then, let's test the seriarize_constants function, and save the states in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export json for preprocessing\n",
    "preprocesinng_json = {\n",
    "    c: transformers[c].seriarize_constants()\n",
    "    for c in df_train.columns\n",
    "    if c != LABEL_COLUMN\n",
    "}\n",
    "\n",
    "with open(\"preprocessing.json\", \"w\") as f:\n",
    "    json.dump(preprocesinng_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesinng_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Model and training/validation step\n",
    "We define a simple neural network model, and training and validation steps in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "# Model\n",
    "class NeuralNetworkModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(54, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 7),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear_relu_stack(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "model = NeuralNetworkModel().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            validation_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    validation_loss /= num_batches\n",
    "    correct /= size\n",
    "    return correct, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training locally.\n",
    "Let's test if it runs locally before passing it Cloud training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = TensorDataset(*training_transformed)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "validation_dataset = TensorDataset(*validation_transformed)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    accuracy, validation_loss = validation(\n",
    "        validation_dataloader, model, loss_fn, device\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {validation_loss:>8f}\"\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on Vertex AI Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the tuning script. \n",
    "In order to run Cloud training, we define a python file that includes all the codes from preprocessing to training. Most of the codes are the same as the one above.<br>\n",
    "But we need to add some codes to do hyperparameter tuning, and save trained model and preprocessing states for later use.\n",
    "\n",
    "Notice the use of the `hypertune` package to report the `accuracy` optimization metric to Vertex AI hyperparameter tuning service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "LABEL_COLUMN = \"Cover_Type\"\n",
    "\n",
    "\n",
    "class FeatureTransformerBase:\n",
    "    def fit(self, series: pd.Series):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def transform(self, feature: list) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit_transform(self, series: pd.Series) -> torch.Tensor:\n",
    "        self.fit(series)\n",
    "        return self.transform(series)\n",
    "    \n",
    "    def seriarize_constants(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "class OneHotEncoder(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        categories = series.unique().tolist()\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in categories]))\n",
    "        self.dictionary = dictionary\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, dictionary=None):\n",
    "        if dictionary:\n",
    "            assert type(dictionary) == list\n",
    "            self.dictionary = vocab(OrderedDict([(cat, 1) for cat in dictionary]))\n",
    "\n",
    "        indices = self.dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(indices, num_classes=len(self.dictionary.get_itos()))\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def seriarize_constants(self):\n",
    "        return {\"dictionary\": self.dictionary.get_itos()}\n",
    "        \n",
    "        \n",
    "class StandardScaler(FeatureTransformerBase):\n",
    "    def fit(self, series):\n",
    "        self.mean = np.float64(series.mean())\n",
    "        self.std = np.float64(series.std())\n",
    "        return self\n",
    "\n",
    "    def transform(self, feature, mean=None, std=None):\n",
    "        if mean:\n",
    "            self.mean = np.float64(mean)\n",
    "        if std:\n",
    "            self.std = np.float64(std)\n",
    "            \n",
    "        standardized = (feature - self.mean) / self.std\n",
    "        return torch.tensor(standardized)[:, None].float()\n",
    "    \n",
    "    def seriarize_constants(self):\n",
    "        return {\"mean\": self.mean, \"std\": self.std}\n",
    "\n",
    "\n",
    "def preprocess(df, transformers):\n",
    "    transformed_features = [transformers[c].transform(df[c].to_list()) for c in df.columns if c != LABEL_COLUMN]\n",
    "    features = torch.cat(transformed_features, 1)\n",
    "    \n",
    "    label = df[LABEL_COLUMN].to_list()\n",
    "    label = torch.LongTensor(label)\n",
    "    return features, label\n",
    "\n",
    "    \n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, batch_size, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    \n",
    "    # Preproc Categorical columns\n",
    "    transformers = {c_feature: OneHotEncoder().fit(df_train[c_feature]) for c_feature in CATEGORICAL_FEATURES}\n",
    "    transformers.update({n_feature: StandardScaler().fit(df_train[n_feature]) for n_feature in NUMERICAL_FEATURES})\n",
    "\n",
    "    training_transformed = preprocess(df_train, transformers)\n",
    "    validation_transformed = preprocess(df_validation, transformers)\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = TensorDataset(*training_transformed)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    validation_dataset = TensorDataset(*validation_transformed)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Set device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Model\n",
    "    class NeuralNetworkModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(54, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 7)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            y = self.linear_relu_stack(x)\n",
    "            return y\n",
    "\n",
    "    model = NeuralNetworkModel().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train(dataloader, model, loss_fn, optimizer, device):\n",
    "        model.train()\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def validation(dataloader, model, loss_fn, device):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        validation_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                validation_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        validation_loss /= num_batches\n",
    "        correct /= size\n",
    "        return correct, validation_loss\n",
    "\n",
    "    epochs = max_iter\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "\n",
    "    if hptune:\n",
    "        accuracy, _ = validation(validation_dataloader, model, loss_fn, device)\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    if not hptune:\n",
    "        # Save the model\n",
    "        model_filename = \"model.pt\"\n",
    "        model_scripted = torch.jit.script(model)\n",
    "        model_scripted.save(model_filename)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "\n",
    "        # export json for preprocessing\n",
    "        preprocesinng_json = {c: transformers[c].seriarize_constants() for c in df_train.columns if c != LABEL_COLUMN}\n",
    "        preproc_json_filename = 'preprocessing.json'\n",
    "        with open(preproc_json_filename, 'w') as f:\n",
    "            json.dump(preprocesinng_json, f)\n",
    "        gcs_preprocessing_json_path = \"{}/{}\".format(job_dir, preproc_json_filename)\n",
    "\n",
    "        # send files to GCS\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        subprocess.check_call(['gsutil', 'cp', preproc_json_filename, gcs_preprocessing_json_path], stderr=sys.stdout)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "To run container-based cloud training, we define a Dockerfile.<br>\n",
    "Notice that we are using the predefined pytorch image from Artifact Registry, and installing additional libraries including `cloud-hypertune` and `torchtext` to the training image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-xla.1-11:latest\n",
    "RUN pip install -U fire cloudml-hypertune pandas==0.25.3\n",
    "RUN pip install -U torchtext==0.12.0\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**.<br>\n",
    "You use **Cloud Build** to build the image and push it your project's **Container Registry**. As you use the remote cloud service to build the image, you don't need a local installation of Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit an Vertex AI hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's build a yaml file to define our hyperparameter tuning job specs. <br>\n",
    "The training application has been designed to accept two hyperparameters that control our model:\n",
    "- Max Iterations\n",
    "- Batch Size\n",
    "\n",
    "The file below configures Vertex AI hypertuning to run up to 5 trials in parallel and to choose from two discrete values of `max_iter` and the linear range between `16` and `128` for `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"forestcover_tuning_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "cat <<EOF > $CONFIG_YAML\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: accuracy\n",
    "    goal: MAXIMIZE\n",
    "  parameters:\n",
    "  - parameterId: max_iter\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 10\n",
    "      - 20\n",
    "  - parameterId: batch_size\n",
    "    integerValueSpec:\n",
    "      minValue: 16\n",
    "      maxValue: 128\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:  \n",
    "  - machineSpec:\n",
    "      machineType: $MACHINE_TYPE\n",
    "    replicaCount: $REPLICA_COUNT\n",
    "    containerSpec:\n",
    "      imageUri: $IMAGE_URI\n",
    "      args:\n",
    "      - --job_dir=$JOB_DIR\n",
    "      - --training_dataset_path=$TRAINING_FILE_PATH\n",
    "      - --validation_dataset_path=$VALIDATION_FILE_PATH\n",
    "      - --hptune\n",
    "EOF\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=5 \\\n",
    "    --parallel-trial-count=5\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Vertex AI Training dashboard and view the progression of the HP tuning job under \"Hyperparameter Tuning Jobs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using GCP Console or programmatically using the following functions (note that this code supposes that the metrics that the hyperparameter tuning engine optimizes is maximized): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to wait for the hyperparameter job to complete before being able to retrieve the best job by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model with the best hyperparameters\n",
    "\n",
    "You can now retrain the model using the best hyperparameters and using combined training and validation splits as a training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best_trial.parameters[0].value)\n",
    "max_iter = int(best_trial.parameters[1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"JOB_VERTEX_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--job_dir={JOB_DIR},\\\n",
    "--training_dataset_path={TRAINING_FILE_PATH},\\\n",
    "--validation_dataset_path={VALIDATION_FILE_PATH},\\\n",
    "--batch_size={batch_size},\\\n",
    "--max_iter={max_iter},\\\n",
    "--nohptune\\\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the training output\n",
    "\n",
    "The training script saved the trained model as the 'model.pkl' in the `JOB_DIR` folder on GCS.\n",
    "\n",
    "**Note:** We need to wait for job triggered by the cell above to complete before running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to Vertex AI Prediction\n",
    "The model training is done. Now we will deploy the model using Vertex AI Predicion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_APP_FOLDER = \"predict_app\"\n",
    "os.makedirs(PREDICT_APP_FOLDER, exist_ok=True)\n",
    "os.environ[\"PREDICT_APP_FOLDER\"] = PREDICT_APP_FOLDER\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TorchServe handler\n",
    "We use Torchserve for the prediction handler. As we have custom preprocessing steps, we define a custom handler by subclassing `BaseHandler.` <br>\n",
    "\n",
    "You can create a custom handler by having a class with any name, but it must have an `initialize()` and a `handle()` method.\n",
    "- `initialize()`: this method is called when loading a model. We add a logic to load the state file in this function.\n",
    "- `handle()`: the main function of this class. We define steps in this function.\n",
    "\n",
    "And most cases, you will likely overwrite the `preprocess(),` `inference(),` and `postprocess()` functions to define custom preprocessing, inferencing, and postprocessing, respectively.<br>\n",
    "We also copy the `transform()` function from our preprocessing classes and define them as `_one_hot_transformer` and `_standard_scale_transformer,` as this python file can't accommodate multiple classes.\n",
    "\n",
    "For more detail, please refer to [the official TorchServe document](https://pytorch.org/serve/custom_service.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PREDICT_APP_FOLDER}/handler.py\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchtext.vocab import vocab\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "CATEGORICAL_FEATURES = [\"Wilderness_Area\", \"Soil_Type\"]\n",
    "NUMERICAL_FEATURES = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "    \n",
    "class ModelHandler(BaseHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._context = None\n",
    "        self.initialized = False\n",
    "        self.explain = False\n",
    "        self.target = 0\n",
    "\n",
    "\n",
    "    def _one_hot_transformer(self, feature, dictionary):\n",
    "        dictionary = vocab(OrderedDict([(cat, 1) for cat in dictionary]))\n",
    "        \n",
    "        indices = dictionary.lookup_indices(feature)\n",
    "        indices = torch.tensor(indices, dtype=torch.long)\n",
    "        one_hot = F.one_hot(indices, num_classes=len(dictionary.get_itos()))\n",
    "        return torch.tensor(one_hot).float()\n",
    "\n",
    "    def _standard_scale_transformer(self, feature, mean, std):\n",
    "        standardized = (feature - np.float64(mean)) / np.float64(std)\n",
    "        return torch.tensor(standardized)[:, None].float()    \n",
    "\n",
    "\n",
    "    def initialize(self, context):\n",
    "        self._context = context\n",
    "        self.initialized = True\n",
    "        \n",
    "        #  load the model\n",
    "        self.manifest = context.manifest\n",
    "\n",
    "        properties = context.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest['model']['serializedFile']\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.pth file\")\n",
    "\n",
    "        self.model = torch.jit.load(model_pt_path)\n",
    "        \n",
    "        # Read preprocesinng setting\n",
    "        preprocessing_json_path = os.path.join(model_dir, \"preprocessing.json\")\n",
    "        with open(preprocessing_json_path) as f:\n",
    "            self.preprocessing_json = json.load(f)\n",
    "        \n",
    "        self.initialized = True\n",
    "\n",
    "\n",
    "    def preprocess(self, _data):\n",
    "        data = _data[0]\n",
    "        \n",
    "        preprocessed_data = [self._one_hot_transformer(data[c_feature], self.preprocessing_json[c_feature][\"dictionary\"]) for c_feature in CATEGORICAL_FEATURES]\n",
    "        preprocessed_data.extend([self._standard_scale_transformer(data[n_feature], self.preprocessing_json[n_feature][\"mean\"], self.preprocessing_json[n_feature][\"std\"]) for n_feature in NUMERICAL_FEATURES])\n",
    "        preprocessed_data = torch.cat(preprocessed_data, 1)\n",
    "        \n",
    "        return preprocessed_data\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        model_output = self.model.forward(model_input)\n",
    "        return model_output\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        postprocess_output = torch.argmax(inference_output, dim=1)        \n",
    "        return [postprocess_output.cpu().numpy().tolist()]\n",
    "\n",
    "    def handle(self, data, context):\n",
    "        model_input = self.preprocess(data)\n",
    "        model_output = self.inference(model_input)\n",
    "        output = self.postprocess(model_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package up files and buld a .mar file for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the trained model and the preprocessing state JSON file that were created during the cloud training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $JOB_DIR/model.pt ./$PREDICT_APP_FOLDER\n",
    "!gsutil cp $JOB_DIR/preprocessing.json ./$PREDICT_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `torch-model-archiver` command to create a prediction package specifying the model file and the preprocessing state JSON file.\n",
    "\n",
    "For details, please refer to [the official README.md](https://github.com/pytorch/serve/blob/master/model-archiver/README.md#torch-model-archiver-for-torchserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver -f \\\n",
    "  --model-name model \\\n",
    "  --version 1.0 \\\n",
    "  --serialized-file ./$PREDICT_APP_FOLDER/model.pt \\\n",
    "  --handler ./$PREDICT_APP_FOLDER/handler.py \\\n",
    "  --extra-files ./$PREDICT_APP_FOLDER/preprocessing.json \\\n",
    "  --export-path=./$PREDICT_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Deploy\n",
    "Before deploying the model to the Vertex AI Prediction, let's test usging the local deployment functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_route = \"/ping\"\n",
    "predict_route = \"/predictions/model\"\n",
    "serving_container_ports = [8080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_container_image_uri = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.1-11:latest\"\n",
    ")\n",
    "\n",
    "local_model = LocalModel(\n",
    "    serving_container_image_uri=serving_container_image_uri,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./instances.json\n",
    "{\n",
    "    \"instances\": [{\"Elevation\":[2841.0], \n",
    "              \"Aspect\": [45.0],\n",
    "              \"Slope\":[0.0],\n",
    "              \"Horizontal_Distance_To_Hydrology\": [644.0], \n",
    "              \"Vertical_Distance_To_Hydrology\": [282.0],\n",
    "              \"Horizontal_Distance_To_Roadways\": [1376.0], \n",
    "              \"Hillshade_9am\": [218.0], \n",
    "              \"Hillshade_Noon\": [237.0], \n",
    "              \"Hillshade_3pm\": [156.0], \n",
    "              \"Horizontal_Distance_To_Fire_Points\": [1003.0],\n",
    "              \"Wilderness_Area\": [\"Commanche\"], \n",
    "              \"Soil_Type\": [\"C4758\"]}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=PREDICT_APP_FOLDER,\n",
    ") as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request_file=\"./instances.json\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the response from the local deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model to Vertex AI Prediction\n",
    "Now we deploy the model to Vertex AI Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "DEPLOY_MODEL_GCS_URI = f\"{ARTIFACT_STORE}/pytorch-model-deploy/{TIMESTAMP}\"\n",
    "\n",
    "!gsutil cp -r $PREDICT_APP_FOLDER $DEPLOY_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -al $DEPLOY_MODEL_GCS_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_display_name = f\"pytorch-model-{TIMESTAMP}\"\n",
    "model_description = (\n",
    "    \"PyTorch based forest cover classifier with the pre-built PyTorch image\"\n",
    ")\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=serving_container_image_uri,\n",
    "    artifact_uri=DEPLOY_MODEL_GCS_URI,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the uploaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_type = \"n1-standard-4\"\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    machine_type=machine_type, accelerator_type=None, accelerator_count=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve predictions\n",
    "#### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\n",
    "    {\n",
    "        \"Elevation\": [2841.0],\n",
    "        \"Aspect\": [45.0],\n",
    "        \"Slope\": [0.0],\n",
    "        \"Horizontal_Distance_To_Hydrology\": [644.0],\n",
    "        \"Vertical_Distance_To_Hydrology\": [282.0],\n",
    "        \"Horizontal_Distance_To_Roadways\": [1376.0],\n",
    "        \"Hillshade_9am\": [218.0],\n",
    "        \"Hillshade_Noon\": [237.0],\n",
    "        \"Hillshade_3pm\": [156.0],\n",
    "        \"Horizontal_Distance_To_Fire_Points\": [1003.0],\n",
    "        \"Wilderness_Area\": [\"Commanche\"],\n",
    "        \"Soil_Type\": [\"C4758\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint.predict(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
