{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "source": [
    "# Vertex pipelines\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "Use components from `google_cloud_pipeline_components` to create a Vertex Pipeline which will\n",
    "  1. train a custom model on Vertex AI\n",
    "  1. create an endpoint to host the model \n",
    "  1. upload the trained model and deploy to the endpoint for serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBRcgrOk7CUf"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to use the components defined in [`google_cloud_pipeline_components`](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud) in conjunction with an experimental `run_as_aiplatform_custom_job` method, to build a [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines) workflow that trains a [custom model](https://cloud.google.com/vertex-ai/docs/training/containers-overview), uploads the model, creates an endpoint, and deploys the model to the endpoint. \n",
    "\n",
    "We'll use the `kfp.v2.google.experimental.run_as_aiplatform_custom_job` method to train a custom model.\n",
    "\n",
    "The google cloud pipeline components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.2/). From this [github page]() you can also find other examples in how to build a Vertex pipeline with AutoML [here](https://github.com/GoogleCloudPlatform/ai-platform-samples/tree/master/ai-platform-unified/notebooks/official/pipelines). You can see other available methods from the [Vertex AI SDK](https://googleapis.dev/python/aiplatform/latest/aiplatform.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment and install necessary packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "yxtzwPPNZ-SH"
   },
   "outputs": [],
   "source": [
    "!pip3 install --user google-cloud-aiplatform --upgrade\n",
    "!pip3 install --user kfp google-cloud-pipeline-components --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages. Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "NN0mULkEeb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.6.4\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your environment variables\n",
    "Next, we'll set up our project variables, like GCP project ID, the bucket and region. Also, to avoid name collisions between resources created, we'll create a timestamp and append it onto the name of resources we create in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT = \"munn-sandbox\"\n",
    "BUCKET = \"munn-sandbox\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "We'll save pipeline artifacts in a directory called `vertex_pipelines` within our bucket. Validate access to your Cloud Storage bucket by examining its contents. It should be empty at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -la gs://$BUCKET/vertex_pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "UFDUBveR5UfJ"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4MjdglUT3Sw"
   },
   "source": [
    "## Define a pipeline that uses the components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65npyM9lYgtr"
   },
   "source": [
    "Set some variables that will be used in constructing the args passed to the custom training job and setting pipeline params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://munn-sandbox/taxifare/data/taxi-train-000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://munn-sandbox/taxifare/data/taxi-train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://munn-sandbox/taxifare/data/taxi-valid-000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://munn-sandbox/taxifare/data/taxi-valid*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpaD14TxZyVm"
   },
   "source": [
    "Next, you define a component with which the custom training job is run.  For this example, this component doesn't do anything (but run a print statement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "5m_ZU0GzBMRi"
   },
   "outputs": [],
   "source": [
    "@component\n",
    "def training_op(input1: str):\n",
    "    print(\"training task: {}\".format(input1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fjGiImBezMo"
   },
   "source": [
    "Now, you define the pipeline.  \n",
    "\n",
    "The `experimental.run_as_aiplatform_custom_job` method takes as args the component defined above, and the list of `worker_pool_specs`— in this case  one— with which the custom training job is configured. \n",
    "See [full function code here](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/google/experimental/custom_job.py)\n",
    "\n",
    "Then, [`google_cloud_pipeline_components`](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud) components are used to define the rest of the pipeline: upload the model, create an endpoint, and deploy the model to the endpoint. (While not shown in this example, the model deploy will create an endpoint if one is not provided.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory and job_name\n",
    "OUTDIR=f\"gs://{BUCKET}/taxifare/trained_model_{TIMESTAMP}\"\n",
    "JOB_NAME=f\"taxifare_{TIMESTAMP}\"\n",
    "\n",
    "PYTHON_PACKAGE_URIS=f\"gs://{BUCKET}/taxifare/taxifare_trainer-0.1.tar.gz\"\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-3:latest\"\n",
    "PYTHON_MODULE=\"trainer.task\"\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=50\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=5000\n",
    "NUM_EVALS=100\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=f\"gs://{BUCKET}/taxifare\"\n",
    "DATA_PATH=f\"{GCS_PROJECT_PATH}/data\"\n",
    "TRAIN_DATA_PATH=f\"{DATA_PATH}/taxi-train*\"\n",
    "EVAL_DATA_PATH=f\"{DATA_PATH}/taxi-valid*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "lwBLkQygbxjM"
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"taxifare--train-endpoint-deploy-\" + TIMESTAMP)\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    model_display_name: str = JOB_NAME,\n",
    "    serving_container_image_uri: str = \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest\",\n",
    "):\n",
    "    train_task = training_op(\"taxifare model training\")\n",
    "    experimental.run_as_aiplatform_custom_job(\n",
    "        train_task,\n",
    "        display_name=JOB_NAME,\n",
    "        worker_pool_specs=[\n",
    "            {\n",
    "                \"pythonPackageSpec\": {\n",
    "                    \"executor_image_uri\": PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\n",
    "                    \"package_uris\": [PYTHON_PACKAGE_URIS],\n",
    "                    \"python_module\": PYTHON_MODULE,\n",
    "                    \"args\": [\n",
    "                        f\"--eval_data_path={EVAL_DATA_PATH}\",\n",
    "                        f\"--output_dir={OUTDIR}\",\n",
    "                        f\"--train_data_path={TRAIN_DATA_PATH}\",\n",
    "                        f\"--batch_size={BATCH_SIZE}\",\n",
    "                        f\"--num_examples_to_train_on={NUM_EXAMPLES_TO_TRAIN_ON}\",\n",
    "                        f\"--num_evals={NUM_EVALS}\",\n",
    "                        f\"--nbuckets={NBUCKETS}\",\n",
    "                        f\"--lr={LR}\",\n",
    "                        f\"--nnsize={NNSIZE}\"\n",
    "                    ],                    \n",
    "                },\n",
    "                \"replica_count\": f\"{REPLICA_COUNT}\",\n",
    "                \"machineSpec\": {\n",
    "                    \"machineType\": f\"{MACHINE_TYPE}\",\n",
    "                },                \n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=f\"{OUTDIR}\",\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_environment_variables={\"NOT_USED\": \"NO_VALUE\"},\n",
    "    )\n",
    "    model_upload_op.after(train_task)\n",
    "\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "        project=project,\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_display_name,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hl1iYEKSzjP"
   },
   "source": [
    "## Compile and run the pipeline\n",
    "\n",
    "Now, you're ready to compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "ycRc83B6bbfO"
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "if not os.path.isdir(\"vertex_pipelines\"):\n",
    "    os.mkdir(\"vertex_pipelines\")\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"./vertex_pipelines/train_upload_deploy.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfNuzFswBB4g"
   },
   "source": [
    "The pipeline compilation generates the `train_upload_deploy.json` job spec file.\n",
    "\n",
    "Next, instantiate an API client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "Hl5Q74_gkW2c"
   },
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jrn6saiQsPh"
   },
   "source": [
    "Then, you run the defined pipeline like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "R4Ha4FoDQpkd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxifare--train-endpoint-deploy-20210707211120-20210707211152?project=munn-sandbox\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"./vertex_pipelines/train_upload_deploy.json\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/vertex_pipelines\",\n",
    "    parameter_values={\"project\": PROJECT_ID},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvBTCP318RKs"
   },
   "source": [
    "Click on the generated link to see your run in the Cloud Console.  It should look something like this:\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/mp/train_endpoint_deploy.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/mp/train_endpoint_deploy.png\" width=\"75%\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4jxmfyT26gj"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "- Delete Cloud Storage objects that were created.  Uncomment and run the command in the cell below **only if you are not using the `PIPELINE_ROOT` path for any other purpose**.\n",
    "- Delete your deployed model: first, undeploy it from its *endpoint*, then delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtZCXIi1aULZ"
   },
   "outputs": [],
   "source": [
    "# Warning: this command will delete ALL Cloud Storage objects under the PIPELINE_ROOT path.\n",
    "# ! gsutil -m rm -r $PIPELINE_ROOT"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-3.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
