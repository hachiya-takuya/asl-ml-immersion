{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow pipelines\n",
    "\n",
    "**Learning Objectives:**\n",
    "  1. Learn how to deploy a Kubeflow cluster on GCP\n",
    "  1. Learn how to create a experiment in Kubeflow\n",
    "  1. Learn how to package you code into a Kubeflow pipeline\n",
    "  1. Learn how to run a Kubeflow pipeline in a repeatable and traceable way\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will first setup a Kubeflow cluster on GCP.\n",
    "Then, we will create a Kubeflow experiment and a Kubflow pipeline from our taxifare machine learning code. At last, we will run the pipeline on the Kubeflow cluster, providing us with a reproducible and traceable way to execute machine learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /home/jupyter/.local/lib/python3.7/site-packages (1.7.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.34.0)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.7.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.8.2)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.9 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.6.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: kubernetes<13,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (12.0.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.16.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.2.12)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.41.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.12.8)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: absl-py<=0.11,>=0.9 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.11.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.10)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (1.31.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (3.0.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (49.6.0.post20210108)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (21.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2021.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (1.3.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (1.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (2.4.7)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (4.6.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.2.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.5.30)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from pydantic<2,>=1.8.2->kfp) (3.10.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (4.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.36.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp) (3.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a Kubeflow cluster on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy a [Kubeflow](https://www.kubeflow.org/) cluster\n",
    "in your GCP project, use the [AI Platform pipelines](https://console.cloud.google.com/ai-platform/pipelines):\n",
    "\n",
    "1. Go to [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines) in the GCP Console.\n",
    "1. Create a new instance\n",
    "2. Hit \"Configure\"\n",
    "3. Check the box \"Allow access to the following Cloud APIs\"\n",
    "1. Hit \"Create Cluster\"\n",
    "4. Hit \"Deploy\"\n",
    "\n",
    "When the cluster is ready, go back to the AI Platform pipelines page and click on \"SETTINGS\" entry for your cluster.\n",
    "This will bring up a pop up with code snippets on how to access the cluster \n",
    "programmatically. \n",
    "\n",
    "Copy the \"host\" entry and set the \"HOST\" variable below with that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"\"  # TODO: fill in the HOST information for the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your KFP cluster with a  Kubernetes secret\n",
    "\n",
    "If you run pipelines that requires calling any GCP services, you need to set the application default credential to a pipeline step by mounting the proper GCP service account token as a Kubernetes secret.\n",
    "\n",
    "First point your kubectl current context to your cluster. Go back to your [Kubeflow cluster dashboard](https://console.cloud.google.com/ai-platform/pipelines/clusters) or navigate to `Navigation menu > AI Platform > Pipelines` and look to see the cluster name, zone and namespace for the pipeline you deployed above. It's likely called `cluster-1` if this is the first AI Pipelines you've created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=dsparing-sandbox\n",
      "env: CLUSTER=cluster-1\n",
      "env: ZONE=us-central1-a\n",
      "env: NAMESPACE=default\n"
     ]
    }
   ],
   "source": [
    "PROJECT = !gcloud config get-value project  # noqa: E999\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT  # change if needed\n",
    "CLUSTER = \"cluster-1\"  # change if needed\n",
    "ZONE = \"us-central1-a\"  # change if needed\n",
    "NAMESPACE = \"default\"  # change if needed\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env CLUSTER=$CLUSTER\n",
    "%env ZONE=$ZONE\n",
    "%env NAMESPACE=$NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for cluster-1.\n"
     ]
    }
   ],
   "source": [
    "# Configure kubectl to connect with the cluster\n",
    "!gcloud container clusters get-credentials \"$CLUSTER\" --zone \"$ZONE\" --project \"$PROJECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a service account called `kfpdemo` with the necessary IAM permissions for our cluster secret. We'll give this service account permissions for any GCP services it might need. This `taxifare` pipeline needs access to Cloud Storage, so we'll give it the `storage.admin` role and `ml.admin`. Open a Cloud Shell and copy/paste this code in the terminal there.\n",
    "\n",
    "```bash\n",
    "PROJECT=$(gcloud config get-value project)\n",
    "\n",
    "# Create service account\n",
    "gcloud iam service-accounts create kfpdemo \\\n",
    "  --display-name kfpdemo --project $PROJECT\n",
    "\n",
    "# Grant permissions to the service account by binding roles\n",
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "    --member=serviceAccount:kfpdemo@$PROJECT.iam.gserviceaccount.com \\\n",
    "    --role=roles/storage.admin\n",
    "    \n",
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "    --member=serviceAccount:kfpdemo@$PROJECT.iam.gserviceaccount.com \\\n",
    "    --role=roles/ml.admin    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create and download a key for this service account and store the service account credential as a Kubernetes secret called `user-gcp-sa` in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud iam service-accounts keys create application_default_credentials.json \\\n",
    "  --iam-account kfpdemo@$PROJECT.iam.gserviceaccount.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_default_credentials.json\n"
     ]
    }
   ],
   "source": [
    "# Check that the key was downloaded correctly.\n",
    "!ls application_default_credentials.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/user-gcp-sa configured\n"
     ]
    }
   ],
   "source": [
    "# Create a k8s secret. If already exists, override.\n",
    "!kubectl create secret generic user-gcp-sa \\\n",
    "  --from-file=user-gcp-sa.json=application_default_credentials.json \\\n",
    "  -n $NAMESPACE --dry-run=client -o yaml  |  kubectl apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a Kubeflow client to pilot the Kubeflow cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host=HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the experiments that are running on this cluster. Since you just launched it, you should see only a single \"Default\" experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2021, 8, 27, 20, 26, 35, tzinfo=tzlocal()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': '37df4198-248e-459f-a799-ed46706e9d8e',\n",
       "                  'name': 'Default',\n",
       "                  'resource_references': None,\n",
       "                  'storage_state': 'STORAGESTATE_AVAILABLE'},\n",
       "                 {'created_at': datetime.datetime(2021, 8, 27, 20, 36, 43, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': '7a7fe7d9-18cc-47fc-aaad-8e21910453f7',\n",
       "                  'name': 'taxifare',\n",
       "                  'resource_references': None,\n",
       "                  'storage_state': 'STORAGESTATE_AVAILABLE'}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a 'taxifare' experiment where we could look at all the various runs of our taxifare pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://738bf02e57e2937c-dot-us-central1.pipelines.googleusercontent.com/#/experiments/details/7a7fe7d9-18cc-47fc-aaad-8e21910453f7\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp = client.create_experiment(name=\"taxifare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the experiment has been created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2021, 8, 27, 20, 26, 35, tzinfo=tzlocal()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': '37df4198-248e-459f-a799-ed46706e9d8e',\n",
       "                  'name': 'Default',\n",
       "                  'resource_references': None,\n",
       "                  'storage_state': 'STORAGESTATE_AVAILABLE'},\n",
       "                 {'created_at': datetime.datetime(2021, 8, 27, 20, 36, 43, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': '7a7fe7d9-18cc-47fc-aaad-8e21910453f7',\n",
       "                  'name': 'taxifare',\n",
       "                  'resource_references': None,\n",
       "                  'storage_state': 'STORAGESTATE_AVAILABLE'}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging your code into Kubeflow components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have packaged our taxifare ml pipeline into three components:\n",
    "* `./components/bq2gcs` that creates the training and evaluation data from BigQuery and exports it to GCS\n",
    "* `./components/trainjob` that launches the training container on AI-platform and exports the model\n",
    "* `./components/deploymodel` that deploys the trained model to AI-platform as a REST API\n",
    "\n",
    "Each of these components has been wrapped into a Docker container, in the same way we did with the taxifare training code in the previous lab.\n",
    "\n",
    "If you inspect the code in these folders, you'll notice that the `main.py` or `main.sh` files contain the code we previously executed in the notebooks (loading the data to GCS from BQ, or launching a training job to AI-platform, etc.). The last line in the `Dockerfile` tells you that these files are executed when the container is run. \n",
    "So we just packaged our ml code into light container images for reproducibility. \n",
    "\n",
    "We have made it simple for you to build the container images and push them to the Google Cloud image registry gcr.io in your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  157.2kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-5:latest\n",
      " ---> 19875ee1008d\n",
      "Step 2/4 : COPY . /code\n",
      " ---> Using cache\n",
      " ---> 9c12e76129be\n",
      "Step 3/4 : WORKDIR /code\n",
      " ---> Using cache\n",
      " ---> 0b77723599c9\n",
      "Step 4/4 : ENTRYPOINT [\"python3\", \"-m\", \"trainer.task\"]\n",
      " ---> Using cache\n",
      " ---> ec727c7b2e63\n",
      "Successfully built ec727c7b2e63\n",
      "Successfully tagged gcr.io/dsparing-sandbox/taxifare_training_container:latest\n"
     ]
    }
   ],
   "source": [
    "# Builds the taxifare trainer container in case you skipped the optional part\n",
    "# of lab 1\n",
    "!taxifare/scripts/build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/taxifare_training_container]\n",
      "\n",
      "\u001b[1B5a90caa8: Preparing \n",
      "\u001b[1B0326dd85: Preparing \n",
      "\u001b[1Bbe2a94be: Preparing \n",
      "\u001b[1B2667b401: Preparing \n",
      "\u001b[1B97d991c7: Preparing \n",
      "\u001b[1Bbdf9b557: Preparing \n",
      "\u001b[1Bdbc2b748: Preparing \n",
      "\u001b[1Bb8f29c2e: Preparing \n",
      "\u001b[1B7b2f7486: Preparing \n",
      "\u001b[1B506c54ba: Preparing \n",
      "\u001b[1B3dc8d38f: Preparing \n",
      "\u001b[1Bd7ce97e4: Preparing \n",
      "\u001b[1B97e5c777: Preparing \n",
      "\u001b[1B5dfd94f2: Preparing \n",
      "\u001b[1Bafebd1ec: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B7318f223: Preparing \n",
      "\u001b[1B3d35a813: Preparing \n",
      "\u001b[1Ba1af4c10: Preparing \n",
      "\u001b[1B9b09744f: Layer already exists \u001b[18A\u001b[2K\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:90f6400640d2e3c16d7e3d5bbeb4974ef1e9c99155fa2a67032608bab735d002 size: 4507\n"
     ]
    }
   ],
   "source": [
    "# Pushes the taxifare trainer container to gcr/io\n",
    "!taxifare/scripts/push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make[1]: Entering directory '/home/jupyter/asl-ml-immersion/notebooks/building_production_ml_systems/solutions/pipelines/components/bq2gcs'\n",
      "rm: cannot remove './venv': No such file or directory\n",
      "OK\n",
      "Sending build context to Docker daemon   21.5kB\n",
      "Step 1/6 : FROM google/cloud-sdk:latest\n",
      " ---> 915a516535e8\n",
      "Step 2/6 : RUN apt-get update &&     apt-get install --yes python3-pip\n",
      " ---> Using cache\n",
      " ---> 0f653294e07c\n",
      "Step 3/6 : COPY . /code\n",
      " ---> Using cache\n",
      " ---> 7d8f8d185c30\n",
      "Step 4/6 : WORKDIR /code\n",
      " ---> Using cache\n",
      " ---> 20d39822bdb4\n",
      "Step 5/6 : RUN pip3 install google-cloud-bigquery\n",
      " ---> Using cache\n",
      " ---> a1baf2091090\n",
      "Step 6/6 : ENTRYPOINT [\"python3\", \"./main.py\"]\n",
      " ---> Using cache\n",
      " ---> 05e9191c9619\n",
      "Successfully built 05e9191c9619\n",
      "Successfully tagged gcr.io/dsparing-sandbox/taxifare-bq2gcs:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/taxifare-bq2gcs]\n",
      "\n",
      "\u001b[1B038e5a12: Preparing \n",
      "\u001b[1B7ee6f1b4: Preparing \n",
      "\u001b[1Bced31aad: Preparing \n",
      "\u001b[1Be268b455: Preparing \n",
      "\u001b[1B15a7c280: Preparing \n",
      "\u001b[1B9ae3a881: Preparing \n",
      "\u001b[1B24ad8c63: Preparing \n",
      "\u001b[1Bd95c5384: Preparing \n",
      "\u001b[1Bd7a1159c: Preparing \n",
      "\u001b[1Bd1217615: Preparing \n",
      "\u001b[1Bc1bc2645: Layer already exists \u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:2b9f25d58c03019983d785b6fd7910e61d886d9d84e96781aade0ae05a50e020 size: 2633\n",
      "make[1]: Leaving directory '/home/jupyter/asl-ml-immersion/notebooks/building_production_ml_systems/solutions/pipelines/components/bq2gcs'\n",
      "make[1]: Entering directory '/home/jupyter/asl-ml-immersion/notebooks/building_production_ml_systems/solutions/pipelines/components/trainjob'\n",
      "rm: cannot remove './venv': No such file or directory\n",
      "OK\n",
      "Sending build context to Docker daemon  14.85kB\n",
      "Step 1/5 : FROM google/cloud-sdk:latest\n",
      " ---> 915a516535e8\n",
      "Step 2/5 : COPY . /code\n",
      " ---> Using cache\n",
      " ---> f598ddbd44a2\n",
      "Step 3/5 : WORKDIR /code\n",
      " ---> Using cache\n",
      " ---> 79f9d21c1bcf\n",
      "Step 4/5 : RUN pip3 install cloudml-hypertune\n",
      " ---> Using cache\n",
      " ---> 9edc5e05ae65\n",
      "Step 5/5 : ENTRYPOINT [\"./main.sh\"]\n",
      " ---> Using cache\n",
      " ---> ae939b43e795\n",
      "Successfully built ae939b43e795\n",
      "Successfully tagged gcr.io/dsparing-sandbox/taxifare-trainjob:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/taxifare-trainjob]\n",
      "\n",
      "\u001b[1B9e8dbe19: Preparing \n",
      "\u001b[1Bf0a19019: Preparing \n",
      "\u001b[1Be268b455: Preparing \n",
      "\u001b[1B15a7c280: Preparing \n",
      "\u001b[1B9ae3a881: Preparing \n",
      "\u001b[1B24ad8c63: Preparing \n",
      "\u001b[1Bd95c5384: Preparing \n",
      "\u001b[1Bd7a1159c: Preparing \n",
      "\u001b[1Bd1217615: Preparing \n",
      "\u001b[2Bd1217615: Layer already exists \u001b[6A\u001b[2K\u001b[5A\u001b[2Klatest: digest: sha256:4d28b7a097e81f911782a0101b395b2c3d56f83b7a55bddd8a3a83d2ab3e18b6 size: 2420\n",
      "make[1]: Leaving directory '/home/jupyter/asl-ml-immersion/notebooks/building_production_ml_systems/solutions/pipelines/components/trainjob'\n",
      "make[1]: Entering directory '/home/jupyter/asl-ml-immersion/notebooks/building_production_ml_systems/solutions/pipelines/components/deploymodel'\n",
      "rm: cannot remove './venv': No such file or directory\n",
      "OK\n",
      "Sending build context to Docker daemon  14.85kB\n",
      "Step 1/4 : FROM google/cloud-sdk:latest\n",
      " ---> 915a516535e8\n",
      "Step 2/4 : COPY . /code\n",
      " ---> Using cache\n",
      " ---> 8ce1e25d9ba8\n",
      "Step 3/4 : WORKDIR /code\n",
      " ---> Using cache\n",
      " ---> 9b585070839b\n",
      "Step 4/4 : ENTRYPOINT [\"./main.sh\"]\n",
      " ---> Using cache\n",
      " ---> 0559c7f14028\n",
      "Successfully built 0559c7f14028\n",
      "Successfully tagged gcr.io/dsparing-sandbox/taxifare-deploymodel:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/dsparing-sandbox/taxifare-deploymodel]\n",
      "\n",
      "\u001b[1B539f9c38: Preparing \n",
      "\u001b[1Be268b455: Preparing \n",
      "\u001b[1B15a7c280: Preparing \n",
      "\u001b[1B9ae3a881: Preparing \n",
      "\u001b[1B24ad8c63: Preparing \n",
      "\u001b[1Bd95c5384: Preparing \n",
      "\u001b[1Bd7a1159c: Preparing \n",
      "\u001b[1Bd1217615: Preparing \n",
      "\u001b[1Bc1bc2645: Layer already exists \u001b[5A\u001b[2K\u001b[3A\u001b[2Klatest: digest: sha256:6028a2f793dc35ee033376f1c43294cfdb0e3128443b6dbea7836393d6f53fd2 size: 2211\n",
      "make[1]: Leaving directory '/home/jupyter/asl-ml-immersion/notebooks/building_production_ml_systems/solutions/pipelines/components/deploymodel'\n"
     ]
    }
   ],
   "source": [
    "# Builds the KF component containers and push them to gcr/io\n",
    "!cd pipelines && make components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the container images are pushed to the [registry in your project](https://console.cloud.google.com/gcr), we need to create yaml files describing to Kubeflow how to use these containers. It boils down essentially to\n",
    "* describing what arguments Kubeflow needs to pass to the containers when it runs them\n",
    "* telling Kubeflow where to fetch the corresponding Docker images\n",
    "\n",
    "In the cells below, we have three of these \"Kubeflow component description files\", one for each of our components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT: Modify the image URI in the cell \n",
    "below to reflect that you pushed the images into the gcr.io associated with your project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bq2gcs.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile bq2gcs.yaml\n",
    "\n",
    "name: bq2gcs\n",
    "    \n",
    "description: |\n",
    "    This component creates the training and\n",
    "    validation datasets as BiqQuery tables and export\n",
    "    them into a Google Cloud Storage bucket at\n",
    "    gs://qwiklabs-gcp-00-568a75dfa3e1/taxifare/data.\n",
    "        \n",
    "inputs:\n",
    "    - {name: Input Bucket , type: String, description: 'GCS directory path.'}\n",
    "\n",
    "implementation:\n",
    "    container:\n",
    "        image: gcr.io/qwiklabs-gcp-00-568a75dfa3e1/taxifare-bq2gcs\n",
    "        args: [\"--bucket\", {inputValue: Input Bucket}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainjob.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainjob.yaml\n",
    "\n",
    "name: trainjob\n",
    "    \n",
    "description: |\n",
    "    This component trains a model to predict that taxi fare in NY.\n",
    "    It takes as argument a GCS bucket and expects its training and\n",
    "    eval data to be at gs://<BUCKET>/taxifare/data/ and will export\n",
    "    the trained model at  gs://<BUCKET>/taxifare/model/.\n",
    "        \n",
    "inputs:\n",
    "    - {name: Input Bucket , type: String, description: 'GCS directory path.'}\n",
    "\n",
    "implementation:\n",
    "    container:\n",
    "        image: gcr.io/qwiklabs-gcp-00-568a75dfa3e1/taxifare-trainjob\n",
    "        args: [{inputValue: Input Bucket}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploymodel.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploymodel.yaml\n",
    "\n",
    "name: deploymodel\n",
    "    \n",
    "description: |\n",
    "    This component deploys a trained taxifare model on GCP as taxifare:dnn.\n",
    "    It takes as argument a GCS bucket and expects the model to deploy \n",
    "    to be found at gs://<BUCKET>/taxifare/model/export/savedmodel/\n",
    "        \n",
    "inputs:\n",
    "    - {name: Input Bucket , type: String, description: 'GCS directory path.'}\n",
    "\n",
    "implementation:\n",
    "    container:\n",
    "        image: gcr.io/qwiklabs-gcp-00-568a75dfa3e1/taxifare-deploymodel\n",
    "        args: [{inputValue: Input Bucket}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a kubeflow pipeline by decorating a regular function with the\n",
    "`@dsl.pipeline` decorator. Now the arguments of this decorated function will be\n",
    "the input parameters of the Kubeflow pipeline.\n",
    "\n",
    "Inside the function, we describe the pipeline by\n",
    "* loading the yaml component files we created above into a Kubeflow `op`\n",
    "* specifying the order into which the Kubeflow ops should be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    trainjob_op = comp.load_component_from_file(TRAINJOB_YAML)\\n    trainjob = trainjob_op(\\n        input_bucket=gcs_bucket_name,\\n    )\\n\\n    deploymodel_op = comp.load_component_from_file(DEPLOYMODEL_YAML)\\n    deploymodel = deploymodel_op(\\n        input_bucket=gcs_bucket_name,\\n    )\\n\\n    trainjob.after(bq2gcs)\\n    deploymodel.after(trainjob)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 3\n",
    "PIPELINE_TAR = \"taxifare.tar.gz\"\n",
    "BQ2GCS_YAML = \"./bq2gcs.yaml\"\n",
    "TRAINJOB_YAML = \"./trainjob.yaml\"\n",
    "DEPLOYMODEL_YAML = \"./deploymodel.yaml\"\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"Taxifare\",\n",
    "    description=\"Train a ml model to predict the taxi fare in NY\",\n",
    ")\n",
    "def pipeline(gcs_bucket_name=\"<bucket where data and model will be exported>\"):\n",
    "\n",
    "    bq2gcs_op = comp.load_component_from_file(BQ2GCS_YAML)\n",
    "    bq2gcs = bq2gcs_op(\n",
    "        input_bucket=gcs_bucket_name,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    trainjob_op = comp.load_component_from_file(TRAINJOB_YAML)\n",
    "    trainjob = trainjob_op(\n",
    "        input_bucket=gcs_bucket_name,\n",
    "    )\n",
    "\n",
    "    deploymodel_op = comp.load_component_from_file(DEPLOYMODEL_YAML)\n",
    "    deploymodel = deploymodel_op(\n",
    "        input_bucket=gcs_bucket_name,\n",
    "    )\n",
    "\n",
    "    trainjob.after(bq2gcs)\n",
    "    deploymodel.after(trainjob)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline function above is then used by the Kubeflow compiler to create a Kubeflow pipeline artifact that can be either uploaded to the Kubeflow cluster from the UI, or programatically, as we will do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/components/_components.py:175: FutureWarning: Container component must specify command to be compatible with KFP v2 compatible mode and emissary executor, which will be the default executor for KFP v2.https://www.kubeflow.org/docs/components/pipelines/installation/choose-executor/\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline, PIPELINE_TAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxifare.tar.gz\n"
     ]
    }
   ],
   "source": [
    "ls $PIPELINE_TAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you untar and uzip this pipeline artifact, you'll see that the compiler has transformed the\n",
    "Python description of the pipeline into yaml description!\n",
    "\n",
    "Now let's feed Kubeflow with our pipeline and run it using our client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://738bf02e57e2937c-dot-us-central1.pipelines.googleusercontent.com/#/runs/details/7d76b668-fefd-4d89-bf43-2b2d2f7433e6\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 4\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=exp.id,\n",
    "    job_name=\"taxifare\",\n",
    "    pipeline_package_path=\"taxifare.tar.gz\",\n",
    "    params={\n",
    "        \"gcs_bucket_name\": BUCKET,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the link to monitor the run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the runs are nicely organized under the experiment in the UI, and new runs can be either manually launched or scheduled through the UI in a completely repeatable and traceable way!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
