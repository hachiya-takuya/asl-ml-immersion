{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edea9eb-383b-49db-8623-c8b08c6ca501",
   "metadata": {},
   "source": [
    "# Diffusion Models for Image Generation \n",
    "\n",
    "Generative image models (and generative AI as a whole) have gained significant attention over the last few years. Model types you may have heard of like GANs, have shown success in creating realistic looking images. Diffusion models are a relatively new class of models and have recently demonstrated state-of-the-art performance with image generation. Diffusion models were [first conceptualized for image generation in 2015](https://arxiv.org/pdf/1503.03585.pdf) and now underpin many of the leading image generation systems. Diffusion models consist of two processes:\n",
    "\n",
    "* The forward (noising) process: Slowly add noise to an image, destroying structure in the data distribution \n",
    "* The reverse (de-noising) process: Learn the reverse diffusion process, yielding a generative model of the data\n",
    "\n",
    "In other words, the forward process adds random noise to an image and the reverse process learns to remove noise from an image. After training, we can use the model from the reverse process to iteratively remove noise from a random-noise starting point and generate images.\n",
    "\n",
    "In this lab, you will:\n",
    "* Load the dataset \n",
    "* Define the de-noinsing model (U-net with residual connections) \n",
    "* Define the diffusion model (forward and reverse processes) \n",
    "* Train the diffusion model and visualize results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c765cd6-ee12-4acc-9b6b-8402f30f3637",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48cd06-8e9f-47e0-915f-30f65f904835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79e03c-6d8a-4407-8b82-5a9ae8bf350c",
   "metadata": {},
   "source": [
    "### Choose dataset\n",
    "This lab can be ran with either the flowers dataset or mnist dataset. **NOTE**: To train the model on the flowers dataset, it requires a GPU. To train a model faster, or without a GPU, choose the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f552b91-5e09-4131-bdad-6d6bb4af0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_choice = \"flowers\"\n",
    "# dataset_choice = 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba120d-26b6-4bf5-a6b9-038e17af5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_choice == \"flowers\":\n",
    "    IMAGE_SIZE = 64\n",
    "    IMAGE_CHANNELS = 3\n",
    "    WIDTHS = [64, 128, 128, 256]\n",
    "    DATASET_NAME = \"oxford_flowers102\"\n",
    "    DATASET_SPLIT = \"test\"\n",
    "    CENTRAL_CROP = 0.8\n",
    "\n",
    "elif dataset_choice == \"mnist\":\n",
    "    IMAGE_SIZE = 28\n",
    "    IMAGE_CHANNELS = 1\n",
    "    WIDTHS = [8, 16, 32]\n",
    "    NUM_EXAMPLES = 10000\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Please specify dataset_choice in the cell above.\")\n",
    "\n",
    "BLOCK_DEPTH = 2\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "MIN_SIGNAL_RATE = 0.02\n",
    "MAX_SIGNAL_RATE = 0.95\n",
    "PLOT_DIFFUSION_STEPS = 25\n",
    "EMA = 0.999\n",
    "OUTPUT_DIR = \"diffusion_model\"\n",
    "CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")\n",
    "\n",
    "print(f\"Using dataset: {dataset_choice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21834ae-5f34-4e28-be06-f32bfadb6642",
   "metadata": {},
   "source": [
    "### Load dataset \n",
    "\n",
    "Define a function to load the dataset. This lab uses either the [Oxford Flower Dataset](https://www.tensorflow.org/datasets/catalog/oxford_flowers102) or the MNIST dataset. The flowers dataset contains images of flowers commonly found in the UK, and the MNIST dataset contains grayscale images of handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe069e4-e8c5-4577-acf2-e496d8adf689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf data ingest for flowers dataset\n",
    "def _preprocess_flowers(data):\n",
    "    image = data[\"image\"]\n",
    "    image = tf.image.central_crop(image, CENTRAL_CROP)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(\n",
    "        image, size=[IMAGE_SIZE, IMAGE_SIZE], antialias=True\n",
    "    )\n",
    "    return tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def create_dataset_flowers(batch_size):\n",
    "    ds = tfds.load(DATASET_NAME, split=DATASET_SPLIT)\n",
    "    ds = ds.map(\n",
    "        _preprocess_flowers, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).cache()\n",
    "    return ds.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# tf data ingest for mnist dataset\n",
    "def _preprocess_mnist(image):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    return tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def create_dataset_mnist(batch_size):\n",
    "    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    ds = tf.data.Dataset.from_tensor_slices(X_train[:NUM_EXAMPLES])\n",
    "    ds = ds.map(_preprocess_mnist, num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
    "    return ds.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6368afc-cdca-4aa7-8438-eef7980bac1b",
   "metadata": {},
   "source": [
    "### Forward Diffusion Process\n",
    "We will have a diffusion process starting at time = 0, and ending at time = 1. At each diffusion time, we will have a noise level `noise_rate` and a signal level `signal_rate`. How do we know the noise and signal level for a given diffusion time? For this, we will use a simple cosine schedule as defined in `diffusion_schedule()`. We generate the noisy image (for the forward diffusion processes) by weighting random noise by the noise rate, and the training image by the signal rate, then adding them together.\n",
    "\n",
    "**Note**: This code needs to be implemented in a training step of a subclasses model eventually. We are only running it here locally first to visualize the forward diffusion process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac64b9a-2582-462e-bf09-81b62f01d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_schedule(\n",
    "    diffusion_times, max_signal_rate=0.98, min_signal_rate=0.02\n",
    "):\n",
    "    # diffusion times -> angles\n",
    "    start_angle = tf.acos(max_signal_rate)\n",
    "    end_angle = tf.acos(min_signal_rate)\n",
    "\n",
    "    diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "    # angles -> signal and noise rates\n",
    "    signal_rates = tf.cos(diffusion_angles)\n",
    "    noise_rates = tf.sin(diffusion_angles)\n",
    "\n",
    "    return noise_rates, signal_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87f587-95b1-409f-8818-01f5a08f35a6",
   "metadata": {},
   "source": [
    "### Visualize forward diffusion process\n",
    "Apply this forward diffusion to a single image and visualize this forward diffusion of adding guassian noise at randomly sampled diffusion times between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef7a5a-3696-41d5-a2f3-f11dde03ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of data\n",
    "if dataset_choice == \"mnist\":\n",
    "    train_ds = create_dataset_mnist(BATCH_SIZE)\n",
    "elif dataset_choice == \"flowers\":\n",
    "    train_ds = create_dataset_flowers(BATCH_SIZE)\n",
    "\n",
    "ds_iter = train_ds.as_numpy_iterator()\n",
    "data_batch = ds_iter.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb950ac-0250-4c22-bb17-b14a9dbab4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random noise the same size as input batch of images\n",
    "noises = tf.random.normal(shape=data_batch.shape)\n",
    "\n",
    "# random sample of diffusion times between 0 and 1\n",
    "diffusion_times = tf.random.uniform(\n",
    "    shape=(data_batch.shape[0], 1, 1, 1), minval=0.0, maxval=1.0\n",
    ")\n",
    "\n",
    "# compute noise and signal rates for each respective diffusion time\n",
    "noise_rates, signal_rates = diffusion_schedule(diffusion_times)\n",
    "\n",
    "# create noisy images\n",
    "noisy_images = signal_rates * data_batch + noise_rates * noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3ae14-8b19-4377-8cb7-54e0e9ef0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original images and noisy images\n",
    "num_rows = 32\n",
    "num_cols = 2\n",
    "plt.figure(figsize=(8, 64))\n",
    "index = 0\n",
    "for row in range(num_rows - 1):\n",
    "    plt.subplot(num_rows, num_cols, index + 1)\n",
    "    plt.imshow(data_batch[row], cmap=\"gray\")\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(num_rows, num_cols, index + 2)\n",
    "    plt.imshow(noisy_images[row], cmap=\"gray\")\n",
    "    plt.title(\n",
    "        f\"t={tf.squeeze(diffusion_times[row]):.2f} | signal={tf.squeeze(signal_rates[row]):.2f} | noise={tf.squeeze(noise_rates[row]):.2f}\"\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    index += 2\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea957ec7-25f2-4fc9-aa19-5778ab0032dc",
   "metadata": {},
   "source": [
    "As you can see visually, a small diffusion time (high signal rate, low noise rate) only adds a little noise and we can still (even visually) see what the original image is. Larger diffusion times (low signal rate, high noise rate) diffuse more \"aggresively\" and we can visually see the structure of the initial image is essentially, completely gone. \n",
    "\n",
    "**NOTE** You can run the previous two cells again to visualize another forward diffusion process, since diffusion times are randomly sampled.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe9716-3832-46f6-ad04-ebdbfe0c20fe",
   "metadata": {},
   "source": [
    "### De-noinsing Model Architecture\n",
    "\n",
    "Now we need to specify the neural network that will be used for denoising (predicting the noise added to a given image). U-Net is a popular semantic segmentation architecture, whose main idea is that it progressively downsamples and then upsamples its input image, and adds skip connections between layers having the same resolution.\n",
    "\n",
    "Our U-Net takes two inputs: the noisy images and the variances of their noise components. The variances are required since denoising a signal requires different operations at different levels of noise. In other words, the model needs to learn how to denoise at different noise rates so we will embed noise variance in such a way that different noise variances are represented differently. We will transform the noise variances using sinusoidal embeddings. This is quite similar to positional encodings used in transformers. The output of the model is the predicted noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6320d8f-73a7-4979-9c74-4053dfa5a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer for embedding noise rate variance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, embedding_dim=32, min_freq=1.0, max_freq=1000.0, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.frequencies = tf.exp(\n",
    "            tf.linspace(\n",
    "                tf.math.log(min_freq), tf.math.log(max_freq), embedding_dim // 2\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.angular_speeds = 2.0 * math.pi * self.frequencies\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "        emb = tf.concat(\n",
    "            [\n",
    "                tf.sin(self.angular_speeds * inputs),\n",
    "                tf.cos(self.angular_speeds * inputs),\n",
    "            ],\n",
    "            axis=3,\n",
    "        )\n",
    "        return emb\n",
    "\n",
    "\n",
    "def ResidualBlock(width):\n",
    "    \"\"\"\n",
    "    Batch norm followed by two conv layers to specified width.\n",
    "    Input also skips these layers and is added to output.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = tf.keras.layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = tf.keras.layers.Conv2D(\n",
    "            width,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "            activation=tf.keras.activations.swish,\n",
    "        )(x)\n",
    "        x = tf.keras.layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = tf.keras.layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "# Downsampling\n",
    "def DownBlock(width, block_depth):\n",
    "    \"\"\"\n",
    "    Component of down stack of residual U-Net. Stacks block_depth ResidualBlocks to specified\n",
    "    width, followed by Average Pooling for dimensionality reduction i.e. downsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = tf.keras.layers.AveragePooling2D(pool_size=2)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "# Upsampling\n",
    "def UpBlock(width, block_depth):\n",
    "    \"\"\"\n",
    "    Component of up stack of residual U-Net. Applies bilinear upsampling and\n",
    "    stacks block_depth ResidualBlocks to specified width. Concats skip connections\n",
    "    from same resolution as downstack.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = tf.keras.layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = tf.keras.layers.Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def get_network(image_size, image_channels, widths, block_depth):\n",
    "    noisy_images = tf.keras.Input(\n",
    "        shape=(image_size, image_size, image_channels)\n",
    "    )\n",
    "    noise_variances = tf.keras.Input(shape=(1, 1, 1))\n",
    "\n",
    "    e = SinEmbedding()(noise_variances)\n",
    "    e = tf.keras.layers.UpSampling2D(size=image_size, interpolation=\"nearest\")(\n",
    "        e\n",
    "    )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(widths[0], kernel_size=1)(noisy_images)\n",
    "    x = tf.keras.layers.Concatenate()([x, e])\n",
    "\n",
    "    skips = []\n",
    "    for width in widths[:-1]:\n",
    "        x = DownBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    for _ in range(block_depth):\n",
    "        x = ResidualBlock(widths[-1])(x)\n",
    "\n",
    "    for width in reversed(widths[:-1]):\n",
    "        x = UpBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        image_channels, kernel_size=1, kernel_initializer=\"zeros\"\n",
    "    )(x)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        [noisy_images, noise_variances], x, name=\"residual_unet\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9788f-df46-45dd-a96f-25e37c7f399c",
   "metadata": {},
   "source": [
    "### Diffusion Model\n",
    "\n",
    "#### Training process\n",
    "When subclassing `tf.keras.Model` the core logic of defining a training step is in `train_step()`. A single training step in our diffusion model does the following:\n",
    "* Normalize the images and samples random Gaussian noise \n",
    "* Samples noise/signal rates from sampled diffusion times \n",
    "* Mixes the images with the noise according to noise/signal rates\n",
    "* Sends the noisy images through the de-noising network (`denoise()`)\n",
    "* Computes loss between predicted noise and noise \n",
    "* Updates model parameters \n",
    "\n",
    "\n",
    "#### Sampling/Generating (reverse diffusion)\n",
    "When sampling or generating via reverse diffusion, the goal is the remove noise from an image using the trained model. At each step, it takes the previous estimate of the noisy image and separates it into image and noise using the trained network. Then it recombines these components using the signal and noise rate of the next step. This process occurs while iterating over diffusion times in reverse (i.e. 1 to 0). This logic is implemented in `reverse_diffusion()`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118c008-0eda-4f98-80a6-e29fa50e76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        image_channels,\n",
    "        widths,\n",
    "        block_depth,\n",
    "        batch_size,\n",
    "        min_signal_rate,\n",
    "        max_signal_rate,\n",
    "        ema,\n",
    "        plot_diffusion_steps,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.image_channels = image_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.min_signal_rate = min_signal_rate\n",
    "        self.max_signal_rate = max_signal_rate\n",
    "        self.plot_diffusion_steps = plot_diffusion_steps\n",
    "        self.ema = ema\n",
    "\n",
    "        self.normalizer = tf.keras.layers.Normalization()\n",
    "        self.network = get_network(\n",
    "            image_size, image_channels, widths, block_depth\n",
    "        )\n",
    "        self.ema_network = tf.keras.models.clone_model(self.network)\n",
    "\n",
    "    def denormalize(self, images):\n",
    "        # convert the pixel values back to 0-1 range\n",
    "        images = self.normalizer.mean + images * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(images, 0.0, 1.0)\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # diffusion times -> angles\n",
    "        start_angle = tf.acos(self.max_signal_rate)\n",
    "        end_angle = tf.acos(self.min_signal_rate)\n",
    "\n",
    "        diffusion_angles = start_angle + diffusion_times * (\n",
    "            end_angle - start_angle\n",
    "        )\n",
    "\n",
    "        # angles -> signal and noise rates\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_images, noise_rates, signal_rates, training):\n",
    "        if training:\n",
    "            network = self.network\n",
    "\n",
    "        # Use EMA network for generation\n",
    "        else:\n",
    "            network = self.ema_network\n",
    "\n",
    "        # predict noise component and calculate the image component using it\n",
    "        pred_noises = network(\n",
    "            [noisy_images, noise_rates**2], training=training\n",
    "        )\n",
    "        pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_images\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        # reverse diffusion = sampling\n",
    "        num_images = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # at the first sampling step, noisy_image is pure noise\n",
    "        # signal rate is assumed to be min_signal_rate\n",
    "        next_noisy_images = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_images = next_noisy_images\n",
    "\n",
    "            # separate the current noisy image to its components\n",
    "            diffusion_times = tf.ones((num_images, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            # network used in eval mode\n",
    "\n",
    "            # remix the predicted components using the next signal and noise rates\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_noisy_images = (\n",
    "                next_signal_rates * pred_images + next_noise_rates * pred_noises\n",
    "            )\n",
    "            # this new noisy image will be used in the next step\n",
    "\n",
    "        return pred_images\n",
    "\n",
    "    def generate(self, num_images, diffusion_steps):\n",
    "        # noise -> images -> denormalized images\n",
    "        initial_noise = tf.random.normal(\n",
    "            shape=(\n",
    "                num_images,\n",
    "                self.image_size,\n",
    "                self.image_size,\n",
    "                self.image_channels,\n",
    "            )\n",
    "        )\n",
    "        generated_images = self.reverse_diffusion(\n",
    "            initial_noise, diffusion_steps\n",
    "        )\n",
    "        generated_images = self.denormalize(generated_images)\n",
    "        return generated_images\n",
    "\n",
    "    def train_step(self, images):\n",
    "        images = self.normalizer(images, training=True)\n",
    "        noises = tf.random.normal(\n",
    "            shape=(\n",
    "                self.batch_size,\n",
    "                self.image_size,\n",
    "                self.image_size,\n",
    "                self.image_channels,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(self.batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_noises, pred_images = self.denoise(\n",
    "                noisy_images, noise_rates, signal_rates, training=True\n",
    "            )\n",
    "\n",
    "            noise_loss = self.loss(noises, pred_noises)  # used for training\n",
    "\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # track the exponential moving averages of weights\n",
    "        for weight, ema_weight in zip(\n",
    "            self.network.weights, self.ema_network.weights\n",
    "        ):\n",
    "            ema_weight.assign(self.ema * ema_weight + (1 - self.ema) * weight)\n",
    "\n",
    "        return {\"noise_loss\": noise_loss}\n",
    "\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6):\n",
    "        # plot random generated images for visual evaluation of generation quality\n",
    "        generated_images = self.generate(\n",
    "            num_images=num_rows * num_cols,\n",
    "            diffusion_steps=self.plot_diffusion_steps,\n",
    "        )\n",
    "\n",
    "        # plot in grayscale for single channel images\n",
    "        cmap = None\n",
    "        if self.image_channels == 1:\n",
    "            cmap = \"gray\"\n",
    "\n",
    "        plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                index = row * num_cols + col\n",
    "                plt.subplot(num_rows, num_cols, index + 1)\n",
    "                plt.imshow(generated_images[index], cmap=cmap)\n",
    "                plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516bf100-2b61-4e8d-b811-3da620640470",
   "metadata": {},
   "source": [
    "### Train the model \n",
    "Before training the model, you need to compile it and specify an optimizer and loss function. We will simply use Adam as an optimizer and mean squared error to compute the loss between the actual noise added to an image and the predicted noise. \n",
    "\n",
    "The model has callbacks that will generate and display images after each epoch of model training, and save checkpoints of the best model (lowest loss).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cd070-b66c-446c-8efd-cd5db4d76a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModel(\n",
    "    image_size=IMAGE_SIZE,\n",
    "    image_channels=IMAGE_CHANNELS,\n",
    "    widths=WIDTHS,\n",
    "    block_depth=BLOCK_DEPTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    min_signal_rate=MIN_SIGNAL_RATE,\n",
    "    max_signal_rate=MAX_SIGNAL_RATE,\n",
    "    plot_diffusion_steps=PLOT_DIFFUSION_STEPS,\n",
    "    ema=EMA,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# compute mean/variance of dataset for normalization layer\n",
    "model.normalizer.adapt(train_ds)\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            CHECKPOINT_DIR,\n",
    "            monitor=\"noise_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028698a-967b-49bb-805e-c45f65ddda11",
   "metadata": {},
   "source": [
    "Congrats! You've succesfully trained a diffusion model to generate images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf9875-c1f4-406c-bcb4-33b8f60cfbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
