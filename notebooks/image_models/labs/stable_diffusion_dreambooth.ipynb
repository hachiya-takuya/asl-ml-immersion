{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE0kF9geMdNX"
   },
   "source": [
    "# Fine-tuning Stable Diffusion with DreamBooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oShpn4pYMdNa"
   },
   "source": [
    "## Learning Objectives\n",
    "- Learn how to use pre-trained Stable Diffusion via Keras CV.\n",
    "- Learn how to fine-tune Stable Diffusion using DreamBooth method.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we implement DreamBooth, a fine-tuning technique to teach new visual concepts to Stable Diffusion with just 3 - 5 images. \n",
    "\n",
    "DreamBooth was proposed in [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) by Ruiz et al.<br>\n",
    "This lab assumes that you have a basic familiarity with diffusion models. If you don't, please first refer to [diffusion_model.ipynb](./diffusion_model.ipynb) first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion on Keras CV\n",
    "In this lab, we fine-tune Stable Diffusion, although DreamBooth itself can be applied to any text-conditioned image generation model.\n",
    "\n",
    "Stable Diffusion is introduced in [\"High-Resolution Image Synthesis with Latent Diffusion Models\"](https://arxiv.org/abs/2112.10752) paper. <br>\n",
    "The Latent Diffusion Models (LDMs) proposed in this paper suggests applying diffusion models to image latent space, instead of the image pixel space. The latent space is obtained using an additional pre-trained [Variational Autoencoder](./variational_autoencoder.ipynb).\n",
    "\n",
    "<img width=\"1084\" alt=\"image\" src=\"https://github.com/GoogleCloudPlatform/asl-ml-immersion/assets/6895245/6fced434-99b7-49aa-8828-ed760b519f63\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there exist multiple open-source implementations that allow you to easily create images from textual prompts, [KerasCV](https://keras.io/keras_cv/)'s implementation offers a few distinct advantages. <br>\n",
    "These include [XLA(Accelerated Linear Algebra)](https://www.tensorflow.org/xla) compilation and [mixed precision](https://www.tensorflow.org/guide/mixed_precision) support, which together achieve state-of-the-art generation speed.\n",
    "\n",
    "Let's generate images using Keras CV's Stable Diffusion implementation.\n",
    "\n",
    "**It takes a few minutes for the first run. Refer to [this guide](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/) for a more detailed performance comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set `PATH` to include the directory containing tensorboard\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "- Instantiate `StableDiffusionV2` by refering to [this guide](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/)\n",
    "- Write your own prompt, and generate image calling `text_to_image()` function of Stable Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set mixed precision\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# TODO: Instanciate Stable Diffusion model\n",
    "stable_diffusion_model = ...\n",
    "\n",
    "# Let's make sure to warm up the model\n",
    "# TODO: Write your prompt, and generate image with the instantiated Stable Diffusion model\n",
    "images = ...\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(len(images)):\n",
    "    ax = plt.subplot(1, len(images), i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! But we want to \"personalize\" this image generation model a bit.\n",
    "\n",
    "Let's try to do it in the next section with DreamBooth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DreamBooth fine-tuning\n",
    "\n",
    "DreamBooth, in a sense, is similar to the traditional way of fine-tuning a text-conditioned diffusion model, but instead of [completely tuning the pre-trained weights toward a specific domain](https://keras.io/examples/generative/finetune_stable_diffusion/) with a relatively large dataset, it is designed to make a model learn a new concept and \"personalize\" a model with **3-5 images**.\n",
    "\n",
    "DreamBooth can achieve it by binding a unique identifier (`hta` in this notebook) with a target concept (e.g., your dog) to learn the new concept without forgetting what it learned in the pre-training. <br>\n",
    "After training, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes.\n",
    "\n",
    "The usage of DreamBooth is very versatile. By teaching Stable Diffusion about your favorite visual concepts, you can\n",
    "\n",
    "**Recontextualize objects in interesting ways:**\n",
    "\n",
    "  ![](https://i.imgur.com/4Da9ozw.png)\n",
    "\n",
    "**Generate artistic renderings of the underlying visual concept:**\n",
    "\n",
    "  ![](https://i.imgur.com/nI2N8bI.png)\n",
    "\n",
    "\n",
    "And many other applications. We welcome you to check out the original\n",
    "[DreamBooth paper](https://arxiv.org/abs/2208.12242) in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Fine-tuning of Stable Diffusion requires a GPU with at least 24 GB of VRAM.<br>\n",
    "However, this notebook is designed to use Vertex AI Training for fine-tuning, so you don't need to have such a large GPU on your machine.\n",
    "\n",
    "So, let's setup Google Cloud `PROJECT` and GCS `BUCKET` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we want to construct a Python file for cloud training, but also want to execute code on this notebook kernel at the same time to check each step interactively.\n",
    "\n",
    "Here, we define a helper magic function to do both. By writing `%%write_and_run` at the top of each cell, we can write and execute the code at the same time.<br>\n",
    "Also, `-a` option can append the code to the existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = \"w\"\n",
    "    if len(argz) == 2 and argz[0] == \"-a\":\n",
    "        mode = \"a\"\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell)\n",
    "    get_ipython().run_cell(cell)\n",
    "\n",
    "\n",
    "TRAINING_APP_FOLDER = \"dreambooth_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsekLnUlMdNd"
   },
   "source": [
    "### Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTB9j2DWMdNd"
   },
   "outputs": [],
   "source": [
    "%%write_and_run {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZHEi4MRMdNe",
    "tags": []
   },
   "source": [
    "### Instance and class images\n",
    "\n",
    "DreamBooth uses a technique called \"prior preservation\" to meaningfully guide the training procedure such that the fine-tuned models can still preserve some of the prior semantics (e.g., the concept of the 'dog' class) of the visual concept you're introducing. To know more about the idea of \"prior preservation,\" refer to [this document](https://dreambooth.github.io/).\n",
    "\n",
    "Here, we need to introduce a few key terms specific to DreamBooth:\n",
    "\n",
    "* **Instance**: Images representing the visual concept you're trying to teach. The number of images is typically just 3 - 5. We typically gather these images ourselves. For prompts, a unique identifier is prepended to the unique class while forming the \"instance prompts.\" In this example, we use `\"hta\"` as this unique identifier. We should construct a prompt that best describes the \"instance images.\" An example prompt could be - `f\"a photo of a {unique_id} {unique_class}\"`. So, for our example, this becomes - `\"a photo of a hta dog\"`. \n",
    "* **Class (Prior)**: Images generated using a \"class prompt\" to use \"Prior Preservation\" in DreamBooth training. \"Prior Preservation\" is used to avoid the model forgetting the general class representation, such as the \"dog\" class or \"person\" class), obtained in pre-training. In this example, we use \"dog\". We leverage the pre-trained model before fine-tunings to generate these class images. Typically, 200 - 300 class images are enough.  The prompt is something like `\"a photo of a dog\"`.\n",
    "\n",
    "**Note** that prior preservation is an optional technique used in DreamBooth, but it almost always helps in improving the quality of the generated images.\n",
    "\n",
    "To generate the class prior, you can simply call the pre-trained Stable Diffusion model with the class prompt.\n",
    "\n",
    "```python\n",
    "class_prompt = \"a photo of a dog\"\n",
    "images = model.text_to_image(class_prompt)\n",
    "```\n",
    "\n",
    "To keep the runtime of this example short, the class prior images are already generated. For the entire code, please refer to [this code](../dreambooth_utils/prior_generator.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data are hosted on a public GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://asl-public/data/dreambooth_dog/instance/* | head -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://asl-public/data/dreambooth_dog/class/* | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUX78P4lMdNf"
   },
   "source": [
    "### Load and visualize images\n",
    "\n",
    "Let's setup a data loader using `tf.data` API and check a few samples both from instance and class images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "instance_images = tf.data.Dataset.list_files(\"gs://asl-public/data/dreambooth_dog/instance/*\")\n",
    "class_images = tf.data.Dataset.list_files(\"gs://asl-public/data/dreambooth_dog/class/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "def decode_jpg(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDAKTMoqMdNg"
   },
   "source": [
    "And then, we plot images with this utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(path_ds, num=5):\n",
    "    f, ax = plt.subplots(1, num, figsize=(16, num))\n",
    "    for idx, img in enumerate(path_ds.map(decode_jpg).take(num)):\n",
    "        ax[idx].imshow(tf.keras.preprocessing.image.array_to_img(img))\n",
    "        ax[idx].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEQeGh5BMdNg"
   },
   "source": [
    "**Instance images**: These beautiful instance photos are taken by [Alvan Nee](https://unsplash.com/@alvannee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(instance_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIjV7ZDJMdNh"
   },
   "source": [
    "**Class images (Prior)**: These images are generated by the pre-trained model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vISBA0v1MdNh"
   },
   "outputs": [],
   "source": [
    "plot_images(class_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvcQ00qDMdNh"
   },
   "source": [
    "### Prepare datasets\n",
    "In training, we pass a pair of instance and class data in each step.<br>\n",
    "But since the lengths of these datasets are different (instance: 5, class: 200), let's repeat the instance dataset and set the same length.\n",
    "\n",
    "To do this, we can simply call `.repeat()` method of `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "instance_len = instance_images.cardinality() # 5\n",
    "class_len = class_images.cardinality() # 200\n",
    "ds_len = instance_len * (class_len // instance_len)\n",
    "\n",
    "instance_images = instance_images.repeat(class_len // instance_len)\n",
    "class_images = class_images.take(ds_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puJ3QICrMdNh"
   },
   "source": [
    "#### Prepare the prompts\n",
    "Each dataset includes two components: prompt and image. Let's preprocess both of them.\n",
    "\n",
    "The prompts dataset is straightforward.\n",
    "We simply repeat the instance prompt (`\"a photo of a hta dog\"`) and class prompt (`\"a photo of a dog\"`) in each step.\n",
    "\n",
    "Since Keras CV includes a pre-trained `SimpleTokenizer`, we can use it to tokenize these prompts.\n",
    "\n",
    "Note that the length of the prompt token (`MAX_PROMPT_LENGTH`) and padding token index (`PADDING_TOKEN`) come from this tokenizer and text encoder.<br>\n",
    "In the Keras CV implementation, a CLIP text encoder is used.\n",
    "\n",
    "Please refer to [the source code](https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/text_encoder.py) for more details of the text encoder and these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "# Refererence: https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/text_encoder.py\n",
    "PADDING_TOKEN = 49407\n",
    "MAX_PROMPT_LENGTH = 77\n",
    "\n",
    "# We just repeat the prompts / captions per images.\n",
    "unique_id = \"hta\"\n",
    "class_label = \"dog\"\n",
    "\n",
    "instance_prompt = f\"a photo of a {unique_id} {class_label}\"\n",
    "class_prompt = f\"a photo of a {class_label}\"\n",
    "\n",
    "# Load the tokenizer.\n",
    "tokenizer = keras_cv.models.stable_diffusion.SimpleTokenizer()\n",
    "\n",
    "# Method to tokenize and pad the tokens.\n",
    "def process_text(caption):\n",
    "    tokens = tokenizer.encode(caption)\n",
    "    tokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "tokenized_instance_prompt = process_text(instance_prompt)\n",
    "tokenized_class_prompt = process_text(class_prompt)\n",
    "\n",
    "instance_prompt_ds = \\\n",
    "    tf.data.Dataset.from_tensor_slices([tokenized_instance_prompt]).repeat(ds_len)\n",
    "class_prompt_ds = \\\n",
    "    tf.data.Dataset.from_tensor_slices([tokenized_class_prompt]).repeat(ds_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'='*10} Instance Prompt Token {'='*10}\")\n",
    "for d in instance_prompt_ds.take(1):\n",
    "    print(d)\n",
    "\n",
    "print(f\"{'='*10} Class Prompt Token {'='*10}\")\n",
    "for d in class_prompt_ds.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4Tb8Q6LMdNi"
   },
   "source": [
    "#### Prepare the images\n",
    "\n",
    "For images, we set up standard preprocessing and a few data argumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2DifmbvMdNi"
   },
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "resolution = 512\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        keras_cv.layers.CenterCrop(resolution, resolution),\n",
    "        keras_cv.layers.RandomFlip(),\n",
    "        keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def process_image(image_path, tokenized_text):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(image, 3)\n",
    "    image = tf.image.resize(image, (resolution, resolution))\n",
    "    return image, tokenized_text\n",
    "\n",
    "\n",
    "def apply_augmentation(image_batch, tokenized_texts):\n",
    "    return augmenter(image_batch), tokenized_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxwocIggMdNi"
   },
   "source": [
    "#### Assemble dataset\n",
    "\n",
    "Now, let's assemble the prompt and image dataset with `tf.data.Dataset.zip()` method.\n",
    "\n",
    "Note that the final dataset consists of a dictionary with two keys (`\"***_images\"` and `\"***_texts\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "def prepare_dict(instance_only=True):\n",
    "    def fn(image_batch, tokenized_texts):\n",
    "        if instance_only:\n",
    "            batch_dict = {\n",
    "                \"instance_images\": image_batch,\n",
    "                \"instance_texts\": tokenized_texts,\n",
    "            }\n",
    "            return batch_dict\n",
    "        else:\n",
    "            batch_dict = {\n",
    "                \"class_images\": image_batch,\n",
    "                \"class_texts\": tokenized_texts,\n",
    "            }\n",
    "            return batch_dict\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def assemble_dataset(\n",
    "    image_ds, tokenized_ds, instance_only=True, batch_size=1\n",
    "):\n",
    "    dataset = tf.data.Dataset.zip((image_ds, tokenized_ds))\n",
    "    dataset = dataset.shuffle(5, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.map(process_image, num_parallel_calls=auto)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(apply_augmentation, num_parallel_calls=auto)\n",
    "\n",
    "    prepare_dict_fn = prepare_dict(instance_only=instance_only)\n",
    "    dataset = dataset.map(prepare_dict_fn, num_parallel_calls=auto)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "instance_dataset = assemble_dataset(\n",
    "    instance_images,\n",
    "    instance_prompt_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "class_dataset = assemble_dataset(\n",
    "    class_images,\n",
    "    class_prompt_ds,\n",
    "    instance_only=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "train_dataset = tf.data.Dataset.zip((instance_dataset, class_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9eufInuMdNi"
   },
   "source": [
    "Now that the dataset has been prepared, let's quickly check the shape of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_dataset.take(1):\n",
    "    for idx in range(len(d)):\n",
    "        print(d[idx].keys())\n",
    "        for key in d[idx].keys():\n",
    "            print(f\"shape of {key:15s}: {d[idx][key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuhJ_6CpMdNj"
   },
   "source": [
    "### DreamBooth training loop\n",
    "Now let's define the main model class for training as `DreamBoothTrainer` class.\n",
    "\n",
    "First, please remember that Stable Diffusion consists of multiple models, including:\n",
    "- Text Encoder (`StableDiffusionV2.text_encoder`)\n",
    "- Diffusion Model (`StableDiffusionV2.diffusion_model`)\n",
    "- Variational AutoEncoder (`StableDiffusionV2.image_encoder` and `StableDiffusionV2.decoder`)\n",
    "\n",
    "We simply start the training from the pre-trained weights. Note that we just construct them in the `__init__`  below.<br>\n",
    "We fine-tune the text encoder and diffusion model but freeze the VAE encoder. VAE decoder is unnecessary in training, since we can compute loss in latent space.\n",
    "\n",
    "`train_step()` is the main function where the fine-tuning process is defined using class and instance prompts.<br>\n",
    "Note that the class and instance data are concatenated first and passed to the training process defined in `with tf.GradientTape` which roughly includes:\n",
    "1. **Prompt Encoding**: The pre-trained text encoder is used.\n",
    "2. **Image Encoding** The VAE encoder is used. Note that `sample_from_encoder_outputs()` function takes care of sampling from VAE's latent space. (Please refer to the VAE notebook if you are unfamiliar with this idea.)\n",
    "3. **Forward Diffusion Process**: Sample noise and add it to the image.\n",
    "4. **Reversed Diffusion Process (Diffusion Model Prediction)** The U-Net based diffusion model predicts the noise from the noisy image.\n",
    "5. **Loss Computation**: Loss is the simple MSE as the standard diffusion models. As defined in `compute_loss()` function, we can optionally add weight to balance class and instant loss.\n",
    "\n",
    "After computing loss, we compute the loss with respect to the diffusion model and text encoder, and update them, respectively. (VAE is frozen.)\n",
    "\n",
    "**Exercise**: Complete the `train_step()` and `compute_loss()` function refering to the instruction above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSH5e9q3MdNj"
   },
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "class DreamBoothTrainer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sd_model,\n",
    "        noise_scheduler,\n",
    "        max_prompt_length=77,  # Default is 77\n",
    "        use_mixed_precision=False,\n",
    "        prior_loss_weight=1.0,\n",
    "        max_grad_norm=1.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.sd_model = sd_model\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.max_prompt_length = max_prompt_length\n",
    "        self.use_mixed_precision = use_mixed_precision\n",
    "        self.prior_loss_weight = prior_loss_weight\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.diffusion_model = sd_model.diffusion_model\n",
    "        self.text_encoder = sd_model.text_encoder\n",
    "\n",
    "        self.image_encoder = sd_model.image_encoder\n",
    "        self.image_encoder.trainable = False\n",
    "\n",
    "    def compile(self, diffusion_optimizer, text_encoder_optimizer, loss):\n",
    "        super().compile(loss=loss)\n",
    "        self.diffusion_optimizer = diffusion_optimizer\n",
    "        self.text_encoder_optimizer = text_encoder_optimizer\n",
    "        \n",
    "    def train_step(self, inputs):\n",
    "        instance_batch = inputs[0]\n",
    "        class_batch = inputs[1]\n",
    "\n",
    "        instance_images = instance_batch[\"instance_images\"]\n",
    "        instance_text = instance_batch[\"instance_texts\"]\n",
    "        class_images = class_batch[\"class_images\"]\n",
    "        class_text = class_batch[\"class_texts\"]\n",
    "\n",
    "        images = tf.concat([instance_images, class_images], 0)\n",
    "        tokenized_texts = tf.concat([instance_text, class_text], 0)\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        POS_IDS = tf.convert_to_tensor(\n",
    "            [list(range(self.max_prompt_length))], dtype=tf.int32\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            embedded_texts = self.text_encoder(\n",
    "                [tokenized_texts, POS_IDS], training=True\n",
    "            )\n",
    "\n",
    "            # Project image into the latent space.\n",
    "            # TODO: Define Image Encoding\n",
    "            latents = ...\n",
    "\n",
    "            # Sample noise that we'll add to the latents.\n",
    "            noise = tf.random.normal(tf.shape(latents))\n",
    "\n",
    "            # Sample a random timestep for each image.\n",
    "            timesteps = tnp.random.randint(\n",
    "                0, self.noise_scheduler.train_timesteps, (batch_size,)\n",
    "            )\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process).\n",
    "            noisy_latents = self.noise_scheduler.add_noise(\n",
    "                tf.cast(latents, noise.dtype), noise, timesteps\n",
    "            )\n",
    "\n",
    "            # Sampled noise is the target \n",
    "            target = noise\n",
    "            timestep_embedding = tf.map_fn(\n",
    "                lambda t: self.sd_model._get_timestep_embedding(t, batch_size),\n",
    "                timesteps,\n",
    "                fn_output_signature=tf.float32,\n",
    "            )[:, 0]\n",
    "\n",
    "            # Predict the noise residual and compute loss.\n",
    "            # TODO: Define Reverse Diffusion\n",
    "            model_pred = self.diffusion_model(...)\n",
    "            \n",
    "            loss = self.compute_loss(target, model_pred)\n",
    "            if self.use_mixed_precision:\n",
    "                loss = self.optimizer.get_scaled_loss(loss)\n",
    "\n",
    "        # Update parameters of the text encoder and diffusion model.\n",
    "        text_trainable_vars = self.text_encoder.trainable_variables\n",
    "        diffusion_trainable_vars = self.diffusion_model.trainable_variables\n",
    "        \n",
    "        text_gradients = tape.gradient(loss, text_trainable_vars)\n",
    "        diffusion_gradients = tape.gradient(loss, diffusion_trainable_vars)\n",
    "\n",
    "        if self.use_mixed_precision:\n",
    "            text_gradients = self.optimizer.get_unscaled_gradients(text_gradients)\n",
    "            diffusion_gradients = self.optimizer.get_unscaled_gradients(diffusion_gradients)\n",
    "        text_gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in text_gradients]\n",
    "        diffusion_gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in diffusion_gradients]\n",
    "\n",
    "        self.text_encoder_optimizer.apply_gradients(zip(text_gradients, text_trainable_vars))\n",
    "        self.diffusion_optimizer.apply_gradients(zip(diffusion_gradients, diffusion_trainable_vars))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def compute_loss(self, target, model_pred):\n",
    "        # Since the first half of the inputs has instance samples and the second half\n",
    "        # has class samples, we do the chunking accordingly.\n",
    "        model_pred, model_pred_prior = tf.split(\n",
    "            model_pred, num_or_size_splits=2, axis=0\n",
    "        )\n",
    "        target, target_prior = tf.split(target, num_or_size_splits=2, axis=0)\n",
    "\n",
    "        # TODO: Define instance loss loss.\n",
    "        loss = ...\n",
    "\n",
    "        # TODO: Define prior loss.\n",
    "        prior_loss = ...\n",
    "\n",
    "        # Add the prior loss to the instance loss.\n",
    "        # TODO: Add up the instance and prior loss using `self.prior_loss_weight`.\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "    def save_weights(\n",
    "        self, filepath, overwrite=True, save_format=None, options=None\n",
    "    ):\n",
    "        # Overriding this method will allow us to customize which weights to save.\n",
    "        # In this case, it will checkpoint the `diffusion_model` and `text_encoder`.\n",
    "        self.diffusion_model.save_weights(\n",
    "            filepath=f\"{filepath}diffusion\",\n",
    "            overwrite=overwrite,\n",
    "            save_format=save_format,\n",
    "            options=options,\n",
    "        )\n",
    "\n",
    "        self.text_encoder.save_weights(\n",
    "            filepath=f\"{filepath}text_encoder\",\n",
    "            overwrite=overwrite,\n",
    "            save_format=save_format,\n",
    "            options=options,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC8aBY7QMdNj"
   },
   "source": [
    "### Trainer initialization\n",
    "\n",
    "So, let's construct the trainer and compile it as usual.\n",
    "\n",
    "Note that we define two optimizers for the diffusion model and the text encoder, respectively, though it is still possible to train both with a single optimizer.<br>\n",
    "By doing so, we can control the learning rates and the scheduling separately.<br>\n",
    "Since Stable Diffusion is very sensitive to hyperparameters and it overfits/underfits easily, it is beneficial to have granular control.\n",
    "\n",
    "**Exercise**: Complete the `compile()` using two optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iJLBPzJMdNj"
   },
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "def build_trainer(\n",
    "    diffusion_learning_rate,\n",
    "    text_learning_rate,\n",
    "    diffusion_beta_1=0.9,\n",
    "    diffusion_beta_2=0.999,\n",
    "    text_beta_1=0.9,\n",
    "    text_beta_2=0.999,\n",
    "    diffusion_weight_decay=(1e-2,),\n",
    "    text_weight_decay=(1e-2,),\n",
    "    epsilon = 1e-08,\n",
    "    use_mp=True # Set it to False if you're not using a GPU with tensor cores.\n",
    "):\n",
    "    if use_mp:\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    dreambooth_model = keras_cv.models.StableDiffusionV2(\n",
    "        img_width=resolution, img_height=resolution, jit_compile=True\n",
    "    )\n",
    "\n",
    "    dreambooth_trainer = DreamBoothTrainer(\n",
    "        sd_model=dreambooth_model,\n",
    "        noise_scheduler=keras_cv.models.stable_diffusion.NoiseScheduler(),\n",
    "        use_mixed_precision=use_mp,\n",
    "    )\n",
    "\n",
    "    diffusion_optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=diffusion_learning_rate,\n",
    "        weight_decay=diffusion_weight_decay,\n",
    "        beta_1=diffusion_beta_1,\n",
    "        beta_2=diffusion_beta_2,\n",
    "        epsilon=epsilon,\n",
    "    )\n",
    "\n",
    "    text_encoder_optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=text_learning_rate,\n",
    "        weight_decay=text_weight_decay,\n",
    "        beta_1=text_beta_1,\n",
    "        beta_2=text_beta_2,\n",
    "        epsilon=epsilon,\n",
    "    )\n",
    "\n",
    "    dreambooth_trainer.compile(...)\n",
    "    \n",
    "    return dreambooth_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Tensorboard Callbacks (Optional)\n",
    "\n",
    "In generative models training, qualitative evaluation (check the generated outputs instead quantitative metrics) is very important.\n",
    "For this purpose, it's useful to define additional Tensorboard callback functions to check the training process with generative output.\n",
    "\n",
    "The custom callback function `\"ImageGenCallback\"` is defined to generate images at the end of each epoch.<br>\n",
    "For more details about the custom callback function, please refer to [the document](https://www.tensorflow.org/guide/keras/writing_your_own_callbacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lIERfgDMdNk"
   },
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "class ImageGenCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tensorboard_path, prompts, num_imgs_to_gen=3):\n",
    "        self.tensorboard_path = tensorboard_path\n",
    "        self.prompts = prompts\n",
    "        self.num_imgs_to_gen = num_imgs_to_gen\n",
    "\n",
    "    def log_image(self, prefix, prompt, step):\n",
    "        images_dreamboothed = self.model.sd_model.text_to_image(\n",
    "            prompt, batch_size=self.num_imgs_to_gen\n",
    "        )\n",
    "        logdir = f\"{self.tensorboard_path}images\"\n",
    "        file_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.image(\n",
    "                f\"{prefix}: '{prompt}'\",\n",
    "                images_dreamboothed,\n",
    "                step=step,\n",
    "            )\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        prefix = \"0_Initial Model\"\n",
    "        for p in self.prompts:\n",
    "            self.log_image(prefix, p, step=0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prefix = f\"{epoch+1}_Epoch {epoch+1}\"\n",
    "        for p in self.prompts:\n",
    "            self.log_image(prefix, p, step=epoch+1)\n",
    "\n",
    "def construct_tb_callbacks(tensorboard_path, prompts, num_imgs_to_gen=3):\n",
    "\n",
    "    tf_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=tensorboard_path,\n",
    "        write_graph=False,\n",
    "    )\n",
    "\n",
    "    return [tf_callback, ImageGenCallback(tensorboard_path, prompts, num_imgs_to_gen)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0STO9fuzMdNk"
   },
   "source": [
    "### Training\n",
    "We first calculate the number of epochs we need to train for.<br>\n",
    "The paper suggests that 1000 steps with a learning rate 6e-6 (for Stable Diffusion) works for most cases. Still, it may require more steps or a different set of hyperparameters for more complicated or subtle objects like human faces.\n",
    "\n",
    "Here, we use 1000 steps with a bit larger learning rate so that it can learn the pattern of the instance dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ceYVsxQMdNk"
   },
   "outputs": [],
   "source": [
    "%%write_and_run -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "MAX_TRAIN_STEP = 1000\n",
    "\n",
    "num_update_steps_per_epoch = ds_len // BATCH_SIZE\n",
    "epochs = math.ceil(MAX_TRAIN_STEP / num_update_steps_per_epoch)\n",
    "\n",
    "print(f\"Training for {epochs} epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8kMupwEMdNk"
   },
   "source": [
    "This cell runs the training.<br>\n",
    "But since we don't want to run it locally, we simply use `%%writefile` magic instead of `%%write_and_run` to write the file without execution.<br>\n",
    "We will dispatch the cloud training job later.\n",
    "\n",
    "If your local machine has A100 GPU attached, you can still run training with the command below. (Run this [bash file](../dreambooth_utils/create_a100_notebook.sh) if you want to create an A100 notebook instance.)\n",
    "\n",
    "```python\n",
    "output_base_dir = \"dreambooth_outputs\"\n",
    "tensorboard_path = f\"{output_base_dir}/logs/\"\n",
    "\n",
    "!rm -rf {output_base_dir}\n",
    "os.makedirs(output_base_dir)\n",
    "\n",
    "dreambooth_trainer= build_trainer(\n",
    "    diffusion_learning_rate = 8e-6,\n",
    "    text_learning_rate = 6e-7,\n",
    ")\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {tensorboard_path}\n",
    "\n",
    "callback_prompts = [f\"A photo of a {unique_id} {class_label}\"]\n",
    "\n",
    "dreambooth_trainer.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=construct_tb_callbacks(tensorboard_path, callback_prompts),\n",
    ")\n",
    "\n",
    "os.makedirs(f\"{output_base_dir}/checkpoints\")\n",
    "dreambooth_trainer.save_weights(f\"{output_base_dir}/checkpoints/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile -a {TRAINING_APP_FOLDER}/dreambooth_trainer.py\n",
    "\n",
    "dreambooth_trainer= build_trainer(\n",
    "    diffusion_learning_rate = 8e-6,\n",
    "    text_learning_rate = 6e-7,\n",
    ")\n",
    "\n",
    "tensorboard_path = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "\n",
    "callback_prompts = [f\"A photo of a {unique_id} {class_label}\"]\n",
    "\n",
    "dreambooth_trainer.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=construct_tb_callbacks(tensorboard_path, callback_prompts),\n",
    ")\n",
    "\n",
    "dreambooth_trainer.save_weights(os.getenv(\"AIP_CHECKPOINT_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud Training\n",
    "Now, the entire training code is ready.<br>\n",
    "We'll containerize the environment in a simple docker container and push it to the artifact registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-12.py310:latest\n",
    "RUN pip install keras_cv==0.5.1 --upgrade --quiet\n",
    "\n",
    "COPY . /code\n",
    "\n",
    "WORKDIR /code\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"dreambooth_trainer.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"dreambooth_trainer\"\n",
    "\n",
    "!gcloud builds submit --tag us-docker.pkg.dev/{PROJECT}/{ARTIFACT_REGISTRY_DIR}/{IMAGE_NAME} {TRAINING_APP_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify `NVIDIA_TESLA_A100` in the worker pool spec. Training takes ~30 minutes with A100 GPU.\n",
    "\n",
    "**However, since A100 GPU is in high demand, the training execution command would return fail because of a resource error.**<br>\n",
    " <span style=\"color:red\">**Even if you see a resource error, please feel free to skip this cell.**</span><br>\n",
    " <span style=\"color:red\">**We already ran the training for you in the same setup and saved the result. You can check it in the next section.**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CONTAINER_URI = (\n",
    "    f\"us-docker.pkg.dev/{PROJECT}/{ARTIFACT_REGISTRY_DIR}/{IMAGE_NAME}\"\n",
    ")\n",
    "\n",
    "TRAIN_MACHINE_TYPE = \"a2-highgpu-1g\"\n",
    "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\"\n",
    "TRAIN_NUM_GPU = 1\n",
    "\n",
    "RESOLUTION = 512\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAIN_MACHINE_TYPE,\n",
    "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": TRAIN_NUM_GPU,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"disk_spec\": {\n",
    "            \"boot_disk_type\": \"pd-ssd\",\n",
    "            \"boot_disk_size_gb\": 500,\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
    "            \"command\": [],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"dreambooth_training_{TIMESTAMP}\"\n",
    "\n",
    "train_job = aiplatform.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=f\"gs://{BUCKET}/dreambooth_outputs/{JOB_NAME}\",\n",
    "    staging_bucket=f\"gs://{BUCKET}\",\n",
    ")\n",
    "\n",
    "train_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoWDGSbEMdNt",
    "tags": []
   },
   "source": [
    "### Inference\n",
    "\n",
    "Now, let's take a look at the result!\n",
    "\n",
    "Replace `output_base_dir` to your bucket if you could run the training with A100.<br>\n",
    "(Or local path if you run it locally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained weights\n",
    "output_base_dir = \"gs://asl-public/model_checkpoints/dreambooth_trained\"\n",
    "\n",
    "# Your cloud training destination path\n",
    "# output_base_dir = f\"gs://{BUCKET}/dreambooth_outputs/{JOB_NAME}\"\n",
    "\n",
    "# Or local training output path\n",
    "# output_base_dir = \"dreambooth_outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how the fine-tuning process went using the tensorboard thanks to the tensorboard callbacks we added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {output_base_dir}/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look? We can see that the model is gradually obtaining the concept of the instance in the training process.\n",
    "\n",
    "Next, let's load the checkpoint of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stable diffusion weights\n",
    "stable_diffusion_model.diffusion_model.load_weights(\n",
    "    f\"{output_base_dir}/checkpoints/diffusion\"\n",
    ")\n",
    "stable_diffusion_model.text_encoder.load_weights(\n",
    "    f\"{output_base_dir}/checkpoints/text_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate images with a prompt.<br>\n",
    "Note how the unique identifier and the class have been used in the prompt.\n",
    "\n",
    "Feel free to experiment with different prompts (Remember to add the unique identifier and the class label!) to see how the results change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(prompts, num_imgs_to_gen, num_steps=50):\n",
    "    images_dreamboothed = stable_diffusion_model.text_to_image(\n",
    "        prompt, batch_size=num_imgs_to_gen, num_steps=num_steps\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images_dreamboothed)):\n",
    "        ax = plt.subplot(1, num_imgs_to_gen, i + 1)\n",
    "        plt.title(prompts)\n",
    "        plt.imshow(images_dreamboothed[i])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mhvr1vxwMdNt"
   },
   "outputs": [],
   "source": [
    "prompt = f\"A photo of a {unique_id} {class_label} in a car\"\n",
    "\n",
    "plot_inference(prompt, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"A photo of a {unique_id} {class_label} in front of Eiffel Tower\"\n",
    "plot_inference(prompt, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIC-bn2wMdNu"
   },
   "outputs": [],
   "source": [
    "prompt = f\"A photo of a black {unique_id} {class_label}\"\n",
    "\n",
    "plot_inference(prompt, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's check if our model doesn't forget the general concept of dog by removing the identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"A photo of a {class_label} in front of Eiffel Tower\"\n",
    "plot_inference(prompt, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0aiugehMdNu"
   },
   "source": [
    "## Acknowledgements\n",
    "This notebook code is based on a [Keras official tutorial by Sayak Paul and Chansung Park](https://keras.io/examples/generative/dreambooth/)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "dreambooth",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
