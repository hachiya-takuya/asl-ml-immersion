{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOM4j723Kxc2"
   },
   "source": [
    "# Contextual Bandits with TF-agents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftcVe8b6Kxc3"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifdzP2rRKxc5"
   },
   "source": [
    "*Contextual Bandit (CB)* is a machine learning framework in which a *agent* selects actions (also called *arms*) in order to maximize rewards in the long term. At each round, the agent receives some information about the current state (also called the *context*) and uses this information to select an action. As a consequence of this choice, it receives a *reward*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one hand, contextual bandit is one of the simplest instance of a *reinforcement learning problem* where a single state (or context) is provided to the agent and the play or *episode* stops after the first action has been chosen and the reward gotten. This setting appears in a number of useful problems in the industry, one of the best known being that of ad placements on a website: The different ads to publish on a webpage are the different actions, the context is given by a user features, and the reward is 1 is the user clicks on the published ad and 0 otherwise.\n",
    "\n",
    "On the other hand, contextual bandit is a natural generalization of a classification problem in supervized learning. Namely, consider a data set of points $(x, y)$ where the $x$'s are the features and the $y$'s are the labels in $k$ possible classes. We can setup an associated contextual bandit problem as follows: The CB agent at each time step is given the context $x$. From that information, it needs to select from $k$ possible actions which are the $k$ possible classes. If the agent chooses the correct class for feature $x$, then the reward is $1$, and zero otherwise. The general goal of maximising the long-term cumulative reward for the CB agent is equivalent to that of minimizing the training loss in supervized learning. Contextual bandit is more general than classification though, since in many useful CB settings we actually know the reward only for the  actions we have taken. \n",
    "\n",
    "In this lab, we will learn how to solve a contextual bandit problem derived from a classification dataset with *Q-learning* and the associated *neural epsilon-greedy strategy* using a powerful reinforcement learning library written in tensorflow: [TensorFlow Agents](https://www.tensorflow.org/agents). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement:** This lab is based on a tutorial originally written by Anant Nawalgaria and Alex Erfurt. We thank them for making their original material available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpEsA3xmKxc6"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let us intall [TensorFlow Agents](https://www.tensorflow.org/agents) if it is not already installed and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze | grep tf_agents || pip install -q tf_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WttMqkUYKxc9"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "\n",
    "# from tensorflow.python.framework.dtypes import int64\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tf_agents.bandits.agents.neural_epsilon_greedy_agent import (\n",
    "    NeuralEpsilonGreedyAgent,\n",
    ")\n",
    "from tf_agents.bandits.environments import environment_utilities as env_util\n",
    "from tf_agents.bandits.environments.classification_environment import (\n",
    "    ClassificationBanditEnvironment,\n",
    ")\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import (\n",
    "    TFUniformReplayBuffer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset into BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we are going to use a classification dataset and turn it into a contextual bandit problem. \n",
    "\n",
    "Our dataset will be the [UCI Machine Learning Repository]( https://archive.ics.uci.edu/ml/datasets/covertype), which associates various cartographic features of a given area with different labels representing different types of forests covering the areas. \n",
    "\n",
    "The original features are as follows (the last column being the label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../../tfx_pipelines/data/dataset.csv\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step, our CB agent will be given a context $x$ representing an area cartographic features (`Elevation`, `Aspect`, `Slope`, etc.). Then it will have to choose among one of 7 possible forest cover types as defined by the last column (`Cover_type`), and represented by the integer from 0 to 6.\n",
    "\n",
    "For convenience, we have pre-precessed the categorical features `Wilderness_Area` and `Soil_Type` into their one-hot-encoded versions. So the dataset we will use will have more columns (55 exactly) than the original covertype dataset. We will name the columns from `C0` to `C54`. The columns from `C0` to `C53` represent the features, while the last column `C54` represents the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines our dataset column names and types and displays a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COLUMNS = 55\n",
    "COLUMN_NAMES = [f\"C{i}\" for i in range(N_COLUMNS)]\n",
    "COLUMN_TYPES = [tf.int64] * N_COLUMNS\n",
    "\n",
    "covertype_df = pd.read_csv(\"../data/covertype.csv\", names=COLUMN_NAMES)\n",
    "covertype_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how many examples we have at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = len(covertype_df)\n",
    "NUM_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load this dataset into `BigQuery` into the table named\n",
    "\n",
    "```bash\n",
    "PROJECT_ID.DATASET_ID.TABLE_ID\n",
    "```\n",
    "\n",
    "where `DATASET_ID` and `TABLE_ID` are defined in the next cell among other variable like the `DATASET_SCHEMA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOCATION = \"US\"\n",
    "DATASET_SOURCE = \"../data/covertype.csv\"\n",
    "DATASET_SCHEMA = \",\".join([f\"C{i}:INTEGER\" for i in range(N_COLUMNS)])\n",
    "DATASET_ID = \"covertype_dataset_rl\"\n",
    "TABLE_ID = \"covertypek_preproc\"\n",
    "\n",
    "os.environ[\"DATASET_LOCATION\"] = DATASET_LOCATION\n",
    "os.environ[\"DATASET_SOURCE\"] = DATASET_SOURCE\n",
    "os.environ[\"DATASET_SCHEMA\"] = DATASET_SCHEMA\n",
    "os.environ[\"DATASET_ID\"] = DATASET_ID\n",
    "os.environ[\"TABLE_ID\"] = TABLE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below run the [bq command line](https://cloud.google.com/bigquery/docs/bq-command-line-tool) to create the dataset and populate the table from `DATASET_SOURCE` using the variable defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATASET_SOURCE \\\n",
    "$DATASET_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a `tf.data.Dataset` connected to the data table we created in our `BigQuery` instance, and our which TensorFlow Agents code will interact with.\n",
    "\n",
    "For that purpose, we will use [Tensorflow_io](https://github.com/tensorflow/io/tree/v0.15.0/tensorflow_io/bigquery), which offers a connector `BigQueryClient` to  stream data directly out of `BigQuery`:\n",
    "\n",
    "```python\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a`BigQuery` client and then a read session from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = BigQueryClient()\n",
    "\n",
    "bq_session = bq_client.read_session(\n",
    "    f\"projects/{PROJECT_ID}\",\n",
    "    PROJECT_ID,\n",
    "    TABLE_ID,\n",
    "    DATASET_ID,\n",
    "    COLUMN_NAMES,\n",
    "    COLUMN_TYPES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our `bq_session` we can create a `tf.data.Dataset` using the `parallel_read_rows` method, which will read our BigQuery rows in parallel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = bq_session.parallel_read_rows(\n",
    "    block_length=NUM_SAMPLES,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the examples are stored in our `tf_dataset` as `OrderedDict` with keys the column names and values the corresponding row values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in tf_dataset.take(1):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Configure the `tf_dataset` we instanciated so that\n",
    "1. the examples are stored as couples $(x, y)$ where $x$ is the feature vector with 54 components and $y$ is the label (**Hint:** Use `.map`)\n",
    "1. it loops over the dataset infefinitively (**Hint:** Use `.repeat`)\n",
    "1. it shuffles the dataset (Use `buffer_size=400000`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2ytDmxGKxdD"
   },
   "outputs": [],
   "source": [
    "LABEL_NAME = \"C54\"\n",
    "\n",
    "\n",
    "def features_and_labels(features):\n",
    "    label = features.pop(LABEL_NAME)\n",
    "    return (\n",
    "        tf.cast(tf.stack(tf.nest.flatten(features), axis=0), tf.float32),\n",
    "        tf.cast(label - 1, tf.int32),\n",
    "    )\n",
    "\n",
    "\n",
    "tf_dataset = (\n",
    "    tf_dataset.map(features_and_labels).repeat().shuffle(buffer_size=400000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that now the dataset has the correct form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZM8zjfrKxdI"
   },
   "outputs": [],
   "source": [
    "for example in tf_dataset.take(1):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do58umOOKxdL"
   },
   "source": [
    "## 3. Initializing and configuring the Environment\n",
    "\n",
    "An environment in the TF-Agents Bandits library is a class that provides observations and reports rewards based on obseravtions and actions.\n",
    "In this section we instantiate the \"covertype bandit environment\"\n",
    "\n",
    "In the TF-Agents bandits library, there is an environment wrapper (named ClassificationBanditEnvironment) that can turn any multiclass labeled dataset to a bandit environment. The context (or observation) will be the features in the dataset, the actions are the label classes, and the rewards are calculated based on some stochastic function of the actual and the guessed labels. This latter function is defined by a table of distributions. For our covertype example, this table is simply the deterministic identity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2ytDmxGKxdD"
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "ROOT_DIR = f\"./contextual_bandit_checkpoints/{TIMESTAMP}\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "TRAINING_LOOPS = 10\n",
    "STEPS_PER_LOOP = 2\n",
    "AGENT_ALPHA = 10.0\n",
    "\n",
    "EPSILON = 0.01\n",
    "LAYERS = (300, 200, 100, 100, 50, 50)\n",
    "LR = 0.002\n",
    "\n",
    "AGENT_CHECKPOINT_NAME = \"agent\"\n",
    "STEP_CHECKPOINT_NAME = \"step\"\n",
    "CHECKPOINT_FILE_PREFIX = \"ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TN0ECw1-KxdL"
   },
   "outputs": [],
   "source": [
    "# initialize the distribution\n",
    "covertype_reward_distribution = tfd.Independent(\n",
    "    tfd.Deterministic(tf.eye(7)), reinterpreted_batch_ndims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8F6MuC3KxdN"
   },
   "outputs": [],
   "source": [
    "covertype_reward_distribution.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkxCINhtKxdQ"
   },
   "source": [
    "# provides an interface to return a reward given an action for features, tf_dataset provides labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIBnUkTAKxdQ"
   },
   "outputs": [],
   "source": [
    "# Initializing the Classification Bandit Environment with the dataset, and reward distri ution\n",
    "environment = ClassificationBanditEnvironment(\n",
    "    tf_dataset, covertype_reward_distribution, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHpGGmPCKxdS"
   },
   "outputs": [],
   "source": [
    "environment.reward_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv5svfBfKxdU"
   },
   "source": [
    "## 4. Initializing the Agent\n",
    "Now that we have the environment and metrics intialized from the Tf.dataset loaded from big query we reach the part where we define and initialize our policy and the Agent which will be utilizing that policy to make decisions given an observation. We have several policies: as shown here:\n",
    "\n",
    "   1. [NeuralEpsilonGreedyAgent](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870): The neural epsilon greedy algorithm makes a value estimate for all the arms, and then chooses the best arm with the probability (1-epsilon) and any of the random arms with a probability of epsilon. this balances the exploration-exploitation tradeoff and epsilon is set to a small value like 10%. Example: In this example we have seven arms: one of each of the classes, and if we set epsilon to say 10%, then 90% of the times the agent will choose the arm with the highest value estimate (exploiting the one most likely to be the predicted class) and 10% of the time it will choose a random arm from all of the 7 arms( thus exploring the other possibilities). Refer [here](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent) for more information of the tensorflow agents version of the same.\n",
    "   \n",
    "   Each Agent is initialized with a policy: which is essentially the function approximator (be it linear or non linear) for estimating the Q values. The agent uses this policy, adds the exploration-exploitation component on top of this and then train the policy. In this example we will use a Deep Q Network as our policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fGaJF6mKxdV"
   },
   "outputs": [],
   "source": [
    "network = QNetwork(\n",
    "    input_tensor_spec=environment.time_step_spec().observation,\n",
    "    action_spec=environment.action_spec(),\n",
    "    fc_layer_params=LAYERS,\n",
    ")\n",
    "\n",
    "agent = NeuralEpsilonGreedyAgent(\n",
    "    time_step_spec=environment.time_step_spec(),\n",
    "    action_spec=environment.action_spec(),\n",
    "    reward_network=network,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "    epsilon=EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYsBS9eRKxdX",
    "tags": []
   },
   "source": [
    "## 5. Define and link the evaluation metrics\n",
    "\n",
    "\n",
    "Just like you have metrics like accuracy/recall in supervised learning, in bandits we use the [regret](https://www.tensorflow.org/agents/tutorials/bandits_tutorial#regret_metric) metric per episode. To calculate the regret, we need to know what the highest possible expected reward is in every time step. For that, we define the `optimal_reward_fn`.\n",
    "\n",
    "Another similar metric is the number of times a suboptimal action was chosen. That requires the definition if the `optimal_action_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-l0e78osKxdX"
   },
   "outputs": [],
   "source": [
    "optimal_reward_fn = functools.partial(\n",
    "    env_util.compute_optimal_reward_with_classification_environment,\n",
    "    environment=environment,\n",
    ")\n",
    "\n",
    "optimal_action_fn = functools.partial(\n",
    "    env_util.compute_optimal_action_with_classification_environment,\n",
    "    environment=environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXSX61X5Kxda"
   },
   "outputs": [],
   "source": [
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "\n",
    "suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n",
    "    optimal_action_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DynX8nIBKxdc"
   },
   "outputs": [],
   "source": [
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "\n",
    "metrics = [\n",
    "    # equivalent to number of steps in bandits problem\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    # measures regret\n",
    "    regret_metric,\n",
    "    # number of times the suboptimal arms are pulled\n",
    "    suboptimal_arms_metric,\n",
    "    # the average return\n",
    "    tf_metrics.AverageReturnMetric(batch_size=environment.batch_size),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi5VVgkeKxde"
   },
   "source": [
    "## 6. Initialize & configure the Replay Buffer\n",
    "Reinforcement learning algorithms use replay buffers to store trajectories of experience when executing a policy in an environment. During training, replay buffers are queried for a subset of the trajectories (either a sequential subset or a sample) to \"replay\" the agent's experience. Sampling from the replay buffer facilitate data re-use and breaks harmful co-relation between sequential data in RL, although in contextual bandits this isn't absolutely required but still helpful.\n",
    "\n",
    "The replay buffer exposes several functions which allow you to manipulate the replay buffer in several ways. Read more on them [here](https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial).\n",
    "\n",
    "In this demo we would be using the `TFUniformReplayBuffer` for which we need to initialize the buffer spec with the spec of the trajectory of the agent's policy, a chosen batch size (number of trajectories to store), and the maximum length of the trajectory. (This is the amount of sequential time steps which will be considered as one data point). So a batch of 3 with 2 time steps each would result in a tensor of shape `(3,2)`. Since unlike regular RL problems, contextual bandits have only one time step we can keep `max_length = 1`. However since this tutorial is to enable you for RL problems as well, let set it to 2. Do not worry, any contextual bandit agent will internally\n",
    "split the time steps inside each data point such that the effective batch size ends up being `(6,1)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P0uSb9eKxdf"
   },
   "source": [
    "Create a Tensorflow based `TFUniformReplayBuffer` and initialize it with an appropriate values:\n",
    "\n",
    "*  `batch_size = 128`\n",
    "* `max_length = 2` ( 2 time steps per item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcsLSlfDKxdf"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "buf = TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=STEPS_PER_LOOP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYYbzaPuKxdh"
   },
   "source": [
    "Now we have a Replay buffer but we also need something to fill it with. Often a common practice is to have \n",
    "the agent interact with and collect experience from the environment, without actually learning from it (i.e. only forward pass). This loop can be either done by you manually as shown [here](https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial#training_the_agent) or you can do it using the `DynamicStepDriver`.\n",
    "The data encountered by the driver at each step is saved in a `NamedTuple` called `Trajectory` and broadcast to a set of observers such as replay buffers and metrics. \n",
    "This trajectory includes the observation from the environment, the action recommended by the policy, the reward obtained, the type of the current and the next step, etc. \n",
    "\n",
    "In order for the driver to fill the replay buffer with data, as well as to compute ongoing metrics, it needs acess to the `add_batch` method of the replay buffer and the metrics. Have a look [here](https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial#data_collection) for more information and example code on how to initialize a step driver with observers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yUkG51PKxdh"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "#  UNCOMMENT THIS BACK WHEN DEBUGGING IS DONE (GABOR)\n",
    "# replay_observer = [buf.add_batch]  # Gabor's debug\n",
    "replay_observer = [buf.add_batch, step_metric] + metrics\n",
    "\n",
    "driver = DynamicStepDriver(\n",
    "    env=environment,\n",
    "    policy=agent.collect_policy,\n",
    "    num_steps=STEPS_PER_LOOP * environment.batch_size,\n",
    "    observers=replay_observer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leUtw6DDKxdk"
   },
   "source": [
    "Here we provide you with a helper function in order to save your agent, the metrics and its lighter policy seperately, while training the model. We make all the aspects into trackable objects and then use checkpoint to save as well warm restart a previous training. For more information on checkpoints and policy savers ( which will be used in the training loop below) refer [here](https://www.tensorflow.org/agents/tutorials/10_checkpointer_policysaver_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWELf3aPKxdk"
   },
   "outputs": [],
   "source": [
    "def restore_and_get_checkpoint_manager(root_dir, agent, metrics, step_metric):\n",
    "    \"\"\"Restores from `root_dir` and returns a function that writes checkpoints.\"\"\"\n",
    "    trackable_objects = {metric.name: metric for metric in metrics}\n",
    "    trackable_objects[AGENT_CHECKPOINT_NAME] = agent\n",
    "    trackable_objects[STEP_CHECKPOINT_NAME] = step_metric\n",
    "    checkpoint = tf.train.Checkpoint(**trackable_objects)\n",
    "    checkpoint_manager = tf.train.CheckpointManager(\n",
    "        checkpoint=checkpoint, directory=root_dir, max_to_keep=5\n",
    "    )\n",
    "    latest = checkpoint_manager.latest_checkpoint\n",
    "\n",
    "    if latest is not None:\n",
    "        print(\"Restoring checkpoint from %s.\", latest)\n",
    "        checkpoint.restore(latest)\n",
    "        print(\"Successfully restored to step %s.\", step_metric.result())\n",
    "    else:\n",
    "        print(\n",
    "            \"Did not find a pre-existing checkpoint. \" \"Starting from scratch.\"\n",
    "        )\n",
    "    return checkpoint_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6ttQ0RjKxdm"
   },
   "outputs": [],
   "source": [
    "checkpoint_manager = restore_and_get_checkpoint_manager(\n",
    "    ROOT_DIR, agent, metrics, step_metric\n",
    ")\n",
    "summary_writer = tf.summary.create_file_writer(ROOT_DIR)\n",
    "summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPJBV8WjKxdo"
   },
   "source": [
    "Now we have all the components ready to start training the model. Here is the process for Training the model\n",
    "1. We first use the DynamicStepdriver instance to collect experience (trajectories) from the environment and fill up the replay buffer.\n",
    "2. We then extract all the stored experience from the replay buffer by specfiying the `batch size` and `num_steps` we initialized the driver with. We extract it as `tf.dataset.Dataset` instance.\n",
    "3. We then iterate on the `tf.dataset.Dataset` and the first sample we draw actually has all the data `batch_size * num_time_steps`\n",
    "4. The agent then trains on the acquired experience\n",
    "5. The replay buffer is cleared to make space for new data\n",
    "6. Log the metrics and store them on disk\n",
    "7. Save the Agent ( via checkpoints) as well as the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hryNDsarKxdo"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "TRAINING_LOOPS = 150\n",
    "\n",
    "for _ in range(TRAINING_LOOPS):\n",
    "    driver.run()\n",
    "    batch_size = driver.env.batch_size\n",
    "\n",
    "    dataset = buf.as_dataset(\n",
    "        sample_batch_size=BATCH_SIZE,\n",
    "        num_steps=STEPS_PER_LOOP,\n",
    "        single_deterministic_pass=True,\n",
    "    )\n",
    "\n",
    "    experience, unused_info = next(iter(dataset))\n",
    "\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    buf.clear()\n",
    "\n",
    "    metric_utils.log_metrics(metrics)\n",
    "    # for m in metrics:\n",
    "    # print(m.name, \": \", m.result())\n",
    "    for metric in metrics:\n",
    "        metric.tf_summaries(train_step=step_metric.result())\n",
    "    checkpoint_manager.save()\n",
    "\n",
    "    # saver.save(os.path.join(ROOT_DIR, \"./\", 'policy_%d' % step_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYlG8vJcKxdq"
   },
   "source": [
    "Now that our model is trained, what if we want to determine which action to take given a new \"context\": for that we will iterate on our dataset to get the next item,\n",
    "    make a timestep out of it by wrapping the results using `ts.TimeStep`. It expects `step_type`, `reward`, `discount`, and `observation` as input: since we are performing prediction you can fill \n",
    "        in dummy values for the first 3: only the observation/context is relevant. Read about how it works [here](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/time_step/TimeStep), and perform the task below\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNwKJjdpKxdr"
   },
   "outputs": [],
   "source": [
    "feature, label = iter(tf_dataset).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv6cTWieKxds"
   },
   "outputs": [],
   "source": [
    "step = ts.TimeStep(\n",
    "    tf.constant(ts.StepType.FIRST, dtype=tf.int32, shape=[1], name=\"step_type\"),\n",
    "    tf.constant(0.0, dtype=tf.float32, shape=[1], name=\"reward\"),\n",
    "    tf.constant(1.0, dtype=tf.float32, shape=[1], name=\"discount\"),\n",
    "    tf.constant(feature, dtype=tf.float32, shape=[1, 54], name=\"observation\"),\n",
    ")\n",
    "\n",
    "agent.policy.action(step).action.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f77CYkOKxdv"
   },
   "source": [
    "One final task : let us upload the tensoboard logs, to get an overview of the performance of our model. We will upload our logs to `tensorboard.dev` and for that you need to \n",
    "copy the following command in terminal and execute it from there, it will give you a link from which you need to copy/paste the authentication code, and once that is done, you will receive the \n",
    "url of your model evaluation, hosted on a public [tensorboard.dev](https://tensorboard.dev/) instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nkHrPBPKxdv"
   },
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir /home/jupyter/tmp/quick_test/v7/ --name \"(optional) My latest experiment\" --description \"(optional) Agent trained\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sol_covtype_notebook.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
