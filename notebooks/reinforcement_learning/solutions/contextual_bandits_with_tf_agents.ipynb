{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOM4j723Kxc2"
   },
   "source": [
    "# Contextual Bandits with TF-agents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftcVe8b6Kxc3"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learn to load a dataset in BigQuery and connect to it using TensorFlow I/O\n",
    "* Learn how to transform a classification dataset into contextual bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifdzP2rRKxc5"
   },
   "source": [
    "*Contextual Bandit (CB)* is a machine learning framework in which a *agent* selects actions (also called *arms*) in order to maximize rewards in the long term. At each round, the agent receives some information about the current state (also called the *context*) and uses this information to select an action. As a consequence of this choice, it receives a *reward*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one hand, contextual bandit is one of the simplest instance of a *reinforcement learning problem* where a single state (or context) is provided to the agent and the play or *episode* stops after the first action has been chosen and the reward gotten. This setting appears in a number of useful problems in the industry, one of the best known being that of ad placements on a website: The different ads to publish on a webpage are the different actions, the context is given by a user features, and the reward is 1 is the user clicks on the published ad and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, contextual bandit is a natural generalization of a classification problem in supervized learning. Namely, consider a data set of points $(x, y)$ where the $x$'s are the features and the $y$'s are the labels in $k$ possible classes. We can setup an associated contextual bandit problem as follows: The CB agent at each time step is given the context $x$. From that information, it needs to select from $k$ possible actions which are the $k$ possible classes. If the agent chooses the correct class for feature $x$, then the reward is $1$, and zero otherwise. The general goal of maximising the long-term cumulative reward for the CB agent is equivalent to that of minimizing the training loss in supervized learning. Contextual bandit is more general than classification though, since in many useful CB settings we actually know the reward only for the  actions we have taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to solve a contextual bandit problem derived from a classification dataset with *Q-learning* and the associated *neural epsilon-greedy strategy* using a powerful reinforcement learning library written in tensorflow: [TensorFlow Agents](https://www.tensorflow.org/agents). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement:** This lab is based on a tutorial originally written by Anant Nawalgaria and Alex Erfurt. We thank them for making their original material available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpEsA3xmKxc6"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let us intall [TensorFlow Agents](https://www.tensorflow.org/agents) if it is not already installed and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze | grep tf_agents || pip install -q tf_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WttMqkUYKxc9"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "\n",
    "# from tensorflow.python.framework.dtypes import int64\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tf_agents.bandits.agents.neural_epsilon_greedy_agent import (\n",
    "    NeuralEpsilonGreedyAgent,\n",
    ")\n",
    "from tf_agents.bandits.environments import environment_utilities as env_util\n",
    "from tf_agents.bandits.environments.classification_environment import (\n",
    "    ClassificationBanditEnvironment,\n",
    ")\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import (\n",
    "    TFUniformReplayBuffer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset into BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we are going to use a classification dataset and turn it into a contextual bandit problem. \n",
    "\n",
    "Our dataset will be the [UCI Machine Learning Repository]( https://archive.ics.uci.edu/ml/datasets/covertype), which associates various cartographic features of a given area with different labels representing different types of forests covering the areas. \n",
    "\n",
    "The original features are as follows (the last column being the label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3142</td>\n",
       "      <td>183</td>\n",
       "      <td>9</td>\n",
       "      <td>648</td>\n",
       "      <td>101</td>\n",
       "      <td>757</td>\n",
       "      <td>223</td>\n",
       "      <td>247</td>\n",
       "      <td>157</td>\n",
       "      <td>1871</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C7757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2156</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1207</td>\n",
       "      <td>187</td>\n",
       "      <td>170</td>\n",
       "      <td>107</td>\n",
       "      <td>960</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C6102</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       3142     183      9                               648   \n",
       "1       2156      18     28                                 0   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                             101                              757   \n",
       "1                               0                             1207   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            223             247            157   \n",
       "1            187             170            107   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type  Cover_Type  \n",
       "0                                1871       Commanche     C7757           1  \n",
       "1                                 960           Cache     C6102           3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../../tfx_pipelines/data/dataset.csv\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step, our CB agent will be given a context $x$ representing an area cartographic features (`Elevation`, `Aspect`, `Slope`, etc.). Then it will have to choose among one of 7 possible forest cover types as defined by the last column (`Cover_type`), and represented by the integer from 0 to 6.\n",
    "\n",
    "For convenience, we have pre-precessed the categorical features `Wilderness_Area` and `Soil_Type` into their one-hot-encoded versions. So the dataset we will use will have more columns (55 exactly) than the original covertype dataset. We will name the columns corresponding to the 54 features from `X0` to `X54`, while the last column `Y` represents the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines our dataset column names and types and displays a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X45</th>\n",
       "      <th>X46</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X0  X1  X2   X3  X4   X5   X6   X7   X8    X9  ...  X45  X46  X47  X48  \\\n",
       "0  2596  51   3  258   0  510  221  232  148  6279  ...    0    0    0    0   \n",
       "1  2590  56   2  212  -6  390  220  235  151  6225  ...    0    0    0    0   \n",
       "\n",
       "   X49  X50  X51  X52  X53  Y  \n",
       "0    0    0    0    0    0  5  \n",
       "1    0    0    0    0    0  5  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SOURCE = \"../data/covertype.csv\"\n",
    "\n",
    "df = pd.read_csv(DATASET_SOURCE, header=None)\n",
    "\n",
    "LABEL_NAME = \"Y\"\n",
    "FEATURE_PREFIX = \"X\"\n",
    "N_SAMPLES, N_COLUMNS = df.shape\n",
    "\n",
    "COLUMN_NAMES = [f\"{FEATURE_PREFIX}{i}\" for i in range(N_COLUMNS - 1)] + [\n",
    "    LABEL_NAME\n",
    "]\n",
    "\n",
    "COLUMN_TYPES = [tf.int64] * N_COLUMNS\n",
    "\n",
    "df.columns = COLUMN_NAMES\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load this dataset into `BigQuery` into the table named\n",
    "\n",
    "```bash\n",
    "PROJECT_ID.DATASET_ID.TABLE_ID\n",
    "```\n",
    "\n",
    "where `DATASET_ID` and `TABLE_ID` are defined in the next cell among other variable like the `DATASET_SCHEMA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOCATION = \"US\"\n",
    "DATASET_SCHEMA = \",\".join([f\"{name}:INTEGER\" for name in COLUMN_NAMES])\n",
    "DATASET_ID = \"contextual_bandit\"\n",
    "TABLE_ID = \"covertype\"\n",
    "\n",
    "os.environ[\"DATASET_LOCATION\"] = DATASET_LOCATION\n",
    "os.environ[\"DATASET_SOURCE\"] = DATASET_SOURCE\n",
    "os.environ[\"DATASET_SCHEMA\"] = DATASET_SCHEMA\n",
    "os.environ[\"DATASET_ID\"] = DATASET_ID\n",
    "os.environ[\"TABLE_ID\"] = TABLE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below run the [bq command line](https://cloud.google.com/bigquery/docs/bq-command-line-tool) to create the dataset and populate the table from `DATASET_SOURCE` using the variable defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATASET_SOURCE \\\n",
    "$DATASET_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a `tf.data.Dataset` connected to the data table we created in our `BigQuery`.\n",
    "\n",
    "For that purpose, we will use [Tensorflow_io](https://github.com/tensorflow/io/tree/v0.15.0/tensorflow_io/bigquery), which offers a connector `BigQueryClient` to  stream data directly out of `BigQuery`:\n",
    "\n",
    "```python\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a`BigQuery` client and then a read session from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 19:18:11.492101: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2022-02-18 19:18:11.492709: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2022-02-18 19:18:11.612444: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "bq_client = BigQueryClient()\n",
    "\n",
    "bq_session = bq_client.read_session(\n",
    "    f\"projects/{PROJECT_ID}\",\n",
    "    PROJECT_ID,\n",
    "    TABLE_ID,\n",
    "    DATASET_ID,\n",
    "    COLUMN_NAMES,\n",
    "    COLUMN_TYPES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our `bq_session` we can create a `tf.data.Dataset` using the `parallel_read_rows` method, which will read our BigQuery rows in parallel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = bq_session.parallel_read_rows(\n",
    "    block_length=N_SAMPLES,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the examples are stored in our `tf_dataset` as `OrderedDict` with keys the column names and values the corresponding row values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 19:18:13.083264: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-18 19:18:13.097697: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-02-18 19:18:13.097748: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('X0', <tf.Tensor: shape=(), dtype=int64, numpy=3244>), ('X1', <tf.Tensor: shape=(), dtype=int64, numpy=54>), ('X10', <tf.Tensor: shape=(), dtype=int64, numpy=1>), ('X11', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X12', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X13', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X14', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X15', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X16', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X17', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X18', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X19', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X2', <tf.Tensor: shape=(), dtype=int64, numpy=40>), ('X20', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X21', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X22', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X23', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X24', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X25', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X26', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X27', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X28', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X29', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X3', <tf.Tensor: shape=(), dtype=int64, numpy=255>), ('X30', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X31', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X32', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X33', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X34', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X35', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X36', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X37', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X38', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X39', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X4', <tf.Tensor: shape=(), dtype=int64, numpy=146>), ('X40', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X41', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X42', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X43', <tf.Tensor: shape=(), dtype=int64, numpy=1>), ('X44', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X45', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X46', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X47', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X48', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X49', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X5', <tf.Tensor: shape=(), dtype=int64, numpy=4039>), ('X50', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X51', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X52', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X53', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X6', <tf.Tensor: shape=(), dtype=int64, numpy=209>), ('X7', <tf.Tensor: shape=(), dtype=int64, numpy=125>), ('X8', <tf.Tensor: shape=(), dtype=int64, numpy=0>), ('X9', <tf.Tensor: shape=(), dtype=int64, numpy=1154>), ('Y', <tf.Tensor: shape=(), dtype=int64, numpy=1>)])\n"
     ]
    }
   ],
   "source": [
    "for example in tf_dataset.take(1):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Configure the `tf_dataset` we instanciated so that\n",
    "1. the examples are stored as couples $(x, y)$ where $x$ is the feature vector with 54 components and $y$ is the label (**Hint:** Use `.map`)\n",
    "1. it loops over the dataset infefinitively (**Hint:** Use `.repeat`)\n",
    "1. it shuffles the dataset (Use `buffer_size=400000`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "R2ytDmxGKxdD"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def features_and_labels(features):\n",
    "    label = features.pop(LABEL_NAME)\n",
    "    return (\n",
    "        tf.cast(tf.stack(tf.nest.flatten(features), axis=0), tf.float32),\n",
    "        tf.cast(label - 1, tf.int32),\n",
    "    )\n",
    "\n",
    "\n",
    "tf_dataset = (\n",
    "    tf_dataset.map(features_and_labels).repeat().shuffle(buffer_size=400000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that now the dataset has the correct form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zZM8zjfrKxdI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 19:18:33.778903: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-02-18 19:18:33.778959: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-02-18 19:18:43.691481: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 158577 of 400000\n",
      "2022-02-18 19:18:53.691623: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 322921 of 400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(54,), dtype=float32, numpy=\n",
      "array([2.526e+03, 7.900e+01, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
      "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
      "       1.400e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
      "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.080e+02,\n",
      "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
      "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.800e+01, 0.000e+00,\n",
      "       0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
      "       0.000e+00, 0.000e+00, 0.000e+00, 5.100e+02, 0.000e+00, 0.000e+00,\n",
      "       0.000e+00, 0.000e+00, 2.380e+02, 2.130e+02, 1.030e+02, 3.000e+02],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 19:18:58.052730: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "for example in tf_dataset.take(1):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do58umOOKxdL"
   },
   "source": [
    "## Initializing and configuring the CB environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do58umOOKxdL"
   },
   "source": [
    "In Tensorflow Agents, there are special classes that provide the contexts and the rewards to the agent. These classe are generally called [environments](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial). \n",
    "\n",
    "The environment defines the type of actions, states/contexts/observations, and rewards allowed in the problem through its `observation_spec`, `action_spec`, and `reward_spec` methods. In TensorFlow agent the environment are specific to a given problem but the agents are generic. This means that if you want to solve your own RL problem with TensorFlow agent, you'll likely have to write the environment that represents your problem and defines the type of actions and states allowed. Then you'll be able to use any of the generic agents in the TensorFlow agents library. The agent will adapt to your problem using `observation_spec`, `action_spec`, and `reward_spec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do58umOOKxdL"
   },
   "source": [
    "In this section, we will instanciate the environment to solve our \"covertype bandit\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do58umOOKxdL"
   },
   "source": [
    "In the TF-Agents bandits library, there is a special environment class named `ClassificationBanditEnvironment` that  turns any multiclass labeled dataset into a bandit environment. The contexts (or observations) will be the features in the dataset, the actions are the label classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do58umOOKxdL"
   },
   "source": [
    "In general, the rewards can be sampled from a probability distribution depending on the actual and the guessed labels. \n",
    "In our case, the rewards will be deterministic: The agent will receive 1 if it chooses the right class, and 0 otherwise. \n",
    "\n",
    "The next cell creates this reward structure using Tensorflow Probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TN0ECw1-KxdL"
   },
   "outputs": [],
   "source": [
    "covertype_reward_distribution = tfd.Independent(\n",
    "    tfd.Deterministic(tf.eye(7)), reinterpreted_batch_ndims=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sample from it, we obtain a $7\\times 7$ identity matrix storing the rewards obtained by the agent if the agent selects class $i$ (corresponding to row $i$) when the actual class is $j$ (corresponding to column $j$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "C8F6MuC3KxdN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 7), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covertype_reward_distribution.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Instanciate the [ClassificationBanditEnvironment](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/environments/classification_environment/ClassificationBanditEnvironment) and invoke its `reward_spec`, `observation_spec`, and `action_spec` methods to make sure they correspond to the covertype bandit problem. \n",
    "(Note that the `ClassificationBanditEnvironment` can process many actions in parallel so it takes a `batch_size` argument to define the number of actions it will process simultaneously. Let set that batch size to 1 for now.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eIBnUkTAKxdQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Define the environment\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "environment = ClassificationBanditEnvironment(\n",
    "    tf_dataset, covertype_reward_distribution, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HHpGGmPCKxdS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.float32, name='reward')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Inspect the reward_spec\n",
    "environment.reward_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(54,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Inspect the observation_spec\n",
    "environment.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(6, dtype=int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Inspect the action_spec\n",
    "environment.action_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the `reset` method on the `environment` you have just instanciated to obtain the first step.\n",
    "Inspect this step `observation` attribute to see which cartographic features the environment has given you to guess the covertype. Then in a next cell guess a possible covertype from 0 to 6, and pass it to the environment using the `step` method, which returns the next step. Inspect the next step `reward` attribute to see if you guessed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 19:19:01.372292: E tensorflow/core/framework/dataset.cc:552] Unimplemented: Cannot compute input sources for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-02-18 19:19:01.372367: E tensorflow/core/framework/dataset.cc:556] Unimplemented: Cannot merge options for dataset of type IO>BigQueryDataset, because the dataset does not implement `InputDatasets`.\n",
      "2022-02-18 19:19:11.226051: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 114622 of 400000\n",
      "2022-02-18 19:19:21.223402: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 223084 of 400000\n",
      "2022-02-18 19:19:31.223374: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 366617 of 400000\n",
      "2022-02-18 19:19:33.211270: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 54), dtype=float32, numpy=\n",
       "array([[ 2.727e+03,  1.300e+01,  1.000e+00,  0.000e+00,  0.000e+00,\n",
       "         0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "         0.000e+00,  0.000e+00,  3.000e+00,  0.000e+00,  0.000e+00,\n",
       "         0.000e+00,  0.000e+00,  0.000e+00,  1.000e+00,  0.000e+00,\n",
       "         0.000e+00,  0.000e+00,  0.000e+00,  3.000e+01,  0.000e+00,\n",
       "         0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "         0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00, -1.000e+00,\n",
       "         0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "         0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "         1.320e+03,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "         2.160e+02,  2.330e+02,  1.540e+02,  1.991e+03]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "first_step = environment.reset()\n",
    "first_step.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "next_step = environment.step(3)\n",
    "next_step.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step retunred by the environment also contains the new context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 54), dtype=float32, numpy=\n",
       "array([[3.083e+03, 1.080e+02, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.200e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 2.550e+02,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 3.700e+01, 0.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 3.276e+03, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 2.400e+02, 2.260e+02, 1.150e+02, 8.340e+02]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_step.observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv5svfBfKxdU",
    "tags": []
   },
   "source": [
    "## Initializing the Agent\n",
    "\n",
    "In the Tensorflow Agents, the classes that implement algorithms to solve the contextual bandit problem posed by the environment are called *agents*.   \n",
    "\n",
    "In this lab, we will will use the [NeuralEpsilonGreedyAgent](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870), which implements the *neural epsilon greedy algorithm*.\n",
    "\n",
    "This algorithm predicts the reward for each possible action given a context as input using a neural network, called a [Q-Network](https://www.tensorflow.org/agents/api_docs/python/tf_agents/networks/q_network/QNetwork), since it outputs the $Q$-values $Q(x, a)$, that is, the predicted rewards for each of the actions $a$ given the an input context $x$.\n",
    "\n",
    "The action is then chosen to be one with maximal $Q$-value with probability $1-\\epsilon$ or a random action with probability $\\epsilon$. This randomness allows the algorithm to explore states that could be promising even though the neural network estimate their reward poorly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Instanciate a [Q-Network](https://www.tensorflow.org/agents/api_docs/python/tf_agents/networks/q_network/QNetwork) so that it takes as \n",
    "* input tensor an observation tensor as defined by the environment \n",
    "* output tensor an action tensor as defined by the environment\n",
    "\n",
    "\n",
    "Configure the structure of the layers as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "LAYERS = (300, 200, 100, 100, 50, 50)\n",
    "\n",
    "network = QNetwork(\n",
    "    input_tensor_spec=environment.observation_spec(),\n",
    "    action_spec=environment.action_spec(),\n",
    "    fc_layer_params=LAYERS,k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Generate a step from the environment using the `step`method, and feed its observation attribute to the $Q$-network.\n",
    "Verify that the tensor of $Q$-values you are getting is of the right shape (you should get a vector with 7 components containing the predicted reward for each of the covertype classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 7), dtype=float32, numpy=\n",
       " array([[ 78.622574, 310.8624  , -93.516   ,  54.339664, 124.87689 ,\n",
       "         246.5174  ,  86.49705 ]], dtype=float32)>,\n",
       " ())"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "action = 2\n",
    "step = environment.step(action)\n",
    "q_values = network(step.observation)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now that we have our `QNetwork` to estimate our action rewards, in the next cell, you will instanciate the agent from the [NeuralEpsilonGreedyAgent](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent) class. You will need to retrieve the `time_step_spec` and the `action_spec` from the environment. The `reward_network` will be the `QNetwork` you instanciated previously. You can take `Adam` as optimizer with a `LEARNING_RATE` of your choice. \n",
    "You will also set the propensity of your agent to explorate rather than exploit the action with highest predicted reward through the value of `EPSILON`. Both the `LEARNING_RATE` and the `EPSILON` greediness are hyper-paramaters that can affect the training of th agent very much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.01\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "# TODO\n",
    "agent = NeuralEpsilonGreedyAgent(\n",
    "    time_step_spec=environment.time_step_spec(),\n",
    "    action_spec=environment.action_spec(),\n",
    "    reward_network=network,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(LEARNING_RATE),\n",
    "    epsilon=EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The agent has a `policy` attribute containing the strategy that the agent will use when confronted to a given context. The `agent.policy` has an`action` method that takes in a `TimeStep` generated by the environment and containing the context. It then issues a `PolicyStep` containing the action chosen by the policy given this contex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_step = agent.policy.action(step)\n",
    "policy_step.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the agent policy uses the `QNetwork` to predict the values of the different actions (or possible covertype classes in our case). However at this stage the `QNetwork` has not been trained and its weights are random. This means that the predicted classes are meaningless.\n",
    "\n",
    "To remediate that, the agent has a `train` method that takes an experience, that is, a triples of a state (or context) $x$, the action taken (or predicted class) $y$, and the obtained reward $r$. It then updates the parameters $\\theta$ of the `QNetwork` $Q_\\theta$ with a gradient update so that the predicted reward $Q_\\theta(x, y)$ used by the agent to guess $y$ in context $x$ increases if $y$ is the correct class (i.e. reward $1$) and decreases if $y$ has been guessed wrongly.\n",
    "\n",
    "Now training one experience at a time may lead to an unstable training, and the algorithm may have difficulty to converge. To stabilize the training one collects a number of these experience in a *experience replay buffer* in a first stage, and then sample batches of these experiences to apply the gradient update to the `QNetwork` parameters in a second stage, as we do in supervized learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi5VVgkeKxde"
   },
   "source": [
    "## Collecting experiences in an experience replay buffer\n",
    "\n",
    "Reinforcement learning algorithms use replay buffers to store trajectories of experience when executing a policy in an environment. During training, replay buffers are queried for a subset of the trajectories (either a sequential subset or a sample) to \"replay\" the agent's experience. Sampling from the replay buffer facilitate data re-use and breaks harmful co-relation between sequential data in RL, although in contextual bandits this isn't absolutely required but still helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi5VVgkeKxde"
   },
   "source": [
    "Tensorflow Agents defines a number of [replay buffer classes](https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial) all sharing a common interface to store and access the experience data. In this lab, we will use [TFUniformReplayBuffer](https://www.tensorflow.org/agents/api_docs/python/tf_agents/replay_buffers/TFUniformReplayBuffer) which samples uniformly experience trajectories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi5VVgkeKxde"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "\n",
    "Initialize a [TFUniformReplayBuffer](https://www.tensorflow.org/agents/api_docs/python/tf_agents/replay_buffers/TFUniformReplayBuffer)  using for `data_spec` the `trajectory_spec` stored in the agent's policy. Then use a `batch_size` of 128, which will be the size of the batches sampled from the replay buffer for each gradient descent update. The`max_length` argument indicates the maximum number of steps we allow to be stored in the replay buffer from a single episode. Since for CB the eposide size is always 1, you'll set this argument to that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "WcsLSlfDKxdf"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "STEPS_PER_LOOP = 2\n",
    "\n",
    "replay_buffer = TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=STEPS_PER_LOOP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYYbzaPuKxdh"
   },
   "source": [
    "Now we have a Replay buffer, but we also need something to fill it with. Often a common practice is to have \n",
    "the agent interact with and collect experiences from the environment, without actually learning from it (that is without updating the parameters of the `QNetwork`) in a first step. \n",
    "\n",
    "This data-collection loop can be carried out using a [DynamicStepDriver](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver), which will \n",
    "1. feed the `TimeStep` generated by the environment and containing the context data for the new step and the reward for the previous step to the agent,\n",
    "1. collect the action generated by the agent policy from the environment contex, and then \n",
    "1. feed that action back to the environment.  \n",
    "\n",
    "All that in a loop. \n",
    "\n",
    "The data encountered by the driver at each step is saved in a `NamedTuple` called [Trajectory](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/Trajectory) and broadcast to a set of observers such as replay buffers. \n",
    "This trajectory includes the observation from the environment, the action recommended by the policy, the reward obtained, the type of the current and the next step, etc. \n",
    "\n",
    "In order for the driver to fill the replay buffer with data it needs acess to the [add_batch method](https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial#data_collection) of the replay buffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Instanciate [DynamicStepDriver](https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers/dynamic_step_driver/DynamicStepDriver) using the environment we used so far. Note that the agent has two different policies:\n",
    "* `agent.policy` which is *exploitation policy* policy that outputs the action with maximal predicted reward. This is the policy that should be deployed in production environment and used when trained.\n",
    "* `agent.collect_policy` which is the *exploration policy* that outputs the maximal reward action only $1 - \\epsilon$ of the time, allowing for exploring a wider range of actions. This is the policy that is the most beneficial to use to collect training data, and the one that the `DynmicStepDriver` needs to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "-yUkG51PKxdh"
   },
   "outputs": [],
   "source": [
    "observer = [replay_buffer.add_batch]\n",
    "\n",
    "# TODO\n",
    "driver = DynamicStepDriver(\n",
    "    env=environment,\n",
    "    policy=agent.collect_policy,\n",
    "    num_steps=100,  # STEPS_PER_LOOP * environment.batch_size,\n",
    "    observers=observer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=BATCH_SIZE,\n",
    "    num_steps=STEPS_PER_LOOP,\n",
    "    single_deterministic_pass=True,\n",
    ")\n",
    "\n",
    "experience, unused_info = next(iter(dataset))\n",
    "\n",
    "train_loss = agent.train(experience).loss\n",
    "\n",
    "buf.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimentations, the batchize was set to ... now for real training, let us redefine all the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "R2ytDmxGKxdD"
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "ROOT_DIR = f\"./contextual_bandit_checkpoints/{TIMESTAMP}\"\n",
    "\n",
    "TRAINING_LOOPS = 10\n",
    "STEPS_PER_LOOP = 2\n",
    "AGENT_ALPHA = 10.0\n",
    "\n",
    "\n",
    "AGENT_CHECKPOINT_NAME = \"agent\"\n",
    "STEP_CHECKPOINT_NAME = \"step\"\n",
    "CHECKPOINT_FILE_PREFIX = \"ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leUtw6DDKxdk"
   },
   "source": [
    "Here we provide you with a helper function in order to save your agent, the metrics and its lighter policy seperately, while training the model. We make all the aspects into trackable objects and then use checkpoint to save as well warm restart a previous training. For more information on checkpoints and policy savers ( which will be used in the training loop below) refer [here](https://www.tensorflow.org/agents/tutorials/10_checkpointer_policysaver_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWELf3aPKxdk"
   },
   "outputs": [],
   "source": [
    "def restore_and_get_checkpoint_manager(root_dir, agent, metrics, step_metric):\n",
    "    \"\"\"Restores from `root_dir` and returns a function that writes checkpoints.\"\"\"\n",
    "    trackable_objects = {metric.name: metric for metric in metrics}\n",
    "    trackable_objects[AGENT_CHECKPOINT_NAME] = agent\n",
    "    trackable_objects[STEP_CHECKPOINT_NAME] = step_metric\n",
    "    checkpoint = tf.train.Checkpoint(**trackable_objects)\n",
    "    checkpoint_manager = tf.train.CheckpointManager(\n",
    "        checkpoint=checkpoint, directory=root_dir, max_to_keep=5\n",
    "    )\n",
    "    latest = checkpoint_manager.latest_checkpoint\n",
    "\n",
    "    if latest is not None:\n",
    "        print(\"Restoring checkpoint from %s.\", latest)\n",
    "        checkpoint.restore(latest)\n",
    "        print(\"Successfully restored to step %s.\", step_metric.result())\n",
    "    else:\n",
    "        print(\n",
    "            \"Did not find a pre-existing checkpoint. \" \"Starting from scratch.\"\n",
    "        )\n",
    "    return checkpoint_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6ttQ0RjKxdm"
   },
   "outputs": [],
   "source": [
    "checkpoint_manager = restore_and_get_checkpoint_manager(\n",
    "    ROOT_DIR, agent, metrics, step_metric\n",
    ")\n",
    "summary_writer = tf.summary.create_file_writer(ROOT_DIR)\n",
    "summary_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPJBV8WjKxdo"
   },
   "source": [
    "Now we have all the components ready to start training the model. Here is the process for Training the model\n",
    "1. We first use the DynamicStepdriver instance to collect experience (trajectories) from the environment and fill up the replay buffer.\n",
    "2. We then extract all the stored experience from the replay buffer by specfiying the `batch size` and `num_steps` we initialized the driver with. We extract it as `tf.dataset.Dataset` instance.\n",
    "3. We then iterate on the `tf.dataset.Dataset` and the first sample we draw actually has all the data `batch_size * num_time_steps`\n",
    "4. The agent then trains on the acquired experience\n",
    "5. The replay buffer is cleared to make space for new data\n",
    "6. Log the metrics and store them on disk\n",
    "7. Save the Agent ( via checkpoints) as well as the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYsBS9eRKxdX",
    "tags": []
   },
   "source": [
    "## Metrics\n",
    "\n",
    "Just like you have metrics like accuracy/recall in supervised learning, in bandits we use the [regret](https://www.tensorflow.org/agents/tutorials/bandits_tutorial#regret_metric) metric per episode. To calculate the regret, we need to know what the highest possible expected reward is in every time step. For that, we define the `optimal_reward_fn`.\n",
    "\n",
    "Another similar metric is the number of times a suboptimal action was chosen. That requires the definition if the `optimal_action_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-l0e78osKxdX"
   },
   "outputs": [],
   "source": [
    "optimal_reward_fn = functools.partial(\n",
    "    env_util.compute_optimal_reward_with_classification_environment,\n",
    "    environment=environment,\n",
    ")\n",
    "\n",
    "optimal_action_fn = functools.partial(\n",
    "    env_util.compute_optimal_action_with_classification_environment,\n",
    "    environment=environment,\n",
    ")\n",
    "\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "\n",
    "suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n",
    "    optimal_action_fn\n",
    ")\n",
    "\n",
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "\n",
    "metrics = [\n",
    "    # equivalent to number of steps in bandits problem\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    # measures regret\n",
    "    regret_metric,\n",
    "    # number of times the suboptimal arms are pulled\n",
    "    suboptimal_arms_metric,\n",
    "    # the average return\n",
    "    tf_metrics.AverageReturnMetric(batch_size=environment.batch_size),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_observer = [replay_buffer.add_batch, step_metric] + metrics\n",
    "\n",
    "driver = DynamicStepDriver(\n",
    "    env=environment,\n",
    "    policy=agent.collect_policy,\n",
    "    num_steps=STEPS_PER_LOOP * environment.batch_size,\n",
    "    observers=replay_observer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hryNDsarKxdo"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "TRAINING_LOOPS = 150\n",
    "\n",
    "for _ in range(TRAINING_LOOPS):\n",
    "    driver.run()\n",
    "    batch_size = driver.env.batch_size\n",
    "\n",
    "    dataset = buf.as_dataset(\n",
    "        sample_batch_size=BATCH_SIZE,\n",
    "        num_steps=STEPS_PER_LOOP,\n",
    "        single_deterministic_pass=True,\n",
    "    )\n",
    "\n",
    "    experience, unused_info = next(iter(dataset))\n",
    "\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    buf.clear()\n",
    "\n",
    "    metric_utils.log_metrics(metrics)\n",
    "    # for m in metrics:\n",
    "    # print(m.name, \": \", m.result())\n",
    "    for metric in metrics:\n",
    "        metric.tf_summaries(train_step=step_metric.result())\n",
    "    checkpoint_manager.save()\n",
    "\n",
    "    # saver.save(os.path.join(ROOT_DIR, \"./\", 'policy_%d' % step_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYlG8vJcKxdq"
   },
   "source": [
    "Now that our model is trained, what if we want to determine which action to take given a new \"context\": for that we will iterate on our dataset to get the next item,\n",
    "    make a timestep out of it by wrapping the results using `ts.TimeStep`. It expects `step_type`, `reward`, `discount`, and `observation` as input: since we are performing prediction you can fill \n",
    "        in dummy values for the first 3: only the observation/context is relevant. Read about how it works [here](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/time_step/TimeStep), and perform the task below\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNwKJjdpKxdr"
   },
   "outputs": [],
   "source": [
    "feature, label = iter(tf_dataset).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv6cTWieKxds"
   },
   "outputs": [],
   "source": [
    "step = ts.TimeStep(\n",
    "    tf.constant(ts.StepType.FIRST, dtype=tf.int32, shape=[1], name=\"step_type\"),\n",
    "    tf.constant(0.0, dtype=tf.float32, shape=[1], name=\"reward\"),\n",
    "    tf.constant(1.0, dtype=tf.float32, shape=[1], name=\"discount\"),\n",
    "    tf.constant(feature, dtype=tf.float32, shape=[1, 54], name=\"observation\"),\n",
    ")\n",
    "\n",
    "agent.policy.action(step).action.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f77CYkOKxdv"
   },
   "source": [
    "One final task : let us upload the tensoboard logs, to get an overview of the performance of our model. We will upload our logs to `tensorboard.dev` and for that you need to \n",
    "copy the following command in terminal and execute it from there, it will give you a link from which you need to copy/paste the authentication code, and once that is done, you will receive the \n",
    "url of your model evaluation, hosted on a public [tensorboard.dev](https://tensorboard.dev/) instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nkHrPBPKxdv"
   },
   "outputs": [],
   "source": [
    "!tensorboard dev upload --logdir /home/jupyter/tmp/quick_test/v7/ --name \"(optional) My latest experiment\" --description \"(optional) Agent trained\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sol_covtype_notebook.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
