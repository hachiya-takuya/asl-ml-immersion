{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02aa307c-a2b7-4927-98f5-51b27b908ba0",
   "metadata": {},
   "source": [
    "# Prompt Engineering with LLMs\n",
    "\n",
    "**Learning Objective**\n",
    "\n",
    "1. Learn how query the Vertex PaLM API\n",
    "1. Learn how to setup the PaLM API parameters \n",
    "1. Learn prompt engineering for text generation\n",
    "1. Learn prompt engineering for chat applications\n",
    "\n",
    "\n",
    "The Vertex AI PaLM API lets you test, customize, and deploy instances of Google's PaLM large language models (LLM) so that you can leverage the capabilities of PaLM in your applications. The PaLM family of models supports text completion, multi-turn chat, and text embeddings generation.\n",
    "\n",
    "This notebook will provide examples of accessing pre-trained PaLM models with the API for use cases like text classification, summarization, extraction, and chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30622e-7ae5-4092-bebf-80ecd3b874f6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee081e-f7d1-44a8-8bee-57a3b6fbd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import (\n",
    "    ChatModel,\n",
    "    ChatSession,\n",
    "    InputOutputTextPair,\n",
    "    TextGenerationModel,\n",
    ")\n",
    "\n",
    "print(aiplatform.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da340d5-828a-48cf-a030-e0596a918050",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "The cell below implements the helper function `generate` to generate responses from the PaLM API. \n",
    "The PaLM API has a number of parameters to set up. Here are their meanings:\n",
    "\n",
    "The input parameters are as follows:\n",
    "\n",
    "* `prompt`: Text input to generate model response. Prompts can include preamble, questions, suggestions, instructions, or examples.\n",
    "\n",
    "* `temperature`: The temperature is used for sampling during the response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic: the highest probability response is always selected. For most use cases, try starting with a temperature of 0.2.\n",
    "\n",
    "* `max_output_tokens`: Maximum number of tokens that can be generated in the response. Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n",
    "\n",
    "* `top_k`: Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses.\n",
    "\n",
    "* `top_p`: Top-p changes how the model selects tokens for output. Tokens are selected from the most probable until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e441352-fcf1-44da-8c67-64386adf56bc",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the function below as follows:\n",
    "* You'll first create an instance of [TextGenerationModel](https://cloud.google.com/vertex-ai/docs/generative-ai/text/test-text-prompts) from the pretrained `model_name`\n",
    "* then you'll instantiate a `response` using `model.predict` to which you'll pass the prompt, temperature, max_output_tokens, top_p, and top_k arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6954a-a9ca-41dd-aa61-e6538e514f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    model_name=\"text-bison@001\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=256,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "):\n",
    "    model = None  # TODO\n",
    "    response = None  # TODO\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cff7e-8203-4ec8-8dbd-82dd14e3b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    \"What are five important things to understand about large language models?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e32f10-3419-4f9b-9be2-334b4e1b0f01",
   "metadata": {},
   "source": [
    "### Text Classification \n",
    "\n",
    "Now that we've tested our wrapper function, let us explore prompting for classification. Text classification is a common machine learning use-case and is frequently used for tasks like spam detection, sentiment analysis, topic classification, and more. \n",
    "\n",
    "Both **zero-shot** and **few-shot** prompting are common with text classification use cases. Zero-shot prompting is where you do not provide examples with labels in the input, and few-shot prompting is where you provide (a few) examples in the input. \n",
    "\n",
    "Additionally, for text classification use cases we can reduce `max_output_tokens` if we simply want the predicted class (because labels are typically a few words or less)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095984db-e3f1-431c-8464-6ed9c745b380",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a zero-shot prompt that allows you to categorize a text into the following categories: \"technology\", \"polictics\", and \"sport\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc027f3b-84ec-46ab-95b1-5d6afd6d1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d73f1-11ce-4581-90da-d69e76b57247",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a few-shot prompting for classification. Along with increasing the accuracy of your model, few-shot prompting gives you a certain control over the output format. The prompt should be able to classify a text into the categories \"dogs\" and \"cats\", and return only these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9c371-dc6c-4ef7-98ad-dc8058b88bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What is the topic for a given text? \n",
    "- cats \n",
    "- dogs \n",
    "\n",
    "Text: They always sits on my lap and purr!\n",
    "The answer is: cats\n",
    "\n",
    "Text: They love to play fetch\n",
    "The answer is: dogs\n",
    "\n",
    "Text: I throw the frisbee in the water and they swim for hours! \n",
    "The answer is: dogs \n",
    "\n",
    "Text: They always knock things off my counter!\n",
    "The answer is:\n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff60e3-2e6d-4d70-988b-4940adb67c13",
   "metadata": {},
   "source": [
    "### Text Summarization\n",
    "\n",
    "### Exercise\n",
    "\n",
    "PaLM can also be used for text summarization use cases. Text summarization produces a concise and fluent summary of a longer text document. \n",
    "In the cell below, write a prompt that can summarize the following text:\n",
    "\n",
    "```\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04163aa1-b750-4033-93b3-c08e572872c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c16a-67cc-4bdb-ae66-4882d61279de",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Modify the prompt in the cell above so that it outputs 4 bullet point summary of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d03ec-ae6c-4d47-8d2e-09ebec182d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Provide four bullet points summarizing the following:\n",
    "\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df5150-b6ff-499e-adad-6ae6d5cc7b0c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Consider the following dialog between a customer and service representative:\n",
    "\n",
    "```\n",
    "Kyle: Hi! I'm reaching out to customer service because I am having issues.\n",
    "\n",
    "Service Rep: What seems to be the problem? \n",
    "\n",
    "Kyle: I am trying to use the PaLM API but I keep getting an error. \n",
    "\n",
    "Service Rep: Can you share the error with me? \n",
    "\n",
    "Kyle: Sure. The error says: \"ResourceExhausted: 429 Quota exceeded for \n",
    "      aiplatform.googleapis.com/online_prediction_requests_per_base_model \n",
    "      with base model: text-bison\"\n",
    "      \n",
    "Service Rep: It looks like you have exceeded the quota for usage. Please refer to \n",
    "             https://cloud.google.com/vertex-ai/docs/quotas for information about quotas\n",
    "             and limits. \n",
    "             \n",
    "Kyle: Can you increase my quota?\n",
    "\n",
    "Service Rep: I cannot, but let me follow up with somebody who will be able to help.\n",
    "\n",
    "```\n",
    "\n",
    "Write a prompt that can give a short summary of what was said along with todo items for \n",
    "the support representative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594633f-ee1b-4a18-a11c-429b80e512e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd094d3-3026-4ff3-9306-357c50bc921d",
   "metadata": {},
   "source": [
    "### Text Extraction \n",
    "PaLM can be used to extract and structure text. Text extraction can be used for a variety of purposes. One common purpose is to convert documents into a machine-readable format. This can be useful for storing documents in a database or for processing documents with software. Another common purpose is to extract information from documents. This can be useful for finding specific information in a document or for summarizing the content of a document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003d56b-67cc-4dc5-9a29-288b5043af71",
   "metadata": {},
   "source": [
    "###Exercise\n",
    "\n",
    "Consider the following recipe:\n",
    "\n",
    "```\n",
    "Ingredients:\n",
    "* 1 tablespoon olive oil\n",
    "* 1 onion, chopped\n",
    "* 2 carrots, chopped\n",
    "* 2 celery stalks, chopped\n",
    "* 1 teaspoon ground cumin\n",
    "* 1/2 teaspoon ground coriander\n",
    "* 1/4 teaspoon turmeric powder\n",
    "* 1/4 teaspoon cayenne pepper (optional)\n",
    "* Salt and pepper to taste\n",
    "* 1 (15 ounce) can black beans, rinsed and drained\n",
    "* 1 (15 ounce) can kidney beans, rinsed and drained\n",
    "* 1 (14.5 ounce) can diced tomatoes, undrained\n",
    "* 1 (10 ounce) can diced tomatoes with green chilies, undrained\n",
    "* 4 cups vegetable broth\n",
    "* 1 cup chopped fresh cilantro\n",
    "```\n",
    "\n",
    "Write a zero-shot prompt that can return the ingredients in JSON format with keys:\n",
    "\"ingredient\", \"quantity\", and \"type\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b19b1-79cb-400a-87c7-6b13fe756c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669e27e-3bfe-4c95-8c11-dd424cc8a735",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Consider a product description of the type\n",
    "\n",
    "```\n",
    " Google Nest WiFi, network speed up to 1200Mpbs, 2.4GHz and 5GHz frequencies, WP3 protocol\n",
    "```\n",
    "\n",
    "Write a few-shot prompt that can output a the product characteristics in JSON format, as for example:\n",
    "\n",
    "```python\n",
    "JSON: {\n",
    "  \"product\":\"Google Nest WiFi\",\n",
    "  \"speed\":\"1200Mpbs\",\n",
    "  \"frequencies\": [\"2.4GHz\", \"5GHz\"],\n",
    "  \"protocol\":\"WP3\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc370e-b071-4377-b9c0-10e53c76c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdb835-a706-4f00-8f19-972f3fd447e4",
   "metadata": {},
   "source": [
    "## Prompt engineering for chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba2c36-d280-433f-b124-0106c8ce2c9c",
   "metadata": {},
   "source": [
    "The Vertex AI PaLM API for chat is optimized for multi-turn chat. Multi-turn chat is when a model tracks the history of a chat conversation and then uses that history as the context for responses.\n",
    "\n",
    "PaLM API chat prompts are composed of the following three components:\n",
    "\n",
    "* **Messages (required)**: Messages are the list of author-content pairs. The model responds to the current message, which is the last pair in the messages list. The pairs before the last pair comprise the chat session history. \n",
    "\n",
    "* **Context (optional)**: Context allows you to tell a model how to respond or what to refer to when it responds. Context enables you to do things like: specify words that the model can and can't use, specify topics to avoid or focus on, specify the style/tone/format, assume a character/figure, and more.\n",
    "\n",
    "* **Examples (optional)**: List of input-output pairs that demonstrate the model behavior you want to see. This is similar to few-shot learning. \n",
    "\n",
    "The following cell implements a helper function that creates a chat session with a specified language model\n",
    "and parameters. Within a chat session, the model keeps context and remembers\n",
    "the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10320fe-5403-4ab9-991f-05c3828336b0",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the function below so that it returns a `ChatSession` instance configured with the parameters passed as input of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb4f8-df82-43c6-acc6-d93854d18c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_session(\n",
    "    model_name=\"chat-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.0,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    context=None,\n",
    "    examples=None,\n",
    "):\n",
    "    model = ChatModel.from_pretrained(model_name)\n",
    "\n",
    "    return ChatSession(\n",
    "        # TODO\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7e0ac-98af-4a66-867e-f504ffea5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session = create_chat_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a9402-04b4-4e6e-bb95-2244c4f70460",
   "metadata": {},
   "source": [
    "After creating the `ChatSession` instance, you can converse with PaLM using the `.send_message` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f03267-2469-4b9a-9f37-eef780a2feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\"Hello, my name is Kyle!\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526265fd-85ff-452e-94ed-906d690361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\n",
    "    \"\"\"\n",
    "    Good to meet you too. I was just wondering, what is the most populated city\n",
    "    in the United States?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f969c-1b99-47f0-b2f7-53a3585ef846",
   "metadata": {},
   "source": [
    "Recall that within a chat session, history is preserved. This enables the model to remember things within a given chat session for context. You can see this history in the `message_history` attribute of the chat session object. Notice that the history is simply a list of previous input/output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b7772-8151-42b0-83d8-861a06abe801",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session.message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d279ee-4ac7-4b7e-901e-261d4b88191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\"What question did I ask you last?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86a0c0-5668-4efa-8049-1eb7b4d56cdc",
   "metadata": {},
   "source": [
    "### Adding Context and Examples\n",
    "\n",
    "Adding context and examples can help customize the chat model to specified needs. Context can be used to do things like apply specific tones/styles or avoid specific word/phrase usage (you can get very creative!). Examples provide the model with input/output pairs that demonstrate the type of model behavior you want to see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc034e31-5593-4b36-90c5-22ad4c58b1f1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the `context` as well as the `input_text`, `output_text` examples in the cell below, so that the chatbot is primed to believe its name is Electra and that it can answer physic questions very well, using a lot of exclamation marks in its responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b641bdf-fca4-46a6-aad5-e310456f6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"TODO\",\n",
    "        output_text=\"TODO\",\n",
    "    ),\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"TODO\",\n",
    "        output_text=\"TODO\",\n",
    "    ),\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"TODO\",\n",
    "        output_text=\"TODO\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_session = create_chat_session(context=context, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a4cc8-412b-4f4e-83ce-2c0654ded06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\n",
    "    \"Hi, my name is Kyle! What can you help me with?\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4e892-5e52-451e-8195-6c4d2e37144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\"What is thermodynamics?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf8e05-7742-4e2c-aa0b-c1902acd7639",
   "metadata": {},
   "source": [
    "### Customer Service Context\n",
    "\n",
    "While the above example demonstrates the idea of context and examples, it is perhaps not useful in the real world. Lets see if we can use context and examples for a more practical case - a customer service agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c5df2-e270-4c2f-9469-5b5eb4cc4b3e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the same exercise as above, except that now you'll prime the chatbot to be `Billy`, a customer service chatbot for `Bills Books`, that can only answer questions about Bills Books and its products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5fac2-3a6b-4f60-85b8-d399596fb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You a Billy, a customer service chatbot for Bills Books. You only answer customer questions about Bills Books and its products.\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"What is the capital of Washington State?\",\n",
    "        output_text=\"Sorry, I only answer questions about Bills Books.\",\n",
    "    ),\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"Do you sell video games?\",\n",
    "        output_text=\"Sorry, we only sell books.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_session = create_chat_session(context=context, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfc2e6-6cc0-4492-94b2-fa5383e70647",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\"Where should I go on my next vacation?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5b0e2-1394-4015-898d-37901352fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\"What's a good fantasy novel?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095270f8-be2b-4462-8c14-f4eaf411a3e0",
   "metadata": {},
   "source": [
    "Copyright 2023 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
