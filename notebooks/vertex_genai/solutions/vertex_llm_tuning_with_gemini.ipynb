{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# Tuning and deploy a foundation model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "**Learning Objective**\n",
    "\n",
    "1. Learn how to generate a JSONL file for Gemini tuning\n",
    "1. Learn how to launch a tuning job\n",
    "1. Learn how to deploy and query a tuned LLM\n",
    "1. Learn how to evaluate a tuned LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e",
    "tags": []
   },
   "source": [
    "Creating an LLM requires massive amounts of data, significant computing resources, and specialized skills. In this notebook, you'll learn how tuning allows you to customize a Gemini foundation model on Vertex Generative AI studio for more specific tasks or knowledge domains.\n",
    "While the prompt design is excellent for quick experimentation, if training data is available, you can achieve higher quality by tuning the model. Tuning a model enables you to customize the model response based on examples of the task you want the model to perform.\n",
    "\n",
    "For more details on tuning have a look at the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from IPython.display import Markdown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from vertexai.preview.tuning import sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BUCKET_NAME = PROJECT_ID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSRiXkavaalH",
    "outputId": "8b752c8a-d575-4982-85f8-5a40317c8ac3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dherin-dev/cls_datasheets.csv\n",
      "gs://dherin-dev/cord19_embeddings.json\n",
      "gs://dherin-dev/data.csv\n",
      "gs://dherin-dev/salads.csv\n",
      "gs://dherin-dev/tune_data_stack_overflow_python_qa.jsonl\n",
      "gs://dherin-dev/115851500182/\n",
      "gs://dherin-dev/7737964263322419200-616112577574862848/\n",
      "gs://dherin-dev/autosxs-1713380879483659/\n",
      "gs://dherin-dev/autosxs-1713383655691384/\n",
      "gs://dherin-dev/autosxs-1713384745904386/\n",
      "gs://dherin-dev/autosxs-human-eval-1713385691277474/\n",
      "gs://dherin-dev/autosxs-p8fk0gjz/\n",
      "gs://dherin-dev/babyweight/\n",
      "gs://dherin-dev/babyweight_220707_021136/\n",
      "gs://dherin-dev/babyweight_220707_021151/\n",
      "gs://dherin-dev/babyweight_220707_021154/\n",
      "gs://dherin-dev/car_damage_lab_images/\n",
      "gs://dherin-dev/classification-bert-20230411003650/\n",
      "gs://dherin-dev/contextual_bandit_checkpoints/\n",
      "gs://dherin-dev/covertype/\n",
      "gs://dherin-dev/flowers/\n",
      "gs://dherin-dev/models/\n",
      "gs://dherin-dev/movies/\n",
      "gs://dherin-dev/staging/\n",
      "gs://dherin-dev/taxifare-20230710171207/\n",
      "gs://dherin-dev/taxifare-20230710191151/\n",
      "gs://dherin-dev/taxifare/\n",
      "gs://dherin-dev/testingasllib/\n",
      "gs://dherin-dev/vectorstore/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $BUCKET_URI || gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdtNETYxaalH"
   },
   "source": [
    "## Training Data\n",
    "\n",
    "\n",
    "In this notebook, we will be tuning Gemini using the Python SDK on a questions & answers dataset from StackOverflow. \n",
    "Our first step will be to query the StackOverflow data on BigQuery Public Datasets, limiting to questions with the `python` tag, and `accepted` answers from 2020-01-01 only. \n",
    "\n",
    "We will limit the dataset to 1000 samples, 800 of which will be used to tune the LLM and the rest for evaluating the tuned model.\n",
    "The second step will be to convert the dataset into a JSONL format, with one example per line, so that the tuning job can consume it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BydoFfTaalI"
   },
   "source": [
    "Next let us run the query to assemble our dataset into the DataFrame `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f8ec7d8e0240808c0b4088dc499abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b251c7b62a74325b7dc140388a048b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery df\n",
    "\n",
    "SELECT CONCAT(q.title, q.body) as input_text, a.body AS output_text\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "JOIN\n",
    "    `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "ON\n",
    "    q.accepted_answer_id = a.id\n",
    "WHERE\n",
    "    q.accepted_answer_id IS NOT NULL AND\n",
    "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
    "    a.creation_date >= \"2020-01-01\"\n",
    "LIMIT\n",
    "    1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9VTaovLtaalI",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to generate coordinate list from matplotli...</td>\n",
       "      <td>&lt;p&gt;Use &lt;a href=\"https://numpy.org/doc/stable/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write files df in loop&lt;p&gt;I have script that ru...</td>\n",
       "      <td>&lt;p&gt;Here is possible simplify your task:&lt;/p&gt;\\n&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTML Plotly Dash&lt;p&gt;I found one HTML template t...</td>\n",
       "      <td>&lt;p&gt;You'll need to add IDs to some components, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Converting struct format string to range of al...</td>\n",
       "      <td>&lt;p&gt;The math can be simplified.  If &lt;code&gt;b&lt;/co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to understand which line of code output...</td>\n",
       "      <td>&lt;p&gt;This line:&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;print(double_lyr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  How to generate coordinate list from matplotli...   \n",
       "1  Write files df in loop<p>I have script that ru...   \n",
       "2  HTML Plotly Dash<p>I found one HTML template t...   \n",
       "3  Converting struct format string to range of al...   \n",
       "4  I want to understand which line of code output...   \n",
       "\n",
       "                                         output_text  \n",
       "0  <p>Use <a href=\"https://numpy.org/doc/stable/r...  \n",
       "1  <p>Here is possible simplify your task:</p>\\n<...  \n",
       "2  <p>You'll need to add IDs to some components, ...  \n",
       "3  <p>The math can be simplified.  If <code>b</co...  \n",
       "4  <p>This line:</p>\\n<pre><code>print(double_lyr...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `input_text` corresponds to the actual questions asked by the StackOverflow users, while the `output_text` column corresponds to the correct answers. From this dataset of 1000 questions-answers pairs, we will now need to generate a JSONL file with one example per line in the format:\n",
    "\n",
    "```python\n",
    "{\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": input_text,\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"model\",\n",
    "          \"content\": output_text,\n",
    "        }\n",
    "      ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is the format we need to tune a Gemini model.\n",
    "\n",
    "To tune Gemini we advise at least 100 to 500 examples. The more examples you provide in your dataset, the better the results. There is no limit for the number of examples in a training dataset. In this case you will use 800.\n",
    "If possible, also provide a validation dataset. A validation dataset helps you measure the effectiveness of a tuning job. Validation datasets support up to 256 examples.\n",
    "\n",
    "Let's first split the data into training and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXqBwSwaaalJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split is set to 80/20\n",
    "train, evaluation = train_test_split(df, test_size=0.2)\n",
    "print(\"train size:\", len(train))\n",
    "print(\"eval size:\", len(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Unable remove row in proper order from qtablev...</td>\n",
       "      <td>&lt;p&gt;Changing the size and layout of the model s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Display numbered list&lt;p&gt;My last question was s...</td>\n",
       "      <td>&lt;p&gt;this could be like&lt;/p&gt;\\n&lt;pre class=\"lang-py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Calling functions in a django app from url&lt;p&gt;C...</td>\n",
       "      <td>&lt;p&gt;I think you want something like Subdomains!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>install local package in tox unencumbered by p...</td>\n",
       "      <td>&lt;p&gt;If you want to start from a clean state, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>BranchOperator is getting skipped airflow&lt;p&gt;I ...</td>\n",
       "      <td>&lt;p&gt;I cannot reproduce failure in task &lt;code&gt;br...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            input_text  \\\n",
       "261  Unable remove row in proper order from qtablev...   \n",
       "887  Display numbered list<p>My last question was s...   \n",
       "401  Calling functions in a django app from url<p>C...   \n",
       "733  install local package in tox unencumbered by p...   \n",
       "120  BranchOperator is getting skipped airflow<p>I ...   \n",
       "\n",
       "                                           output_text  \n",
       "261  <p>Changing the size and layout of the model s...  \n",
       "887  <p>this could be like</p>\\n<pre class=\"lang-py...  \n",
       "401  <p>I think you want something like Subdomains!...  \n",
       "733  <p>If you want to start from a clean state, yo...  \n",
       "120  <p>I cannot reproduce failure in task <code>br...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf-q8TpnaalJ"
   },
   "source": [
    "For tuning, the training and evaluation data first needs to be converted into a JSONL format. For this, we provide the following two helper functions.\n",
    "The first one converts a single `input_text` and `output_text` record into the JSONL format required by Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_for_gemini(input_text, output_text):\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": input_text,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"model\",\n",
    "                    \"content\": output_text,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The second helper function exports the data into a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tuning_data(file_name, df):\n",
    "    with open(file_name, \"a\") as file:\n",
    "        for row in df.iterrows():\n",
    "            jsonline = format_for_gemini(\n",
    "                row[1][\"input_text\"],\n",
    "                row[1][\"output_text\"],\n",
    "            )\n",
    "            file.write(jsonline)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let us now create our training and evaluation files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data_filename = \"tune_data_stack_overflow_python_qa.jsonl\"\n",
    "evaluation_data_filename = \"evaluation_data_stack_overflow_python_qa.jsonl\"\n",
    "\n",
    "!test -f $training_data_filename    && rm $training_data_filename\n",
    "!test -f $evaluation_data_filename  && rm $evaluation_data_filename\n",
    "\n",
    "export_tuning_data(training_data_filename, train)\n",
    "export_tuning_data(evaluation_data_filename, evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FV8Wxz7JaalN"
   },
   "source": [
    "You can then export the local files to GCS, so that they can be used by Vertex AI for the tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vDDLHac5aalN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://tune_data_stack_overflow_python_qa.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
      "Operation completed over 1 objects/2.3 MiB.                                      \n",
      "Copying file://evaluation_data_stack_overflow_python_qa.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][621.7 KiB/621.7 KiB]                                                \n",
      "Operation completed over 1 objects/621.7 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $training_data_filename   $BUCKET_URI\n",
    "!gsutil cp $evaluation_data_filename $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff68wmzoaalN"
   },
   "source": [
    "You can check to make sure that the files successfully transferred to your Google Cloud Storage bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2-DnKpYlaalN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2439571  2024-08-07T01:32:45Z  gs://dherin-dev/tune_data_stack_overflow_python_qa.jsonl#1722994365137329  metageneration=1\n",
      "TOTAL: 1 objects, 2439571 bytes (2.33 MiB)\n",
      "    636641  2024-08-07T01:32:47Z  gs://dherin-dev/evaluation_data_stack_overflow_python_qa.jsonl#1722994367446931  metageneration=1\n",
      "TOTAL: 1 objects, 636641 bytes (621.72 KiB)\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_URI = f\"{BUCKET_URI}/{training_data_filename}\"\n",
    "EVALUATION_DATA_URI = f\"{BUCKET_URI}/{evaluation_data_filename}\"\n",
    "\n",
    "!gsutil ls -al $TRAINING_DATA_URI\n",
    "!gsutil ls -al $EVALUATION_DATA_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mW7K57BaalN",
    "tags": []
   },
   "source": [
    "### Model Tuning\n",
    "Now it's time to start to tune a model. You will use the Vertex AI SDK to submit our tuning job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "on4baTh5aalN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SupervisedTuningJob\n",
      "SupervisedTuningJob created. Resource name: projects/115851500182/locations/us-central1/tuningJobs/5004357251754885120\n",
      "To use this SupervisedTuningJob in another session:\n",
      "tuning_job = sft.SupervisedTuningJob('projects/115851500182/locations/us-central1/tuningJobs/5004357251754885120')\n",
      "View Tuning Job:\n",
      "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/5004357251754885120?project=115851500182\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-5e8ad878-30b7-429f-85ee-c99ac007470b\" href=\"#view-view-vertex-resource-5e8ad878-30b7-429f-85ee-c99ac007470b\">\n",
       "          <span class=\"material-icons view-vertex-icon\">tune</span>\n",
       "          <span>View Tuning Job</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-5e8ad878-30b7-429f-85ee-c99ac007470b');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/5004357251754885120?project=115851500182');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/5004357251754885120?project=115851500182', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-2d571862-9aa8-4228-92b2-d54c23e571d2\" href=\"#view-view-vertex-resource-2d571862-9aa8-4228-92b2-d54c23e571d2\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-2d571862-9aa8-4228-92b2-d54c23e571d2');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20240806185719729050/runs?project=dherin-dev');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20240806185719729050/runs?project=dherin-dev', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "sft_tuning_job = sft.train(\n",
    "    source_model=\"gemini-1.0-pro-002\",\n",
    "    train_dataset=TRAINING_DATA_URI,\n",
    "    # The following parameters are optional\n",
    "    validation_dataset=EVALUATION_DATA_URI,\n",
    "    epochs=4,\n",
    "    adapter_size=4,\n",
    "    learning_rate_multiplier=1.0,\n",
    "    tuned_model_display_name=\"stackoverflow_tuned_gemini_pro\",\n",
    ")\n",
    "\n",
    "# Polling for job completion\n",
    "while not sft_tuning_job.has_ended:\n",
    "    time.sleep(60)\n",
    "    sft_tuning_job.refresh()\n",
    "\n",
    "print(sft_tuning_job.tuned_model_name)\n",
    "print(sft_tuning_job.tuned_model_endpoint_name)\n",
    "print(sft_tuning_job.experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6JC8XplaalO"
   },
   "source": [
    "## Retrieve the tuned model from your Vertex AI Model registry\n",
    "\n",
    "\n",
    "When your tuning job is finished, your model will be available on Vertex. The next cell shows you how to list tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vertexai.tuning._supervised_tuning.SupervisedTuningJob object at 0x7fc031e445b0> \n",
      "resource name: projects/115851500182/locations/us-central1/tuningJobs/5004357251754885120\n"
     ]
    }
   ],
   "source": [
    "responses = sft.SupervisedTuningJob.list()\n",
    "\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZriyF0V-aalO"
   },
   "source": [
    "It's time to get predictions. First you need to get retrieve the tune model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "j66dr12taalO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_model = GenerativeModel(sft_tuning_job.tuned_model_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDOueoptaalO"
   },
   "source": [
    "Now you can start sending a prompt to the API. Feel free to update the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "2ERbfPJPaalO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I've used Cloud Storage buckets to store data for my TensorFlow jobs running in Google Cloud AI Platform. I think I can help.\n",
       "\n",
       "I think your current code is using the `tf.train` saver. To save the checkpoint in Cloud Storage, you will need to use the `tf.train.Checkpoint` API instead.\n",
       "\n",
       "Here's a snippet:\n",
       "\n",
       "```python\n",
       "import os\n",
       "import tensorflow as tf\n",
       "from tensorflow.python.lib.io import file_io\n",
       "\n",
       "# Set up the bucket name and directory\n",
       "bucket_name = \"your_bucket_name\"\n",
       "checkpoint_dir = \"your_checkpoint_dir\"\n",
       "\n",
       "# Set up the checkpoint\n",
       "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
       "\n",
       "# Set up the checkpoint manager\n",
       "checkpoint_manager = tf.train.CheckpointManager(\n",
       "    checkpoint, directory=os.path.join(file_io.get_hdfs_root(), bucket_name, checkpoint_dir), max_to_keep=3)\n",
       "\n",
       "# Save the checkpoint\n",
       "checkpoint_manager.save()\n",
       "```\n",
       "\n",
       "This will save the checkpoint to the specified bucket and directory.\n",
       "\n",
       "**Note:**\n",
       "\n",
       "* You will need to create the bucket and directory in Cloud Storage first.\n",
       "* You will need to set environment variables `GOOGLE_APPLICATION_CREDENTIALS` and `TF_CONFIG` before running your script. The `GOOGLE_APPLICATION_CREDENTIALS` variable should point to a service account key file with the appropriate permissions to access the bucket. The `TF_CONFIG` variable should specify the Cloud AI Platform job name, which you can find in the Cloud Console.\n",
       "\n",
       "Here's an example of how to set the environment variables:\n",
       "\n",
       "```bash\n",
       "export GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service_account.json\n",
       "export TF_CONFIG='{\"job_name\": \"my_job_name\"}'\n",
       "```\n",
       "\n",
       "For more information on using Cloud AI Platform, please see the [documentation](https://cloud.google.com/ai-platform/training/docs/estimator/using-gpus).\n",
       "\n",
       "I hope this helps!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "How can I store my TensorFlow checkpoint on Google Cloud Storage?\n",
    "\n",
    "Python example:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = tuned_model.generate_content(PROMPT)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manual Evaluation\n",
    "\n",
    "It's essential to evaluate your model to understand its performance. Evaluation can be done in an automated way using evaluation metrics like F1, Bleu, or Rouge. You can also leverage human evaluation methods. Human evaluation methods involve asking humans to rate the quality of the LLM's answers. This can be done through crowdsourcing or by having experts evaluate the responses. Some standard human evaluation metrics include fluency, coherence, relevance, and informativeness. Often you want to choose a mix of evaluation metrics to get a good understanding of your model performance. \n",
    "\n",
    "\n",
    "Among other metrics we will compute the following two metrics that provide crude measures albeit automated of how two texts may have the same meaning: \n",
    "- The [BLEU](https://en.wikipedia.org/wiki/BLEU) evaluation metric is a sort of **precision** metric, measuring the proportion of $n$-grams in the generated sentence matching $n$-grams in the reference sentence. It goes from 0 to 1 with a higher score for more similar sentences. BLEU1 considers uni-grams only, while BLEU2 considers bi-grams. \n",
    "\n",
    "- The [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) evaluation metric is a sort of **recall** metric, measuring the proportion of $n$-grams in the reference sentence that are matched by $n$-grams in the generated sentence. It goes from 0 to 1 with a higher score for more similar sentences. ROUGE1 considers uni-grams only, while ROUGE2 considers bi-grams.\n",
    "\n",
    "\n",
    "We will use  [evaluate](https://github.com/huggingface/evaluate/tree/main) to to compute the scores.\n",
    "Earlier in the notebook, you created a train and eval dataset. Now it's time to take some of the eval data. You will use the questions to get a response from our tuned model, and the answers we will use as a reference:\n",
    "- **Candidates**: Answers generated by the tuned model.\n",
    "- **References**: Original answers that we will use to compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "LKMmIH0XaalO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>How can I join a df made in iteration, to buil...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; col_names = ['Code', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Azure Machine Learning Python Module failing t...</td>\n",
       "      <td>&lt;p&gt;After speaking with Microsoft Support, it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>faster way to fetch the data from postgresql i...</td>\n",
       "      <td>&lt;p&gt;For this query:&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;SELECT scri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>Bracket not showing as closed in Python 3&lt;p&gt;I'...</td>\n",
       "      <td>&lt;p&gt;You are using square brackets inside the fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Output shape of Sequential Network is wrong in...</td>\n",
       "      <td>&lt;p&gt;With the current code, it is the expected o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            input_text  \\\n",
       "771  How can I join a df made in iteration, to buil...   \n",
       "56   Azure Machine Learning Python Module failing t...   \n",
       "440  faster way to fetch the data from postgresql i...   \n",
       "627  Bracket not showing as closed in Python 3<p>I'...   \n",
       "424  Output shape of Sequential Network is wrong in...   \n",
       "\n",
       "                                           output_text  \n",
       "771  <pre><code>&gt;&gt;&gt; col_names = ['Code', '...  \n",
       "56   <p>After speaking with Microsoft Support, it t...  \n",
       "440  <p>For this query:</p>\\n<pre><code>SELECT scri...  \n",
       "627  <p>You are using square brackets inside the fu...  \n",
       "424  <p>With the current code, it is the expected o...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can change the number of rows you want to use\n",
    "EVAL_ROWS = 60\n",
    "INPUT_LIMIT = 10000  # characters\n",
    "evaluation = evaluation[evaluation.input_text.apply(len) <= INPUT_LIMIT]\n",
    "evaluation = evaluation.head(EVAL_ROWS)\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the cell below will query our tuned model using the `evaluation.input_text` and store the ground truth in `evaluation.output_text` in a DataFrame next to the model answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_eval_data(model, evaluation):\n",
    "    model_answers = []\n",
    "\n",
    "    for prompt in evaluation.input_text:\n",
    "        response = model.generate_content(prompt)\n",
    "        model_answers.append(response.text)\n",
    "\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\"candidate\": model_answers, \"reference\": evaluation.output_text}\n",
    "    )\n",
    "    mask = eval_df.candidate == \"\"\n",
    "    return eval_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df = create_eval_data(tuned_model, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>&lt;p&gt;This does it&lt;/p&gt;\\n&lt;pre class=\"lang-py prett...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; col_names = ['Code', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>&lt;p&gt;For anyone else stuck in this issue, it was...</td>\n",
       "      <td>&lt;p&gt;After speaking with Microsoft Support, it t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>&lt;p&gt;I think there are some points to focus on:&lt;...</td>\n",
       "      <td>&lt;p&gt;For this query:&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;SELECT scri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>&lt;p&gt;Your &lt;code&gt;eval_express&lt;/code&gt; function tak...</td>\n",
       "      <td>&lt;p&gt;You are using square brackets inside the fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>&lt;p&gt;You need to reshape your input and labels. ...</td>\n",
       "      <td>&lt;p&gt;With the current code, it is the expected o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             candidate  \\\n",
       "771  <p>This does it</p>\\n<pre class=\"lang-py prett...   \n",
       "56   <p>For anyone else stuck in this issue, it was...   \n",
       "440  <p>I think there are some points to focus on:<...   \n",
       "627  <p>Your <code>eval_express</code> function tak...   \n",
       "424  <p>You need to reshape your input and labels. ...   \n",
       "\n",
       "                                             reference  \n",
       "771  <pre><code>&gt;&gt;&gt; col_names = ['Code', '...  \n",
       "56   <p>After speaking with Microsoft Support, it t...  \n",
       "440  <p>For this query:</p>\\n<pre><code>SELECT scri...  \n",
       "627  <p>You are using square brackets inside the fu...  \n",
       "424  <p>With the current code, it is the expected o...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the next cell computes the uni-gram BLEU and ROUGE scores. It averages these scores over all the reference answers and those generated by our tuned model, giving scores that can serve as performance metrics for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_scores(eval_data):\n",
    "    predictions = eval_data.candidate.tolist()\n",
    "    references = eval_data.reference.tolist()\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    rouge_value = rouge.compute(predictions=predictions, references=references)[\n",
    "        \"rouge1\"\n",
    "    ]\n",
    "    bleu_value = bleu.compute(predictions=predictions, references=references)[\n",
    "        \"bleu\"\n",
    "    ]\n",
    "    return {\"rouge\": rouge_value, \"bleu\": bleu_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634e431cea8743b4b4a000fe159686ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a724ccb3ff14ee488a59cc3b4b965f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0961d081b9a94a9196ad91b91f9cff62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0cef3ef62749929313554b7a982399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'rouge': 0.31049716525058924, 'bleu': 0.2078256786858426}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_scores(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two versions of the model (possibly tuned with a different amount of data or training steps), you can now compare the scores to decide which one is the best. However, remember that these automated metrics are very crude proxy of human assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtYr_KNPaalO",
    "tags": []
   },
   "source": [
    "## Automated Evaluation\n",
    "\n",
    "\n",
    "Let us conclude by noting that a Vertex tuning job collects and reports model tuning and model evaluation metrics, which can then be visualized in Vertex AI Studio by clicking on your tuned model name in the tuning section of Vertex AI Studio.\n",
    "Here is a description of the metrics that are computed:\n",
    "\n",
    "\n",
    "#### Model tuning metrics\n",
    "\n",
    "The model tuning job automatically collects the following tuning metrics for `gemini-1.0-pro-002`.\n",
    "\n",
    "* `/train_total_loss`: Loss for the tuning dataset at a training step.\n",
    "* `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n",
    "* `/train_num_predictions`: Number of predicted tokens at a training step.\n",
    "\n",
    "\n",
    "#### Model validation metrics:\n",
    "\n",
    "You can configure a model tuning job to collect the following validation metrics for gemini-1.0-pro-002 by passing an evaluation dataset as we did in this lab.\n",
    "\n",
    "`/eval_total_loss`: Loss for the validation dataset at a validation step.\n",
    "`/eval_fraction_of_correct_next_step_preds`: The token accuracy at an validation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the validation dataset.\n",
    "`/eval_num_predictions`: Number of predicted tokens at a validation step.\n",
    "\n",
    "\n",
    "The metrics visualizations are available after the model tuning job completes. \n",
    "If you don't specify a validation dataset when you create the tuning job, \n",
    "only the visualizations for the tuning metrics are available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement \n",
    "\n",
    "This notebook is adapted from a [tutorial](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/tuning/getting_started_tuning.ipynb)\n",
    "written by Polong Lin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "source": [
    "Copyright 2023 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "gemini_kernel",
   "name": "tf2-gpu.2-12.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-12:m123"
  },
  "kernelspec": {
   "display_name": "gemini_kernel (Local)",
   "language": "python",
   "name": "gemini_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
