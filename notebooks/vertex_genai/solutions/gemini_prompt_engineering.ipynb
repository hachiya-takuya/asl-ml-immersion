{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02aa307c-a2b7-4927-98f5-51b27b908ba0",
   "metadata": {},
   "source": [
    "# Prompt Engineering with LLMs using Gemini\n",
    "\n",
    "**Learning Objective**\n",
    "\n",
    "1. Learn how query the Vertex Gemini API\n",
    "1. Learn how to setup the Gemini API parameters \n",
    "1. Learn prompt engineering for text generation\n",
    "1. Learn prompt engineering for chat applications\n",
    "\n",
    "\n",
    "The Vertex AI Gemini API lets you test, customize, and deploy instances of Google's Gemini large language models (LLM) so that you can leverage the capabilities of Gemini in your applications. The Gemini family of models supports text completion, multi-turn chat, and text embeddings generation.\n",
    "\n",
    "This notebook will provide examples of accessing pre-trained Gemini models with the API for use cases like text classification, summarization, extraction, and chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30622e-7ae5-4092-bebf-80ecd3b874f6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ee081e-f7d1-44a8-8bee-57a3b6fbd750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 23:23:59.508424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da340d5-828a-48cf-a030-e0596a918050",
   "metadata": {},
   "source": [
    "## Text generation\n",
    "\n",
    "The cell below implements the helper function `generate_content` to generate responses from the Gemini API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea6954a-a9ca-41dd-aa61-e6538e514f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    model_name=\"gemini-1.0-pro\",\n",
    "):\n",
    "    model = GenerativeModel(model_name)\n",
    "    responses = model.generate_content(prompt, stream=True)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96cff7e-8203-4ec8-8dbd-82dd14e3b4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Five key points about large language models:\n",
      "\n",
      "1. **They are trained on massive amounts of text data:** This data can include books, articles, websites, and other forms of text. The more data they are trained on, the better they become at understanding and generating human language.\n",
      "\n",
      "2. **They are not sentient or conscious:** They cannot think or feel for themselves. They simply process and respond to information based on the patterns they have learned from the data they were trained on.\n",
      "\n",
      "3. **They can be biased:** The data they are trained on may contain biases, which can lead to biased outputs. It is important to be aware of this and to use them cautiously.\n",
      "\n",
      "4. **They are constantly evolving:** Developers are constantly improving and updating large language models. This means that their capabilities and limitations are constantly changing.\n",
      "\n",
      "5. **They have the potential to be incredibly beneficial:** They can be used for a variety of tasks, including writing, translation, and customer service. As they continue to improve, they have the potential to make our lives easier and more efficient. \n"
     ]
    }
   ],
   "source": [
    "responses = generate(\n",
    "    \"What are five important things to understand about large language models?\"\n",
    ")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e32f10-3419-4f9b-9be2-334b4e1b0f01",
   "metadata": {},
   "source": [
    "### Text Classification \n",
    "\n",
    "Now that we've tested our wrapper function, let us explore prompting for classification. Text classification is a common machine learning use-case and is frequently used for tasks like spam detection, sentiment analysis, topic classification, and more. \n",
    "\n",
    "Both **zero-shot** and **few-shot** prompting are common with text classification use cases. Zero-shot prompting is where you do not provide examples with labels in the input, and few-shot prompting is where you provide (a few) examples in the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095984db-e3f1-431c-8464-6ed9c745b380",
   "metadata": {},
   "source": [
    "Let us start with zero-shot classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc027f3b-84ec-46ab-95b1-5d6afd6d1156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Classifying the text:\n",
      "\n",
      "Based on the provided text, the most appropriate label would be:\n",
      "\n",
      "**Technology** \n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Zero-shot prompting:** This term is directly related to the field of artificial intelligence and machine learning, specifically within the context of large language models (LLMs). \n",
      "* **Google's Gemini API:**  \"Gemini\" directly refers to a large language model developed by Google AI, further strengthening the connection to technology. APIs are also primarily associated with software and technological applications. \n",
      "* **Politics and Sports:**  There is no indication or connection to either politics or sports within the provided text. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Classify the following: \n",
    "text: \"Zero-shot prompting is really easy with Google's Gemini API.\"\n",
    "label: technology, politics, sports \n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d73f1-11ce-4581-90da-d69e76b57247",
   "metadata": {},
   "source": [
    "Here is now an example of a few-shot prompting for classification. Along with increasing the accuracy of your model, few-shot prompting gives you a certain control over the output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4d9c371-dc6c-4ef7-98ad-dc8058b88bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the topic for a given text? \n",
    "- cats \n",
    "- dogs \n",
    "\n",
    "Text: They always sits on my lap and purr!\n",
    "The answer is: cats\n",
    "\n",
    "Text: They love to play fetch\n",
    "The answer is: dogs\n",
    "\n",
    "Text: I throw the frisbee in the water and they swim for hours! \n",
    "The answer is: dogs \n",
    "\n",
    "Text: They always knock things off my counter!\n",
    "The answer is:\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff60e3-2e6d-4d70-988b-4940adb67c13",
   "metadata": {},
   "source": [
    "### Text Summarization\n",
    "\n",
    "LLMs can also be used for text summarization use cases. Text summarization produces a concise and fluent summary of a longer text document. The prompt inn the cell below simply instruct Gemini to summarize a given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04163aa1-b750-4033-93b3-c08e572872c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  Summary:\n",
      "\n",
      "Transformers are deep learning models that excel at processing sequential data like natural language. Unlike RNNs, they process the entire input simultaneously, thanks to a self-attention mechanism that provides context for every part of the input sequence. This allows for faster training times and makes them suitable for tasks like translation and text summarization. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a very short summary for the following:\n",
    "\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c16a-67cc-4bdb-ae66-4882d61279de",
   "metadata": {},
   "source": [
    "If you need the summary to be in a certain way, as for instance a bullet point summary, you can instruct Gemini to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029d03ec-ae6c-4d47-8d2e-09ebec182d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Transformer Summary:\n",
      "\n",
      "* **Deep Learning Model:** A type of neural network architecture.\n",
      "* **Self-Attention Mechanism:** Differentially weights the importance of different parts of the input data, including the model's output, providing context for each position in the sequence.\n",
      "* **Processes Entire Input at Once:** Unlike RNNs which process data sequentially, transformers process the entire input simultaneously, allowing for faster training through parallelization.\n",
      "* **Applications:** Designed for sequential input data, particularly in natural language processing tasks like translation and text summarization. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide four bullet points summarizing the following:\n",
    "\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df5150-b6ff-499e-adad-6ae6d5cc7b0c",
   "metadata": {},
   "source": [
    "Dialog summarization falls under the category of text summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f594633f-ee1b-4a18-a11c-429b80e512e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## One-liner Summary:\n",
      "\n",
      "Customer encountered quota error while using Gemini API, service rep advises on quota limitations and next steps.\n",
      "\n",
      "## To-Do's for Service Rep:\n",
      "\n",
      "- **Follow up with someone who can increase customer's quota.**\n",
      "- **Share the provided link on quotas and limitations with the customer.**\n",
      "- **Provide an estimated timeframe for resolution.**"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a one-liner summary of the following chat and at the end, summarize to-do's for the service rep: \n",
    "\n",
    "Customer: Hi! I'm reaching out to customer service because I am having issues.\n",
    "\n",
    "Service Rep: What seems to be the problem? \n",
    "\n",
    "Customer: I am trying to use the Gemini API but I keep getting an error. \n",
    "\n",
    "Service Rep: Can you share the error with me? \n",
    "\n",
    "Customer: Sure. The error says: \"ResourceExhausted: 429 Quota exceeded for \n",
    "      aiplatform.googleapis.com/online_prediction_requests_per_base_model \n",
    "      with base model: text-bison\"\n",
    "      \n",
    "Service Rep: It looks like you have exceeded the quota for usage. Please refer to \n",
    "             https://cloud.google.com/vertex-ai/docs/quotas for information about quotas\n",
    "             and limits. \n",
    "             \n",
    "Customer: Can you increase my quota?\n",
    "\n",
    "Service Rep: I cannot, but let me follow up with somebody who will be able to help.\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd094d3-3026-4ff3-9306-357c50bc921d",
   "metadata": {},
   "source": [
    "### Text Extraction \n",
    "Gemini can be used to extract and structure text. Text extraction can be used for a variety of purposes. One common purpose is to convert documents into a machine-readable format. This can be useful for storing documents in a database or for processing documents with software. Another common purpose is to extract information from documents. This can be useful for finding specific information in a document or for summarizing the content of a document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003d56b-67cc-4dc5-9a29-288b5043af71",
   "metadata": {},
   "source": [
    "Let us start with zero-shot extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776b19b1-79cb-400a-87c7-6b13fe756c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"ingredients\": [\n",
      "    {\n",
      "      \"ingredient\": \"Olive oil\",\n",
      "      \"quantity\": \"1 tablespoon\",\n",
      "      \"type\": \"Oil\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Onion\",\n",
      "      \"quantity\": \"1\",\n",
      "      \"type\": \"Vegetable\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Carrot\",\n",
      "      \"quantity\": \"2\",\n",
      "      \"type\": \"Vegetable\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Celery stalks\",\n",
      "      \"quantity\": \"2\",\n",
      "      \"type\": \"Vegetable\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Ground cumin\",\n",
      "      \"quantity\": \"1 teaspoon\",\n",
      "      \"type\": \"Spice\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Ground coriander\",\n",
      "      \"quantity\": \"1/2 teaspoon\",\n",
      "      \"type\": \"Spice\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Turmeric powder\",\n",
      "      \"quantity\": \"1/4 teaspoon\",\n",
      "      \"type\": \"Spice\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Cayenne pepper\",\n",
      "      \"quantity\": \"1/4 teaspoon\",\n",
      "      \"type\": \"Spice\",\n",
      "      \"optional\": true\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Salt\",\n",
      "      \"quantity\": \"To taste\",\n",
      "      \"type\": \"Seasoning\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Pepper\",\n",
      "      \"quantity\": \"To taste\",\n",
      "      \"type\": \"Seasoning\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Black beans\",\n",
      "      \"quantity\": \"1 (15 ounce) can\",\n",
      "      \"type\": \"Canned good\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Kidney beans\",\n",
      "      \"quantity\": \"1 (15 ounce) can\",\n",
      "      \"type\": \"Canned good\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Diced tomatoes\",\n",
      "      \"quantity\": \"1 (14.5 ounce) can\",\n",
      "      \"type\": \"Canned good\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Diced tomatoes with green chilies\",\n",
      "      \"quantity\": \"1 (10 ounce) can\",\n",
      "      \"type\": \"Canned good\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Vegetable broth\",\n",
      "      \"quantity\": \"4 cups\",\n",
      "      \"type\": \"Liquid\"\n",
      "    },\n",
      "    {\n",
      "      \"ingredient\": \"Fresh cilantro\",\n",
      "      \"quantity\": \"1 cup\",\n",
      "      \"type\": \"Herb\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the ingredients from the following recipe. \n",
    "Return the ingredients in JSON format with keys: ingredient, quantity, type.\n",
    "\n",
    "Ingredients:\n",
    "* 1 tablespoon olive oil\n",
    "* 1 onion, chopped\n",
    "* 2 carrots, chopped\n",
    "* 2 celery stalks, chopped\n",
    "* 1 teaspoon ground cumin\n",
    "* 1/2 teaspoon ground coriander\n",
    "* 1/4 teaspoon turmeric powder\n",
    "* 1/4 teaspoon cayenne pepper (optional)\n",
    "* Salt and pepper to taste\n",
    "* 1 (15 ounce) can black beans, rinsed and drained\n",
    "* 1 (15 ounce) can kidney beans, rinsed and drained\n",
    "* 1 (14.5 ounce) can diced tomatoes, undrained\n",
    "* 1 (10 ounce) can diced tomatoes with green chilies, undrained\n",
    "* 4 cups vegetable broth\n",
    "* 1 cup chopped fresh cilantro\n",
    "\"\"\"\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669e27e-3bfe-4c95-8c11-dd424cc8a735",
   "metadata": {},
   "source": [
    "As for classification, few-shot prompting gives you more control on the format of what is extracted: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78fc370e-b071-4377-b9c0-10e53c76c62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"product\": \"Google Pixel 7\",\n",
      "  \"network\": \"5G\",\n",
      "  \"RAM\": \"8GB\",\n",
      "  \"processor\": \"Tensor G2\",\n",
      "  \"storage\": \"128GB\",\n",
      "  \"color\": \"Lemongrass\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the technical specifications from the text below in JSON format.\n",
    "\n",
    "Text: Google Nest WiFi, network speed up to 1200Mpbs, 2.4GHz and 5GHz frequencies, WP3 protocol\n",
    "JSON: {\n",
    "  \"product\":\"Google Nest WiFi\",\n",
    "  \"speed\":\"1200Mpbs\",\n",
    "  \"frequencies\": [\"2.4GHz\", \"5GHz\"],\n",
    "  \"protocol\":\"WP3\"\n",
    "}\n",
    "\n",
    "Text: Google Pixel 7, 5G network, 8GB RAM, Tensor G2 processor, 128GB of storage, Lemongrass\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "responses = generate(prompt)\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdb835-a706-4f00-8f19-972f3fd447e4",
   "metadata": {},
   "source": [
    "## Prompt engineering for chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba2c36-d280-433f-b124-0106c8ce2c9c",
   "metadata": {},
   "source": [
    "The Vertex AI Gemini API for chat is optimized for multi-turn chat. Multi-turn chat is when a model tracks the history of a chat conversation and then uses that history as the context for responses.\n",
    "\n",
    "Gemini enables you to have freeform conversations across multiple turns. The ChatSession class simplifies the process by managing the state of the conversation, so unlike with generate_content, you do not have to store the conversation history as a list.\n",
    "\n",
    "Let's initialize the chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543fb4f8-df82-43c6-acc6-d93854d18c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vertexai.generative_models._generative_models.ChatSession at 0x7f1b7f1a1510>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "chat = model.start_chat(history=[])\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239950b-cb28-4cb5-aedf-0278ed356e06",
   "metadata": {},
   "source": [
    "The ChatSession.send_message method returns the same GenerateContentResponse type as GenerativeModel.generate_content. It also appends your message and the response to the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53f03267-2469-4b9a-9f37-eef780a2feda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A computer is like a super-smart friend that helps you do things by following your instructions step-by-step! \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    \"In one sentence, explain how a computer works to a young child.\"\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825730b6-168f-455b-9ac3-f98a04e5dee2",
   "metadata": {},
   "source": [
    "Recall that within a chat session, history is preserved. This enables the model to remember things within a given chat session for context. You can see this history in the `history` attribute of the chat session object. Notice that the history is simply a list of previous input/output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526265fd-85ff-452e-94ed-906d690361b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[role: \"user\"\n",
       " parts {\n",
       "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
       " },\n",
       " role: \"model\"\n",
       " parts {\n",
       "   text: \"A computer is like a super-smart friend that helps you do things by following your instructions step-by-step! \\n\"\n",
       " }]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d279ee-4ac7-4b7e-901e-261d4b88191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked me to explain in one sentence how a computer works to a young child. \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"What question did I ask you last?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86a0c0-5668-4efa-8049-1eb7b4d56cdc",
   "metadata": {},
   "source": [
    "### Adding Context\n",
    "\n",
    "While the `ChatSession` class shown earlier can handle many use cases, it does make some assumptions. If your use case doesn't fit into this chat implementation it's good to remember that `ChatSession` is just a wrapper around `GenerativeModel.generate_content`. In addition to single requests, it can handle multi-turn conversations.\n",
    "\n",
    "The individual messages are `protos.Content objects` or compatible dictionaries, as seen in previous sections. As a dictionary, the message requires role and parts keys. The role in a conversation can either be the user, which provides the prompts, or model, which provides the responses.\n",
    "\n",
    "Pass a list of `protos.Content` objects and it will be treated as multi-turn chat:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b641bdf-fca4-46a6-aad5-e310456f6615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine\n",
      " a computer is like a really smart toy box.  \n",
      "\n",
      "Inside, there are\n",
      " lots of tiny parts that work together, like little helpers. One part is like\n",
      " a brain, it's called the **CPU**.  It's super fast and can follow instructions really well. \n",
      "\n",
      "Another part is like the toy\n",
      " box itself, it's called the **memory**. It stores all the instructions and information the computer needs.  \n",
      "\n",
      "Then there's the **screen**,\n",
      " which is like a window to the toy box. It shows you what the computer is doing and lets you play with the toys (or games!).\n",
      "\n",
      "When you press a key on the keyboard, it tells the CPU what to do.  \n",
      "The CPU then checks the memory for instructions and tells the other parts what to do.  This makes the computer do all sorts of amazing things, like showing you pictures, playing music, or even talking to you! \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Briefly explain how a computer works to a young child.\"],\n",
    "    }\n",
    "]\n",
    "responses = model.generate_content(str(messages), stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da257d4-e31e-45ed-884b-fac663862e30",
   "metadata": {},
   "source": [
    "To continue the conversation, add the response and another message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d8a4cc8-412b-4f4e-83ce-2c0654ded06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{'role': 'user', 'parts': ['Briefly explain how a computer\n",
      " works to a young child.']}, {'role': 'model', 'parts\n",
      "': ['Imagine a computer like a big brain! It has a special place to store all its thoughts and memories, just like your brain does. This place is\n",
      " called the **memory**. Then, it has a special part that tells it what to do, like when you decide to play a game or draw a picture.\n",
      " This part is called the **processor**.  The computer uses electricity to do all these things.  It's like magic!  Whenever you want to tell the computer what to do, you use the **keyboard** or **mouse**\n",
      " to give it instructions. And it shows you what it's doing on the **screen**.']}, {'role': 'user', 'parts': ['Okay, how about a more detailed explanation to a high school student?']}, {'\n",
      "role': 'model', 'parts': ['A computer works by processing information using **binary code**, which is a system of ones and zeros. Think of it like a set of light switches, each one either on (1) or off (0). These ones and zeros represent instructions and data. \\n\\n\n",
      "The **central processing unit (CPU)** is the brain of the computer. It takes instructions from the **memory** (where data is stored) and executes them. The **memory** is like a temporary workspace where the CPU can access information quickly. \\n\\nWhen you use a computer, you interact with it\n",
      " through **input devices** like the keyboard and mouse. These devices translate your actions into binary code that the CPU can understand. The computer then processes this information and displays the results on the **output device**, such as the monitor or speakers. \\n\\nThe computer also has a **hard drive** or **solid\n",
      " state drive (SSD)**, which acts as long-term storage for data and programs. Think of it like a library where all the important information is kept. \\n\\nIn essence, computers are complex machines that follow instructions in binary code to process information, manipulate data, and produce results.']}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"model\", \"parts\": [response.text]})\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "            \"Okay, how about a more detailed explanation to a high school student?\"\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "responses = model.generate_content(str(messages), stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062fa76-69b4-461c-b2ff-37ace5427fd8",
   "metadata": {},
   "source": [
    "### Adding chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0a97a-fe2c-498c-81bb-2ca4164a7d8e",
   "metadata": {},
   "source": [
    "You can add chat history to a chat by adding messages from role user and model alternately. System messages can be set in the first part for the first message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "380b9209-300b-4107-87bc-36a7c195ae32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, your favorite movies, Lord of the Rings and The Hobbit, are both based on books by J.R.R. Tolkien. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat2 = model.start_chat(\n",
    "    history=[\n",
    "        Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                Part.from_text(\n",
    "                    \"\"\"\n",
    "    My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\n",
    "    Who do you work for?\n",
    "    \"\"\"\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "        Content(role=\"model\", parts=[Part.from_text(\"I work for Ned.\")]),\n",
    "        Content(role=\"user\", parts=[Part.from_text(\"What do I like?\")]),\n",
    "        Content(\n",
    "            role=\"model\", parts=[Part.from_text(\"Ned likes watching movies.\")]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = chat2.send_message(\"Are my favorite movies based on a book series?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea55819d-862e-41ef-be09-0fbc6df3f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Hobbit was first published in 1937, and The Lord of the Rings was published in three volumes between 1954 and 1955. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat2.send_message(\"When were these books published?\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ff364-3db0-49e7-a87e-4a2681595aa5",
   "metadata": {},
   "source": [
    "Copyright 2024 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93c5fac2-3a6b-4f60-85b8-d399596fb681",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InputOutputTextPair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mYou a Billy, a customer service chatbot for Bills Books. You only answer customer questions about Bills Books and its products.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mInputOutputTextPair\u001b[49m(\n\u001b[1;32m      7\u001b[0m         input_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of Washington State?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         output_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSorry, I only answer questions about Bills Books.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     ),\n\u001b[1;32m     10\u001b[0m     InputOutputTextPair(\n\u001b[1;32m     11\u001b[0m         input_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo you sell video games?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         output_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSorry, we only sell books.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     ),\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     16\u001b[0m chat_session \u001b[38;5;241m=\u001b[39m create_chat_session(context\u001b[38;5;241m=\u001b[39mcontext, examples\u001b[38;5;241m=\u001b[39mexamples)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InputOutputTextPair' is not defined"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "You a Billy, a customer service chatbot for Bills Books. You only answer customer questions about Bills Books and its products.\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"What is the capital of Washington State?\",\n",
    "        output_text=\"Sorry, I only answer questions about Bills Books.\",\n",
    "    ),\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"Do you sell video games?\",\n",
    "        output_text=\"Sorry, we only sell books.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_session = create_chat_session(context=context, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfc2e6-6cc0-4492-94b2-fa5383e70647",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\"Where should I go on my next vacation?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5b0e2-1394-4015-898d-37901352fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_session.send_message(\"What's a good fantasy novel?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095270f8-be2b-4462-8c14-f4eaf411a3e0",
   "metadata": {},
   "source": [
    "Copyright 2023 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
