{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b46970-5565-49b4-ae2a-e915e90b6c2f",
   "metadata": {},
   "source": [
    "# Intro to Retrieval Augmented Generation Systems, LangChain & ChromaDB\n",
    "\n",
    "This notebook walks through building a question/answer system that retrieves information to formulate responses, effectively grounding the LLM with specific information. A pre-trained LLM, or likely even a fine-tuned LLM will not be sufficient (in and of itself) when you want a system that understands specific, possibly private data or information that was not in its training dataset.\n",
    "\n",
    "In this lab you will:\n",
    "* Learn about the different components of a retrieval augmented system\n",
    "* Build a simple retrieval augmented generation system \n",
    "* Use LangChain and ChromaDB to simplify and scale the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b2b6e-670e-49c1-ba5c-4e7bd5bc61c9",
   "metadata": {},
   "source": [
    "This lab uses a special kernel with langchain dependencies. Run the cell below to create the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d15f9a-3ab7-4d99-994d-d5d235ddee3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kernels/langchain.sh\n",
      "Installed kernelspec langchain_kernel in /home/jupyter/.local/share/jupyter/kernels/langchain_kernel\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-vertexai 1.0.1 requires langchain-core<0.2.0,>=0.1.42, but you have langchain-core 0.1.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.2 requires langsmith<0.0.84,>=0.0.83, but you have langsmith 0.1.140 which is incompatible.\n",
      "langchain-community 0.0.18 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.140 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.2 requires langsmith<0.0.84,>=0.0.83, but you have langsmith 0.1.140 which is incompatible.\n",
      "langchain-community 0.0.18 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.140 which is incompatible.\n",
      "datasets 2.14.5 requires fsspec[http]<2023.9.0,>=2023.1.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "kfp 2.4.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 31.0.0 which is incompatible.\n",
      "kfp 2.4.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "kfp 2.4.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\n",
      "tf-models-official 2.12.0 requires pyyaml<6.0,>=5.1, but you have pyyaml 6.0.2 which is incompatible.\n",
      "asl 0.1 requires google-cloud-aiplatform==1.43.0, but you have google-cloud-aiplatform 1.71.1 which is incompatible.\n",
      "asl 0.1 requires pyyaml==5.3.1, but you have pyyaml 6.0.2 which is incompatible.\n",
      "ydata-profiling 4.10.0 requires typeguard<5,>=3, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.2 requires langsmith<0.0.84,>=0.0.83, but you have langsmith 0.1.140 which is incompatible.\n",
      "asl 0.1 requires google-cloud-aiplatform==1.43.0, but you have google-cloud-aiplatform 1.71.1 which is incompatible.\n",
      "asl 0.1 requires pyyaml==5.3.1, but you have pyyaml 6.0.2 which is incompatible.\n",
      "ydata-profiling 4.10.0 requires typeguard<5,>=3, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "asl 0.1 requires google-cloud-aiplatform==1.43.0, but you have google-cloud-aiplatform 1.48.0 which is incompatible.\n",
      "asl 0.1 requires pyyaml==5.3.1, but you have pyyaml 6.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.2 requires langsmith<0.0.84,>=0.0.83, but you have langsmith 0.1.140 which is incompatible.\n",
      "kfp 2.4.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 31.0.0 which is incompatible.\n",
      "kfp 2.4.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "kfp 2.4.0 requires urllib3<2.0.0, but you have urllib3 2.2.3 which is incompatible.\n",
      "tf-models-official 2.12.0 requires pyyaml<6.0,>=5.1, but you have pyyaml 6.0.2 which is incompatible.\n",
      "asl 0.1 requires google-cloud-aiplatform==1.43.0, but you have google-cloud-aiplatform 1.48.0 which is incompatible.\n",
      "asl 0.1 requires pyyaml==5.3.1, but you have pyyaml 6.0.2 which is incompatible.\n",
      "ydata-profiling 4.10.0 requires typeguard<5,>=3, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mKernel has been successfully created\n"
     ]
    }
   ],
   "source": [
    "!cd ~/asl-ml-immersion && make langchain_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4e10c-2fce-444a-a302-e153a68b0a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Select the kernel `langchain_kernel` in the top right before going forward in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e8220-6d91-4938-8188-7b3d986b2845",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6855635-3a1c-4295-acb3-999a634db4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fafe53-dee2-4b22-b66a-d43f0b7d6fa4",
   "metadata": {},
   "source": [
    "### Build a simple retrieval augmented generation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168271d5-86a6-451f-ab66-3dbdd902f82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this toy example, we want to ground an LLM on information that an off-the-shelf LLM would not know. For example, instructions left for a house sitter that will be watching two pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428d7ff0-426f-4c0b-bb58-2a6177088e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estrella is a dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finnegan is a cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finnegan gets fed five times daily. Estrella g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estrella usually goes on one long walk per day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please play with Finnegan for 30 minutes each ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                  Estrella is a dog\n",
       "1                                  Finnegan is a cat\n",
       "2  Finnegan gets fed five times daily. Estrella g...\n",
       "3  Estrella usually goes on one long walk per day...\n",
       "4  Please play with Finnegan for 30 minutes each ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of things we want to ground the LLM on.\n",
    "information = [\n",
    "    \"Estrella is a dog\",\n",
    "    \"Finnegan is a cat\",\n",
    "    \"Finnegan gets fed five times daily. Estrella gets fed three times daily.\",\n",
    "    \"Estrella usually goes on one long walk per day, but needs to go outside every 4-6 hours\",\n",
    "    \"Please play with Finnegan for 30 minutes each day. His favorite toy is the fake mouse!\",\n",
    "]\n",
    "\n",
    "information_df = pd.DataFrame({\"text\": information})\n",
    "information_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfca1b4-0600-44fc-909e-730ab4afcd56",
   "metadata": {},
   "source": [
    "At the core of most retrieval generation systems is a vector database. A vector database stores embedded representations of information. \n",
    "\n",
    "Let's add a column to our information dataframe that is an embedded representation of the text. We will use the [Vertex AI text-embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d3e8c6-f8d5-49b2-8cd4-6d9a54ecd6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estrella is a dog</td>\n",
       "      <td>[0.01971660926938057, -0.009960231371223927, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finnegan is a cat</td>\n",
       "      <td>[-0.053840961307287216, -0.014684000052511692,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finnegan gets fed five times daily. Estrella g...</td>\n",
       "      <td>[0.038750406354665756, 0.005375586915761232, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estrella usually goes on one long walk per day...</td>\n",
       "      <td>[0.06929957121610641, -0.02032635547220707, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please play with Finnegan for 30 minutes each ...</td>\n",
       "      <td>[-0.0562220960855484, 0.0075227334164083, 0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                  Estrella is a dog   \n",
       "1                                  Finnegan is a cat   \n",
       "2  Finnegan gets fed five times daily. Estrella g...   \n",
       "3  Estrella usually goes on one long walk per day...   \n",
       "4  Please play with Finnegan for 30 minutes each ...   \n",
       "\n",
       "                                              vector  \n",
       "0  [0.01971660926938057, -0.009960231371223927, -...  \n",
       "1  [-0.053840961307287216, -0.014684000052511692,...  \n",
       "2  [0.038750406354665756, 0.005375586915761232, 0...  \n",
       "3  [0.06929957121610641, -0.02032635547220707, -0...  \n",
       "4  [-0.0562220960855484, 0.0075227334164083, 0.06...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "information_df[\"vector\"] = [\n",
    "    x.values for x in embedding_model.get_embeddings(information)\n",
    "]\n",
    "information_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bab4e4-d701-4cfd-b0fe-71307bd5a1b5",
   "metadata": {},
   "source": [
    "Retrieval systems need a way of finding the most relevant information to answer a given query. This is done with a nearest neighbor (semantic similarity) search. Let's define a function to take in a query (text) input and return a distance metric for each text in our information. We will need to: \n",
    "* Embed the query with the same embedding model used for the information \n",
    "* Computes a distance metric between the query vector and each information vector. We will use cosine similarity, one of the many similarity measures that can be used.\n",
    "* Returns a list of distance metrics between the query vector and each information vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71088247-87d3-4aeb-a5a9-c8dc3f74f4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_and_compute_distances(query: str):\n",
    "    # Get vector for query string\n",
    "    query_embedding = embedding_model.get_embeddings([query])[\n",
    "        0\n",
    "    ].values  # Query embedding\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    # Compute distances between query vector and all information vectors\n",
    "    for _, row in information_df.iterrows():\n",
    "        distances.append(\n",
    "            {\n",
    "                \"information\": row.text,\n",
    "                \"distance\": scipy.spatial.distance.cosine(\n",
    "                    query_embedding, row.vector\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa74a6-28e8-429b-9afc-04caf1efa71c",
   "metadata": {},
   "source": [
    "Test this function out on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dfed57b-8edb-4a17-963d-e796669f3f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'information': 'Estrella is a dog', 'distance': 0.10145225769886146},\n",
       " {'information': 'Finnegan is a cat', 'distance': 0.603588576980711},\n",
       " {'information': 'Finnegan gets fed five times daily. Estrella gets fed three times daily.',\n",
       "  'distance': 0.3921942752541223},\n",
       " {'information': 'Estrella usually goes on one long walk per day, but needs to go outside every 4-6 hours',\n",
       "  'distance': 0.27359611095514047},\n",
       " {'information': 'Please play with Finnegan for 30 minutes each day. His favorite toy is the fake mouse!',\n",
       "  'distance': 0.6738025833106978}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_and_compute_distances(query=\"What type of animal is Estrella?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda4660-ce3c-4c7e-b61b-d682ea43c9ab",
   "metadata": {},
   "source": [
    "Notice that the vector that has the lowest cosine similarity (meaning most similiar) to the vector for \"What type of animal is Estrella?\" is the vector for \"Estrella is a dog\". This highlights the core assumption that underpins retrieval augmented systems: information relevant to answering a question will be close in vector space to the question itself.\n",
    "\n",
    "Now all we have to do is write a function that incorporates the text corresponding to the closest information vectors in a prompt, then send that prompt to an LLM to answer the question with the information.\n",
    "\n",
    "Start by writing a helper function to put together this prompt. `context` will be the relevant information strings (found via nearest neighbor search) and `query` will be the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6444d77-92a9-4411-b233-c70e687fcfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(query: str, context: list[str]):\n",
    "    prompt = f\"\"\"\n",
    "    Using only the provided context, answer the question.\n",
    "    \n",
    "    Context:\n",
    "    {','.join(context)}\n",
    "    \n",
    "    Question: {query}.\n",
    "    \n",
    "    If you cannot answer the question using only the provided context, respond that you do not have the context needed to answer the question.\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd9157-94f2-4c34-aa6a-f7979393c9cb",
   "metadata": {},
   "source": [
    "Now put everything together in a function that \n",
    "* Embeds the query\n",
    "* Computes the distance between query vector and all information vectors \n",
    "* Gets the k most relevant information texts by sorting by distance \n",
    "* Uses the k most relevant information texts in a prompt to an LLM along with the query \n",
    "* Returns the LLM response and the information used (citations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ff87db5-0ca3-4c6b-8c9f-cc47f0705010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VertexAI(\n",
    "    model_name=\"gemini-1.5-flash-001\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "def retrieval_chain(query: str, k: int = 2):\n",
    "    # Compute distances for query and all information vectors\n",
    "    distances = embed_and_compute_distances(query)\n",
    "\n",
    "    # Sort the information from smallest distance to greatest distance\n",
    "    sorted_distances = sorted(distances, key=lambda x: x[\"distance\"])\n",
    "\n",
    "    # Get the text corresponding to the k closest vectors\n",
    "    closest_information_texts = [x[\"information\"] for x in sorted_distances[:k]]\n",
    "\n",
    "    # Incorporate the closest k information texts in a prompt to an LLM\n",
    "    prompt = get_prompt(query, closest_information_texts)\n",
    "\n",
    "    # Send prompt through LLM\n",
    "    response = model.predict(prompt)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Information used: {closest_information_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d812fae-17c7-4840-9781-44b18256116b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Estrella is a **dog**. \n",
      "\n",
      "Information used: ['Estrella is a dog', 'Estrella usually goes on one long walk per day, but needs to go outside every 4-6 hours']\n"
     ]
    }
   ],
   "source": [
    "retrieval_chain(\"What type of animal is Estrella?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79964f00-2741-4d8f-a3ae-47c810b941e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Finnegan gets fed five times a day. \n",
      "\n",
      "Information used: ['Finnegan gets fed five times daily. Estrella gets fed three times daily.', 'Finnegan is a cat']\n"
     ]
    }
   ],
   "source": [
    "retrieval_chain(\"How many times a day do I need to feed Finnegan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b24083e3-c49a-4424-8965-0541cf4eb8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I do not have the context needed to answer the question. \n",
      "\n",
      "Information used: ['Please play with Finnegan for 30 minutes each day. His favorite toy is the fake mouse!', 'Estrella is a dog']\n"
     ]
    }
   ],
   "source": [
    "retrieval_chain(\"What stock should I invest in this month?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c3d6c-1a10-4e53-9f30-85c2160e874a",
   "metadata": {},
   "source": [
    "Notice that the prompt is constructed such that if a question is asked that cannot be answered from the information provided, the LLM will not try to answer it.\n",
    "\n",
    "It is also worth noting that we are arbitrarily setting k=2 (including the closest 2 information texts in the prompt). Different use cases require different k's and there is no perfect one-size-fits-all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f361a-ed4a-47bc-91a7-99ca8721a228",
   "metadata": {},
   "source": [
    "### Simplify and Scale with LangChain and Chroma\n",
    "Of course with only 5 examples of grounding information, we could easily include all five in a prompt. In other words, the extra retrieval step to identify *what* is needed in the prompt was unnessesary. Of course in the real world we may have thousands or millions of grounding information examples. Additionally as the number of grounding examples grows, simply computing a distance for every single vector is incredibly innefficient. In other words, production retrieval augmented generation systems require:\n",
    "* Scalable vector databases to store large amounts of information\n",
    "* Efficient ways of performing nearest neighbor searches \n",
    "\n",
    "Of course there are many options for a vectorstore, including managed and scalable offerings like [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview). For simplicity, in this lab we will use [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) as a vectorstore and [Langchain](https://github.com/langchain-ai/langchain) to orchestrate the retrieval system. Langchain will provide classes and methods that help simplify the steps we had to implement ourselves in the toy example above.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6113e3-d700-4e53-a599-3362935190ab",
   "metadata": {},
   "source": [
    "#### Document Loading\n",
    "\n",
    "Langchain provides classes to load data from different sources. Some useful data loaders are [Google Cloud Storage Directory Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/google_cloud_storage_directory), [Google Drive Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/google_drive), [Recursive URL Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/recursive_url_loader), [PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/pdf), [JSON Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/json), [Wikipedia Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/wikipedia), and [more](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n",
    "\n",
    "In this notebook we will use the Wikipedia loader to create a private knowledge base of wikipedia articles about large language models, but the overall process is similiar regardless of which document loader you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeb93ade-e33f-4408-aba3-10f6c05c01eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n\\n== History ==\\n\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved then-SOTA (state of the art) perplexity. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.\\n\\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before Transformers, it was done by seq2seq deep LSTM networks.\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 Seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June 2024, The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\\nAs of 2024, the largest and most capable models are all based on the Transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\\n\\n\\n== Dataset preprocessing ==\\n\\n\\n=== Tokenization ===\\n\\nBecause machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily', metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=10).load()\n",
    "\n",
    "# Take a look at a single document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077ce8e-fb29-43e7-b7ec-313b043ee84b",
   "metadata": {},
   "source": [
    "#### Split text into chunks \n",
    "Now that we have the documents we will split them into chunks. Each chunk will become one vector in the vector store. To do this we will define a chunk size (number of characters) and a chunk overlap (amount of overlap i.e. sliding window). The perfect chunk size can be difficult to determine. Too large of a chunk size leads to too much information per chunk (individual chunks not specific enough), however too small of a chunk size leads to not enough information per chunk. In both cases, nearest neighbors lookup with a query/question embedding may struggle to retrieve the actually relevant chunks, or fail altogether if the chunks are too large to use as context with an LLM query.\n",
    "\n",
    "In this notebook we will use a chunk size of 800 chacters and a chunk overlap of 400 characters, but feel free to experiment with other sizes! Note: you can specify a custom `length_function` with `RecursiveCharacterTextSplitter` if you want chunk size/overlap to be determined by something other than Python's len function. In addition to `RecursiveCharacterTextSplitter`, there are [other text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) you can consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff137784-b004-4a96-99db-142f9208571e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.', metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}),\n",
       " Document(page_content='The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.', metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Look at the first two chunks\n",
    "chunks[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c31316e9-273f-4ca6-a252-8de9f0e9224e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10\n",
      "Number of chunks: 80\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52830b13-f2fe-4a28-a8a3-d529da45757f",
   "metadata": {},
   "source": [
    "#### Embed Document Chunks \n",
    "Now we need to embed the document chunks and store them in a vectorstore. For this, we can use any text embedding model, however we need to be sure to use the same text embedding model when we embed our queries/questions at prediction time. To make things simple we will use the PaLM API for Embeddings. The langchain library provides a nice wrapper class around the PaLM Embeddings API, VertexAIEmbeddings().\n",
    "\n",
    "Since Vertex AI Vector Search takes awhile (~45 minutes) to create an index, we will use Chroma instead to keep things simple. Of course, in a real-world use case with a large private knowledge-base, you may not be able to fit everything in memory. Langchain has a nice wrapper class for Chroma which allows us to pass in a list of documents, and an embedding class to create the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "012aa63d-726d-4393-8293-88776458c40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "\n",
    "# set persist directory so the vector store is saved to disk\n",
    "db = Chroma.from_documents(chunks, embedding, persist_directory=\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f28527-2b38-4c51-a739-b56e499383c9",
   "metadata": {},
   "source": [
    "#### Putting it all together \n",
    "\n",
    "Now that everything is in place, we can tie it all together with a langchain chain. A langchain chain simply orchestrates the multiple steps required to use an LLM for a specific use case. In this case the process we will chain together first embeds the query/question, then performs a nearest neighbors lookup to find the relevant chunks, then uses the relevant chunks to formulate a response with an LLM. We will use the Chroma database as our vector store and PaLM as our LLM. Langchain provides a wrapper around PaLM, `VertexAI()`.\n",
    "\n",
    "For this simple Q/A use case we can use langchain's `RetrievalQA` to link together the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c1b4f33-0fe2-46e8-a898-ade9e26e7510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10},  # number of nearest neighbors to retrieve\n",
    ")\n",
    "\n",
    "# You can also set temperature, top_p, top_k\n",
    "llm = VertexAI(model_name=\"gemini-1.5-flash-001\", max_output_tokens=1024)\n",
    "\n",
    "# q/a chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d0811-c32c-4059-a7e0-8f8771dd68d2",
   "metadata": {},
   "source": [
    "Now that everything is tied together we can send queries and get answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "474abe04-d499-4066-8d41-8ff38d67b932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    response = qa({\"query\": question})\n",
    "    print(f\"Response: {response['result']}\\n\")\n",
    "\n",
    "    citations = {doc.metadata[\"source\"] for doc in response[\"source_documents\"]}\n",
    "    print(f\"Citations: {citations}\\n\")\n",
    "\n",
    "    # uncomment below to print source chunks used\n",
    "    # print(f\"Source Chunks Used: {response['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "535cd186-d1c0-4f1f-9496-f1ea35a07429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The technology that underpins large language models is **the Transformer architecture**. \n",
      "\n",
      "\n",
      "Citations: {'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/Open-source_artificial_intelligence', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://en.wikipedia.org/wiki/Language_model'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What technology underpins large language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6926718-6237-415d-8633-fd554528323e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The transformer architecture was introduced in 2017 at the NeurIPS conference. \n",
      "\n",
      "\n",
      "Citations: {'https://en.wikipedia.org/wiki/BERT_(language_model)', 'https://en.wikipedia.org/wiki/Large_language_model', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://en.wikipedia.org/wiki/T5_(language_model)'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"When was the transformer introduced?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fa985-4a03-4002-bf42-d19fc27c75a3",
   "metadata": {},
   "source": [
    "Congrats! You have now built a toy retrieval augmented generation system from scratch and applied the learnings to build a more real system using a vector database and orchestration with langchain."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "langchain_kernel",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "langchain_kernel (Local)",
   "language": "python",
   "name": "langchain_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
