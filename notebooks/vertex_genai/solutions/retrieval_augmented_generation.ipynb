{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b46970-5565-49b4-ae2a-e915e90b6c2f",
   "metadata": {},
   "source": [
    "# Intro to Retrieval Augmented Generation Systems, LangChain & ChromaDB\n",
    "\n",
    "This notebook walks through building a question/answer system that retrieves information to formulate responses, effectively grounding the LLM with specific information. A pre-trained LLM, or likely even a fine-tuned LLM will not be sufficient (in and of itself) when you want a system that understands specific, possibly private data or information that was not in its training dataset.\n",
    "\n",
    "In this lab you will:\n",
    "* Learn about the different components of a retrieval augmented system\n",
    "* Build a simple retrieval augmented generation system \n",
    "* Use LangChain and ChromaDB to simplify and scale the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e8220-6d91-4938-8188-7b3d986b2845",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6855635-3a1c-4295-acb3-999a634db4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from google import genai\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c54ce1f-3611-4d86-9a93-d3bd25e12e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-004\"\n",
    "GENERATIVE_MODEL = \"gemini-2.0-flash-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fafe53-dee2-4b22-b66a-d43f0b7d6fa4",
   "metadata": {},
   "source": [
    "### Build a simple retrieval augmented generation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168271d5-86a6-451f-ab66-3dbdd902f82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this toy example, we want to ground an LLM on information that an off-the-shelf LLM would not know. For example, instructions left for a house sitter that will be watching two pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428d7ff0-426f-4c0b-bb58-2a6177088e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estrella is a dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finnegan is a cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finnegan gets fed five times daily. Estrella g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estrella usually goes on one long walk per day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please play with Finnegan for 30 minutes each ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                  Estrella is a dog\n",
       "1                                  Finnegan is a cat\n",
       "2  Finnegan gets fed five times daily. Estrella g...\n",
       "3  Estrella usually goes on one long walk per day...\n",
       "4  Please play with Finnegan for 30 minutes each ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of things we want to ground the LLM on.\n",
    "information = [\n",
    "    \"Estrella is a dog\",\n",
    "    \"Finnegan is a cat\",\n",
    "    \"Finnegan gets fed five times daily. Estrella gets fed three times daily.\",\n",
    "    \"Estrella usually goes on one long walk per day, but needs to go outside every 4-6 hours\",\n",
    "    \"Please play with Finnegan for 30 minutes each day. His favorite toy is the fake mouse!\",\n",
    "]\n",
    "\n",
    "information_df = pd.DataFrame({\"text\": information})\n",
    "information_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfca1b4-0600-44fc-909e-730ab4afcd56",
   "metadata": {},
   "source": [
    "At the core of most retrieval generation systems is a vector database. A vector database stores embedded representations of information. \n",
    "\n",
    "Let's add a column to our information dataframe that is an embedded representation of the text. We will use the [Google GenAI SDK](https://github.com/googleapis/python-genai?tab=readme-ov-file#embed-content) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819c14be-aabe-4906-8e5a-289a1c66f8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d3e8c6-f8d5-49b2-8cd4-6d9a54ecd6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estrella is a dog</td>\n",
       "      <td>[0.01971660926938057, -0.009960231371223927, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finnegan is a cat</td>\n",
       "      <td>[-0.053840961307287216, -0.014684000052511692,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finnegan gets fed five times daily. Estrella g...</td>\n",
       "      <td>[0.038750406354665756, 0.005375586915761232, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estrella usually goes on one long walk per day...</td>\n",
       "      <td>[0.06929957121610641, -0.02032635547220707, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please play with Finnegan for 30 minutes each ...</td>\n",
       "      <td>[-0.0562220960855484, 0.0075227334164083, 0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                  Estrella is a dog   \n",
       "1                                  Finnegan is a cat   \n",
       "2  Finnegan gets fed five times daily. Estrella g...   \n",
       "3  Estrella usually goes on one long walk per day...   \n",
       "4  Please play with Finnegan for 30 minutes each ...   \n",
       "\n",
       "                                              vector  \n",
       "0  [0.01971660926938057, -0.009960231371223927, -...  \n",
       "1  [-0.053840961307287216, -0.014684000052511692,...  \n",
       "2  [0.038750406354665756, 0.005375586915761232, 0...  \n",
       "3  [0.06929957121610641, -0.02032635547220707, -0...  \n",
       "4  [-0.0562220960855484, 0.0075227334164083, 0.06...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_df[\"vector\"] = [\n",
    "    x.values\n",
    "    for x in client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL, contents=information\n",
    "    ).embeddings\n",
    "]\n",
    "information_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bab4e4-d701-4cfd-b0fe-71307bd5a1b5",
   "metadata": {},
   "source": [
    "Retrieval systems need a way of finding the most relevant information to answer a given query. This is done with a nearest neighbor (semantic similarity) search. Let's define a function to take in a query (text) input and return a distance metric for each text in our information. We will need to: \n",
    "* Embed the query with the same embedding model used for the information \n",
    "* Computes a distance metric between the query vector and each information vector. We will use cosine similarity, one of the many similarity measures that can be used.\n",
    "* Returns a list of distance metrics between the query vector and each information vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71088247-87d3-4aeb-a5a9-c8dc3f74f4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embed_and_compute_distances(query: str):\n",
    "    # Get vector for query string\n",
    "    query_embedding = (\n",
    "        client.models.embed_content(model=EMBEDDING_MODEL, contents=query)\n",
    "        .embeddings[0]\n",
    "        .values\n",
    "    )  # Query embedding\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    # Compute distances between query vector and all information vectors\n",
    "    for _, row in information_df.iterrows():\n",
    "        distances.append(\n",
    "            {\n",
    "                \"information\": row.text,\n",
    "                \"distance\": scipy.spatial.distance.cosine(\n",
    "                    query_embedding, row.vector\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa74a6-28e8-429b-9afc-04caf1efa71c",
   "metadata": {},
   "source": [
    "Test this function out on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dfed57b-8edb-4a17-963d-e796669f3f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'information': 'Estrella is a dog', 'distance': 0.10145225769886146},\n",
       " {'information': 'Finnegan is a cat', 'distance': 0.603588576980711},\n",
       " {'information': 'Finnegan gets fed five times daily. Estrella gets fed three times daily.',\n",
       "  'distance': 0.3921942752541223},\n",
       " {'information': 'Estrella usually goes on one long walk per day, but needs to go outside every 4-6 hours',\n",
       "  'distance': 0.27359611095514047},\n",
       " {'information': 'Please play with Finnegan for 30 minutes each day. His favorite toy is the fake mouse!',\n",
       "  'distance': 0.6738025833106978}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_and_compute_distances(query=\"What type of animal is Estrella?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda4660-ce3c-4c7e-b61b-d682ea43c9ab",
   "metadata": {},
   "source": [
    "Notice that the vector that has the lowest cosine similarity (meaning most similiar) to the vector for \"What type of animal is Estrella?\" is the vector for \"Estrella is a dog\". This highlights the core assumption that underpins retrieval augmented systems: information relevant to answering a question will be close in vector space to the question itself.\n",
    "\n",
    "Now all we have to do is write a function that incorporates the text corresponding to the closest information vectors in a prompt, then send that prompt to an LLM to answer the question with the information.\n",
    "\n",
    "Start by writing a helper function to put together this prompt. `context` will be the relevant information strings (found via nearest neighbor search) and `query` will be the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6444d77-92a9-4411-b233-c70e687fcfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(query: str, context: list[str]):\n",
    "    prompt = f\"\"\"\n",
    "    Using only the provided context, answer the question.\n",
    "    \n",
    "    Context:\n",
    "    {','.join(context)}\n",
    "    \n",
    "    Question: {query}.\n",
    "    \n",
    "    If you cannot answer the question using only the provided context, respond that you do not have the context needed to answer the question.\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd9157-94f2-4c34-aa6a-f7979393c9cb",
   "metadata": {},
   "source": [
    "Now put everything together in a function that \n",
    "* Embeds the query\n",
    "* Computes the distance between query vector and all information vectors \n",
    "* Gets the k most relevant information texts by sorting by distance \n",
    "* Uses the k most relevant information texts in a prompt to an LLM along with the query \n",
    "* Returns the LLM response and the information used (citations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff87db5-0ca3-4c6b-8c9f-cc47f0705010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieval_chain(query: str, k: int = 2):\n",
    "    # Compute distances for query and all information vectors\n",
    "    distances = embed_and_compute_distances(query)\n",
    "\n",
    "    # Sort the information from smallest distance to greatest distance\n",
    "    sorted_distances = sorted(distances, key=lambda x: x[\"distance\"])\n",
    "\n",
    "    # Get the text corresponding to the k closest vectors\n",
    "    closest_information_texts = [x[\"information\"] for x in sorted_distances[:k]]\n",
    "\n",
    "    # Incorporate the closest k information texts in a prompt to an LLM\n",
    "    prompt = get_prompt(query, closest_information_texts)\n",
    "\n",
    "    # Send prompt through LLM\n",
    "    response = client.models.generate_content(\n",
    "        model=GENERATIVE_MODEL, contents=prompt\n",
    "    )\n",
    "    # return response\n",
    "\n",
    "    print(f\"Response: {response.text}\")\n",
    "    print(f\"Information used: {closest_information_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d812fae-17c7-4840-9781-44b18256116b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Estrella is a dog.\n",
      "\n",
      "Information used: ['Estrella is a dog', 'Estrella usually goes on one long walk per day, but needs to go outside every 4-6 hours']\n"
     ]
    }
   ],
   "source": [
    "retrieval_chain(\"What type of animal is Estrella?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79964f00-2741-4d8f-a3ae-47c810b941e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Finnegan gets fed five times daily.\n",
      "\n",
      "Information used: ['Finnegan gets fed five times daily. Estrella gets fed three times daily.', 'Finnegan is a cat']\n"
     ]
    }
   ],
   "source": [
    "retrieval_chain(\"How many times a day do I need to feed Finnegan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24083e3-c49a-4424-8965-0541cf4eb8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I do not have the context needed to answer the question.\n",
      "\n",
      "Information used: ['Please play with Finnegan for 30 minutes each day. His favorite toy is the fake mouse!', 'Estrella is a dog']\n"
     ]
    }
   ],
   "source": [
    "retrieval_chain(\"What stock should I invest in this month?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c3d6c-1a10-4e53-9f30-85c2160e874a",
   "metadata": {},
   "source": [
    "Notice that the prompt is constructed such that if a question is asked that cannot be answered from the information provided, the LLM will not try to answer it.\n",
    "\n",
    "It is also worth noting that we are arbitrarily setting k=2 (including the closest 2 information texts in the prompt). Different use cases require different k's and there is no perfect one-size-fits-all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f361a-ed4a-47bc-91a7-99ca8721a228",
   "metadata": {},
   "source": [
    "### Simplify and Scale with LangChain and Chroma\n",
    "Of course with only 5 examples of grounding information, we could easily include all five in a prompt. In other words, the extra retrieval step to identify *what* is needed in the prompt was unnessesary. Of course in the real world we may have thousands or millions of grounding information examples. Additionally as the number of grounding examples grows, simply computing a distance for every single vector is incredibly innefficient. In other words, production retrieval augmented generation systems require:\n",
    "* Scalable vector databases to store large amounts of information\n",
    "* Efficient ways of performing nearest neighbor searches \n",
    "\n",
    "Of course there are many options for a vectorstore, including managed and scalable offerings like [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview). For simplicity, in this lab we will use [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) as a vectorstore and [Langchain](https://github.com/langchain-ai/langchain) to orchestrate the retrieval system. Langchain will provide classes and methods that help simplify the steps we had to implement ourselves in the toy example above.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6113e3-d700-4e53-a599-3362935190ab",
   "metadata": {},
   "source": [
    "#### Document Loading\n",
    "\n",
    "Langchain provides classes to load data from different sources. Some useful data loaders are [Google Cloud Storage Directory Loader](https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/), [Google Drive Loader](https://python.langchain.com/docs/integrations/document_loaders/google_drive), [Recursive URL Loader](https://python.langchain.com/docs/integrations/document_loaders/recursive_url/), [PDF Loader](https://python.langchain.com/docs/integrations/document_loaders/#pdfs), [JSON Loader](https://python.langchain.com/docs/integrations/document_loaders/json/), [Wikipedia Loader](https://python.langchain.com/docs/integrations/document_loaders/wikipedia/), and [more](https://python.langchain.com/docs/integrations/document_loaders/).\n",
    "\n",
    "In this notebook we will use the Wikipedia loader to create a private knowledge base of wikipedia articles about large language models, but the overall process is similiar regardless of which document loader you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb93ade-e33f-4408-aba3-10f6c05c01eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model (specifically a type of large X model (LxM)) designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='A large language model (LLM) is a type of machine learning model (specifically a type of large X model (LxM)) designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n\\n== History ==\\n\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 300 million words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.\\n\\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. Because it preceded the existence of transformers, it was done by seq2seq deep LSTM networks.\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.\\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multi')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=10).load()\n",
    "\n",
    "# Take a look at a single document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077ce8e-fb29-43e7-b7ec-313b043ee84b",
   "metadata": {},
   "source": [
    "#### Split text into chunks \n",
    "Now that we have the documents we will split them into chunks. Each chunk will become one vector in the vector store. To do this we will define a chunk size (number of characters) and a chunk overlap (amount of overlap i.e. sliding window). The perfect chunk size can be difficult to determine. Too large of a chunk size leads to too much information per chunk (individual chunks not specific enough), however too small of a chunk size leads to not enough information per chunk. In both cases, nearest neighbors lookup with a query/question embedding may struggle to retrieve the actually relevant chunks, or fail altogether if the chunks are too large to use as context with an LLM query.\n",
    "\n",
    "In this notebook we will use a chunk size of 800 chacters and a chunk overlap of 400 characters, but feel free to experiment with other sizes! Note: you can specify a custom `length_function` with `RecursiveCharacterTextSplitter` if you want chunk size/overlap to be determined by something other than Python's len function. In addition to `RecursiveCharacterTextSplitter`, there are [other text splitters](https://python.langchain.com/docs/how_to/#text-splitters) you can consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff137784-b004-4a96-99db-142f9208571e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model (specifically a type of large X model (LxM)) designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='A large language model (LLM) is a type of machine learning model (specifically a type of large X model (LxM)) designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n\\n== History =='),\n",
       " Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of machine learning model (specifically a type of large X model (LxM)) designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='== History ==\\n\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 300 million words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Look at the first two chunks\n",
    "chunks[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c31316e9-273f-4ca6-a252-8de9f0e9224e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10\n",
      "Number of chunks: 73\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52830b13-f2fe-4a28-a8a3-d529da45757f",
   "metadata": {},
   "source": [
    "#### Embed Document Chunks \n",
    "Now we need to embed the document chunks and store them in a vectorstore. For this, we can use any text embedding model, however we need to be sure to use the same text embedding model when we embed our queries/questions at prediction time. To make things simple we will use the Gemini API for Embeddings. The langchain library provides a nice wrapper class around the Gemini Embeddings API, VertexAIEmbeddings().\n",
    "\n",
    "Since Vertex AI Vector Search takes awhile (~45 minutes) to create an index, we will use Chroma instead to keep things simple. Of course, in a real-world use case with a large private knowledge-base, you may not be able to fit everything in memory. Langchain has a nice wrapper class for Chroma which allows us to pass in a list of documents, and an embedding class to create the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "012aa63d-726d-4393-8293-88776458c40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = VertexAIEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# set persist directory so the vector store is saved to disk\n",
    "db = Chroma.from_documents(chunks, embedding, persist_directory=\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f28527-2b38-4c51-a739-b56e499383c9",
   "metadata": {},
   "source": [
    "#### Putting it all together \n",
    "\n",
    "Now that everything is in place, we can tie it all together with a langchain chain. A langchain chain simply orchestrates the multiple steps required to use an LLM for a specific use case. In this case the process we will chain together first embeds the query/question, then performs a nearest neighbors lookup to find the relevant chunks, then uses the relevant chunks to formulate a response with an LLM. We will use the Chroma database as our vector store and Gemini as our LLM. Langchain provides a wrapper around Gemini, `VertexAI()`.\n",
    "\n",
    "For this simple Q/A use case we can use langchain's `RetrievalQA` to link together the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c1b4f33-0fe2-46e8-a898-ade9e26e7510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10},  # number of nearest neighbors to retrieve\n",
    ")\n",
    "\n",
    "# You can also set temperature, top_p, top_k\n",
    "llm = VertexAI(model_name=GENERATIVE_MODEL, max_output_tokens=1024)\n",
    "\n",
    "# q/a chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d0811-c32c-4059-a7e0-8f8771dd68d2",
   "metadata": {},
   "source": [
    "Now that everything is tied together we can send queries and get answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "474abe04-d499-4066-8d41-8ff38d67b932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    response = qa.invoke({\"query\": question})\n",
    "    print(f\"Response: {response['result']}\\n\")\n",
    "\n",
    "    citations = {doc.metadata[\"source\"] for doc in response[\"source_documents\"]}\n",
    "    print(f\"Citations: {citations}\\n\")\n",
    "\n",
    "    # uncomment below to print source chunks used\n",
    "    # print(f\"Source Chunks Used: {response['source_documents']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "535cd186-d1c0-4f1f-9496-f1ea35a07429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Large language models are predominantly based on transformers.\n",
      "\n",
      "\n",
      "Citations: {'https://en.wikipedia.org/wiki/Large_language_model', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', 'https://en.wikipedia.org/wiki/Reasoning_language_model', 'https://en.wikipedia.org/wiki/Language_model', 'https://en.wikipedia.org/wiki/Foundation_model', 'https://en.wikipedia.org/wiki/Llama_(language_model)', 'https://en.wikipedia.org/wiki/List_of_large_language_models'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What technology underpins large language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6926718-6237-415d-8633-fd554528323e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The transformer architecture was introduced in 2017.\n",
      "\n",
      "\n",
      "Citations: {'https://en.wikipedia.org/wiki/Large_language_model', 'https://en.wikipedia.org/wiki/Language_model', 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"When was the transformer introduced?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67fa985-4a03-4002-bf42-d19fc27c75a3",
   "metadata": {},
   "source": [
    "Congrats! You have now built a toy retrieval augmented generation system from scratch and applied the learnings to build a more real system using a vector database and orchestration with langchain."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
