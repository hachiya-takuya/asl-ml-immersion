{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78db9fda-db2e-4a97-90bc-b7509d077892",
   "metadata": {},
   "source": [
    "# Building Agent with LangGraph\n",
    "\n",
    "This notebook explores the capabilities of LangGraph for building intelligent agents. We will start with a very simple graph to illustrate the fundamental concepts of states, nodes, and edges. Then, we will progress to building a more complex agent that can perform tasks, use tools, and even incorporate human feedback. Finally, we will construct an advanced essay writing agent that leverages multiple components for planning, research, generation, and reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85eada-7657-4650-bebc-5eab4e4b19a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769268f9-b3fe-4516-9aa8-83f3d166950b",
   "metadata": {},
   "source": [
    "## The Simplest Graph\n",
    "\n",
    "Let's build a simple graph with 3 nodes and one conditional edge. \n",
    "\n",
    "This initial example demonstrates the core components of a LangGraph application: defining a state, creating nodes as Python functions that modify this state, and connecting these nodes with edges to define the flow of execution. A conditional edge is introduced to show how the graph can make decisions based on the current state.\n",
    "\n",
    "![Screenshot 2024-08-20 at 3.11.22 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dba5f465f6e9a2482ad935_simple-graph1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf8d87-3433-4477-a7ff-b87a3613bd72",
   "metadata": {},
   "source": [
    "### State\n",
    "\n",
    "First, define the [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) of the graph. \n",
    "\n",
    "The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
    "\n",
    "It can be defined using any Python type, but is typically a TypedDict or Pydantic BaseModel.\n",
    "Let's use the `BaseModel` class here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8afdb1-c671-4bd0-bcd4-c6c9d3fea1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    graph_state: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20919de-3bef-471b-aede-53e0f193f8da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nodes\n",
    "\n",
    "[Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) are just python functions.\n",
    "\n",
    "The first positional argument is the state, as defined above.\n",
    "\n",
    "Because the state is a `TypedDict` with schema as defined above, each node can access the key, `graph_state`, with `state['graph_state']`.\n",
    "\n",
    "Each node returns a new value of the state key `graph_state`.\n",
    "  \n",
    "By default, the new value returned by each node [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior state value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1748d9-edf6-420f-9fab-f3d383bbc6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def node_1(state: State):\n",
    "    print(\"---Node 1---\")\n",
    "    return {\"graph_state\": state.graph_state + \" I am\"}\n",
    "\n",
    "\n",
    "def node_2(state: State):\n",
    "    print(\"---Node 2---\")\n",
    "    return {\"graph_state\": state.graph_state + \" happy!\"}\n",
    "\n",
    "\n",
    "def node_3(state: State):\n",
    "    print(\"---Node 3---\")\n",
    "    return {\"graph_state\": state.graph_state + \" sad!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e633fbc-ecbf-4cb2-8c37-7362d5ad57a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", node_2)\n",
    "builder.add_node(\"node_3\", node_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf276b-d836-46ed-b7b6-05a0a215d6d6",
   "metadata": {},
   "source": [
    "### Edges\n",
    "\n",
    "[Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges) connect the nodes.\n",
    "\n",
    "Normal Edges are used if you want to *always* go from, for example, `node_1` to `node_2`.\n",
    "\n",
    "[Conditional Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#conditional-edges) are used if you want to *optionally* route between nodes.\n",
    " \n",
    "Conditional edges are implemented as functions that return the next node to visit based upon some logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336cd71-bd50-4872-8e35-c6ec0c06157f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def decide_mood(state) -> Literal[\"node_2\", \"node_3\"]:\n",
    "\n",
    "    # Often, we will use state to decide on the next node to visit\n",
    "    user_input = state.graph_state\n",
    "\n",
    "    # Here, let's just do a 50 / 50 split between nodes 2, 3\n",
    "    if random.random() < 0.5:\n",
    "\n",
    "        # 50% of the time, we return Node 2\n",
    "        return \"node_2\"\n",
    "\n",
    "    # 50% of the time, we return Node 3\n",
    "    return \"node_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b0e90-b7a8-4440-8cf8-702ca3e3a2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logic\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_conditional_edges(\"node_1\", decide_mood)\n",
    "builder.add_edge(\"node_2\", END)\n",
    "builder.add_edge(\"node_3\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294d3cf-ce9f-4360-a2c6-37fb5b9f2bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a65210-2356-45a1-8a9a-70228cc1f246",
   "metadata": {},
   "source": [
    "### Graph Invocation\n",
    "\n",
    "The compiled graph implements the [runnable](https://python.langchain.com/docs/concepts/runnables/) protocol.\n",
    "\n",
    "This provides a standard way to execute LangChain components.\n",
    " \n",
    "`invoke` is one of the standard methods in this interface.\n",
    "\n",
    "The input is a dictionary `{\"graph_state\": \"Hi, this is lance.\"}`, which sets the initial value for our graph state dict.\n",
    "\n",
    "When `invoke` is called, the graph starts execution from the `START` node.\n",
    "\n",
    "It progresses through the defined nodes (`node_1`, `node_2`, `node_3`) in order.\n",
    "\n",
    "The conditional edge will traverse from node `1` to node `2` or `3` using a 50/50 decision rule.\n",
    "\n",
    "Each node function receives the current state and returns a new value, which overrides the graph state.\n",
    "\n",
    "The execution continues until it reaches the `END` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571984d-aaec-45cd-bc61-6408b2939020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.invoke({\"graph_state\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079aa58d-27d5-4443-88b2-f0ac2be9c133",
   "metadata": {},
   "source": [
    "## Simple Agent with Function Calling tool\n",
    "\n",
    "Now, we can extend this into a generic agent architecture.\n",
    "\n",
    "This section demonstrates how to build a basic agent capable of using tools through function calling. We will define a set of tools (e.g., for arithmetic operations) and bind them to a language model. The LangGraph setup will then manage the flow: the agent (LLM) decides if a tool is needed, calls the appropriate tool, and then potentially loops back to the agent for further processing or produces a final response. This illustrates a fundamental ReAct (Reasoning and Acting) style loop.\n",
    "\n",
    "In the function calling notebook, we explored invoked the model and, if it chose to call a tool, we returned a `ToolMessage` to the user.\n",
    " \n",
    "![Screenshot 2024-08-21 at 12.45.43 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0b4a2c1e5e02f3e78b_agent2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ec193-ee71-43b3-bd2a-19b0e8f828ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI, VertexAI, VertexAIEmbeddings\n",
    "\n",
    "llm = ChatVertexAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be573c-d487-4637-ab71-7a7342dfbdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# This will be a tool\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cda783-8353-4dd8-af4a-0c71efa6b799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    ")\n",
    "\n",
    "# Node\n",
    "\n",
    "\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba1d2c-940a-4328-aa62-28a862519bd3",
   "metadata": {},
   "source": [
    "For state, we use a prebuilt `MessagesState` which has 'messages' property, and define a `Tools` node with our list of tools.\n",
    "\n",
    "The `assistant` node is just our model with bound tools.\n",
    "\n",
    "We create a graph with `assistant` and `tools` nodes.\n",
    "\n",
    "We add `tools_condition` edge, which routes to `End` or to `tools` based on  whether the `assistant` calls a tool.\n",
    "\n",
    "Now, we add one new step:\n",
    "\n",
    "We connect the `Tools` node *back* to the `Assistant`, forming a loop.\n",
    "\n",
    "* After the `assistant` node executes, `tools_condition` checks if the model's output is a tool call.\n",
    "* If it is a tool call, the flow is directed to the `tools` node.\n",
    "* The `tools` node connects back to `assistant`.\n",
    "* This loop continues as long as the model decides to call tools.\n",
    "* If the model response is not a tool call, the flow is directed to END, terminating the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c4640-b031-4620-aab0-96fbd4c23b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f68be-b4d9-4b6b-9219-91edcc9cd863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac1b4a-2dc8-46f5-b44b-66e51e399a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3.\")}\n",
    "\n",
    "# thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for event in react_graph.stream(initial_input, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc5aca-1f72-44b1-9582-62ca7a016ea9",
   "metadata": {},
   "source": [
    "## Breakpoint and Human-in-the-loop\n",
    "\n",
    "LangGraph allows for the inclusion of breakpoints in the agent's execution flow. This is crucial for debugging, monitoring, and enabling human-in-the-loop interventions. By interrupting the graph at a specific node (e.g., before a tool is called), we can inspect the state and decide whether to proceed, modify the state, or halt execution. This section demonstrates how to set up such a breakpoint and prompt for user approval before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ece1e-ae92-42d9-89f0-4c4bbaabe255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "react_graph = builder.compile(checkpointer=memory, interrupt_before=[\"tools\"])\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for event in react_graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49b364-ffc6-4574-be37-4e3f50645670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in react_graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Get user feedback\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "\n",
    "# Check approval\n",
    "if user_approval.lower() == \"yes\":\n",
    "\n",
    "    # If approved, continue the graph execution\n",
    "    for event in react_graph.stream(None, thread, stream_mode=\"values\"):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e6f61-045a-41e1-9ba8-3b5eb5679865",
   "metadata": {},
   "source": [
    "---\n",
    "## Build a complex Essays writing agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e66398-cac4-4c23-a2a5-41b39ae9b5e8",
   "metadata": {},
   "source": [
    "This section demonstrates how to build a LangGraph-powered AI agent to generate, revise, and critique essays using Gemini. The LangGraph code was adapted from the awesome DeepLearning.AI course on AI Agents in LangGraph.\n",
    "\n",
    "By defining a structured state flow with nodes such as \"Planner,\" \"Research Plan,\" \"Generate,\" \"Reflect,\" and \"Research Critique,\" the system iteratively creates an essay on a given topic, incorporates feedback, and provides research-backed insights.\n",
    "\n",
    "The agent will perform the following steps:\n",
    "1.  **Plan:** Create an initial outline for the essay based on the given topic.\n",
    "2.  **Research Plan:** Generate search queries based on the topic to gather initial information.\n",
    "3.  **Generate:** Write a draft of the essay using the plan and researched content.\n",
    "4.  **Reflect:** Critique the generated draft, identifying areas for improvement.\n",
    "5.  **Research Critique:** Generate new search queries based on the critique to find information for revisions.\n",
    "6.  **Loop:** The generation, reflection, and research critique steps can loop a specified number of times to iteratively improve the essay.\n",
    "\n",
    "<img width=\"1084\" alt=\"image\" src=\"https://camo.githubusercontent.com/f39ea212d055cb3982e931286d62cd08ec812881f4257ff1a4fd0755fc9cb628/68747470733a2f2f6769746875622e636f6d2f476f6f676c65436c6f7564506c6174666f726d2f67656e657261746976652d61692f626c6f622f6d61696e2f776f726b73686f70732f61692d6167656e74732f332d6c616e6767726170682d65737361792e706e673f7261773d31\">\n",
    "\n",
    "The workflow enables automated essay generation with revision controls, making it ideal for structured writing tasks or educational use cases. Additionally, the notebook uses external search tools to gather and integrate real-time information into the essay content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea60c6-49db-45ad-931a-6c0700b75959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eefa0-c4f4-49df-abe0-0c48b82c8a03",
   "metadata": {},
   "source": [
    "### Configure Tavily\n",
    "Get an API key for [Tavily](https://tavily.com/), a web search API for Generative AI models.\n",
    "Tavily will be used by our agent to perform research and gather up-to-date information from the web to incorporate into the essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90197078-b2b1-4d36-9dbe-8cb3050d47e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"YOUR API KEY\"\n",
    "tavily = TavilyClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286a187-a74a-4392-9c4f-5dc505d985c4",
   "metadata": {},
   "source": [
    "### Initialize agent memory, agent state, and schema for search queries\n",
    "\n",
    "Before defining the agent's nodes and graph structure, we need to set up the foundational components:\n",
    "* **Agent State (`AgentState`):** This pydantic `BaseModel` will hold all the information that flows through our graph. It includes the task, essay plan, draft, critique, researched content, revision number, and maximum allowed revisions. Each node in the graph will read from and write to this state.\n",
    "* **Search Queries Schema (`Queries`):** This Pydantic `BaseModel` defines the expected structure for the search queries that our research nodes will generate. Using a schema helps ensure that the LLM produces queries in a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4ee2a-9692-4631-ac48-22f32e4edbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgentState(BaseModel):\n",
    "    task: str\n",
    "    plan: Optional[str] = \"\"\n",
    "    draft: Optional[str] = \"\"\n",
    "    critique: Optional[str] = \"\"\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    \"\"\"Variants of query to search for\"\"\"\n",
    "\n",
    "    queries: list[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298d510-e4e0-4635-b8c3-9522cdb70bfb",
   "metadata": {},
   "source": [
    "### Define prompt templates for each stage\n",
    "\n",
    "Clear and effective prompts are essential for guiding the LLM at each stage of the essay writing process. We define specific prompt templates for:\n",
    "* **PLAN_PROMPT:** Instructs the LLM to act as an expert writer and create a high-level outline for the essay.\n",
    "* **RESEARCH_PLAN_PROMPT:** Guides the LLM to act as a researcher and generate search queries to gather initial information for the essay.\n",
    "* **WRITER_PROMPT:** Instructs the LLM to act as an essay assistant, write a 3-page essay based on the plan and research, and revise based on critique. It also specifies Markdown formatting.\n",
    "* **REFLECTION_PROMPT:** Tells the LLM to act as a teacher grading the essay, providing critique and detailed recommendations.\n",
    "* **RESEARCH_CRITIQUE_PROMPT:** Guides the LLM to generate search queries based on the essay draft and the critique to find information for revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc2694-91b2-452e-ad80-bd887d41bc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay.\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any\n",
    "relevant notes or instructions for the sections.\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can\n",
    "be used when writing the following essay. Generate a list of search queries that will gather\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 3-pages essays.\n",
    "Generate the best essay possible for the user's request and the initial outline.\n",
    "If the user provides critique, respond with a revised version of your previous attempts.\n",
    "Use Markdown formatting to specify a title and section headers for each paragraph.\n",
    "Utilize all of the information below as needed:\n",
    "---\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission.\n",
    "Generate critique and recommendations for the user's submission.\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can\n",
    "be used when making any requested revisions for the draft (draft and requests are outlined below).\n",
    "Generate a list of search queries that will gather any relevant information.\n",
    "Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb47feb-227e-4618-8b4c-6eaf973042ff",
   "metadata": {},
   "source": [
    "### Define node functions for each stage\n",
    "\n",
    "Each node in our LangGraph represents a specific step in the essay writing workflow. These Python functions take the current `AgentState` as input and return a dictionary with the updated state values for their respective operations:\n",
    "\n",
    "* `plan_node`: Invokes the LLM with the `PLAN_PROMPT` to generate an essay outline.\n",
    "* `research_plan_node`: Uses the LLM (with structured output for `Queries`) and the `RESEARCH_PLAN_PROMPT` to generate search queries. It then executes these queries using the Tavily client and appends the results to the state's content list.\n",
    "* `generation_node`: Invokes the LLM with the `WRITER_PROMPT`, the essay task, the plan, and the researched content to generate an essay draft. It also increments the revision number.\n",
    "* `reflection_node`: Uses the LLM with the `REFLECTION_PROMPT` to critique the current draft.\n",
    "* `research_critique_node`: Similar to `research_plan_node`, but uses the `RESEARCH_CRITIQUE_PROMPT` along with the current draft and critique to generate targeted search queries for revision. Results are added to the content list.\n",
    "* `should_continue`: This is a conditional edge function. It checks if the current `revision_number` has exceeded `max_revisions`. If so, it returns `END` to terminate the graph; otherwise, it returns \"reflect\" to continue the revision cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8a344-2679-4628-a811-6bd8a13cdf03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state.task),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"plan\": response.content}\n",
    "\n",
    "\n",
    "# Conducts research based on the generated plan and web search results\n",
    "def research_plan_node(state: AgentState):\n",
    "    queries = llm.with_structured_output(Queries).invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "            HumanMessage(content=state.task),\n",
    "        ]\n",
    "    )\n",
    "    content = state.content or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=3)\n",
    "        for r in response[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "    return {\"content\": content}\n",
    "\n",
    "\n",
    "# Generates a draft based on the content and plan\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state.content or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state.task}\\n\\nHere is my plan:\\n\\n{state.plan}\"\n",
    "    )\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_number\": state.revision_number + 1,\n",
    "    }\n",
    "\n",
    "\n",
    "# Provides feedback or critique on the draft\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state.draft),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "\n",
    "# Conducts research based on the critique\n",
    "def research_critique_node(state: AgentState):\n",
    "    queries = llm.with_structured_output(Queries).invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "            HumanMessage(content=state.draft),\n",
    "            HumanMessage(content=state.critique),\n",
    "        ]\n",
    "    )\n",
    "    content = state.content or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=3)\n",
    "        for r in response[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "    return {\"content\": content}\n",
    "\n",
    "\n",
    "# Determines whether the critique and research cycle should\n",
    "# continue based on the number of revisions\n",
    "def should_continue(state):\n",
    "    if state.revision_number > state.max_revisions:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790f5b1-6e59-48ca-bebe-286b2cad53da",
   "metadata": {},
   "source": [
    "### Define and compile the graph\n",
    "\n",
    "With the state, prompts, and node functions defined, we can now construct the essay writing agent's graph:\n",
    "1.  **Initialize `StateGraph`:** We create an instance of `StateGraph` with our `AgentState`.\n",
    "2.  **Add Nodes:** Each function defined in the previous step (`plan_node`, `generation_node`, etc.) is added as a node to the graph, with a unique string identifier.\n",
    "3.  **Set Entry Point:** `builder.set_entry_point(\"planner\")` designates the `planner` node as the starting point of the graph.\n",
    "4.  **Add Edges:**\n",
    "    * Sequential Edges: We define the primary flow: `planner` -> `research_plan` -> `generate`. And the revision loop: `reflect` -> `research_critique` -> `generate`.\n",
    "    * Conditional Edges: `builder.add_conditional_edges` is used after the `generate` node. It calls the `should_continue` function. Based on its return value, the graph either transitions to the `reflect` node (to continue revising) or to `END` (to finish).\n",
    "5.  **Initialize Memory:** A `MemorySaver` is initialized to enable checkpointing, which saves the state of the graph.\n",
    "6.  **Compile Graph:** `builder.compile(checkpointer=memory)` finalizes the graph structure and incorporates the memory saver, making the graph ready for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5d924-0d54-4ec1-9956-9a711e7ff497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the state graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes for each step in the workflow\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "# Set the entry point of the workflow\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "# Add conditional edges for task continuation or end\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", should_continue, {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "# Define task sequence edges\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "# Initialize agent memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with memory state management\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda299a-0631-4c2c-a4df-e0ffa85ef5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdbb4e-0ae3-4648-8c23-451e9d0597cc",
   "metadata": {},
   "source": [
    "### Run the agent - write on!\n",
    "\n",
    "Now it's time to run our essay writing agent. We'll provide an initial essay topic and configuration, then stream the agent's execution to observe each step and its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2793b-6583-4b56-bf14-b49e2d20116e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the topic of the essay\n",
    "ESSAY_TOPIC = (\n",
    "    \"What were the impacts of Hurricane Helene and Hurricane Milton in 2024?\"\n",
    ")\n",
    "\n",
    "# Define a thread configuration with a unique thread ID\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Stream through the graph execution with an initial task and state\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"task\": ESSAY_TOPIC,  # Initial task\n",
    "        \"max_revisions\": 2,  # Maximum number of revisions allowed\n",
    "        \"revision_number\": 1,  # Current revision number\n",
    "        \"content\": [],  # Initial empty content list\n",
    "    },\n",
    "    thread,\n",
    "):\n",
    "    step = next(iter(s))\n",
    "    display(Markdown(f\"# {step}\"))\n",
    "    for key, content in s[step].items():\n",
    "        if key == \"revision_number\":\n",
    "            display(Markdown(f\"**Revision Number**: {content}\"))\n",
    "        elif isinstance(content, list):\n",
    "            for c in content:\n",
    "                display(Markdown(c))\n",
    "        else:\n",
    "            display(Markdown(content))\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b33d2-6198-4748-8a3b-76d14fbda8ba",
   "metadata": {},
   "source": [
    "Copyright 2025 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd01b0-21d6-4847-9cca-45a0dbacadaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
