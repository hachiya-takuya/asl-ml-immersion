{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251219ff-472d-4cd5-99ed-192f811a7bdb",
   "metadata": {},
   "source": [
    "# Prompt Management and Context Caching with Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70e5bb-c551-47bc-b386-60b2426b9beb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1.  Learn how to use Vertex AI SDK to manage the lifecycle of prompt templates.\n",
    "2.  Learn how to define, save, load and manage the prompts directly within Python code.\n",
    "3.  Understand the concept of context caching and its benefits when working with large language models.\n",
    "4.  Learn how to use the Vertex AI SDK to create and utilize cached content with Gemini models.\n",
    "5.  Compare the performance of using cached content versus generating content from scratch, highlighting the speed and cost advantages.\n",
    "\n",
    "## Overview\n",
    "This notebook explores two key aspects of working with generative AI on Google Cloud. The first part focuses on Vertex AI's prompt management capabilities, explaining how to programmatically create, version, and organize prompt templates using the Vertex AI SDK. The second part introduces the Gemini API's context caching feature, designed to optimize requests with large, consistent initial contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919d988-e8f3-4935-98b4-f4b576de81c5",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57172993-318c-4a69-91fd-56e4accd6fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import vertexai\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from vertexai.preview import prompts\n",
    "from vertexai.preview.prompts import Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81084c80-7378-4c5c-994b-b0632911ce99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "MODEL = \"gemini-2.0-flash-001\"\n",
    "\n",
    "client = genai.Client(vertexai=True, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06e100-5083-4ebc-9e49-0ce0a8a41a2c",
   "metadata": {},
   "source": [
    "## Prompt Management\n",
    "\n",
    "Vertex AI offers prompt management through its user interface, Vertex AI Studio, and programmatically via the Vertex AI SDK. This section focuses on the latter method, demonstrating how to leverage the `vertexai.preview.prompts` module to define, save, and manage prompts specifically for Gemini text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebb728-254c-4e4c-8b1f-b8a68827e1ed",
   "metadata": {},
   "source": [
    "### The Prompt class\n",
    "\n",
    "To effectively manage prompts, we will use the [Prompt class](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.prompts#prompt). This class represent a prompt object, encapsulates the prompt data, variables, generation configuration, and other relevant information.\n",
    "\n",
    "Consider managing a social media page that features two-sentence stories with two main characters. The [Prompt class](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.preview.prompts#prompt) can define a reusable prompt template, enabling the generation of multiple stories with varied character pairings from a single structure. \n",
    "\n",
    "Let's construct the prompt object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6674d29f-154e-44fd-8c4d-ddb84d9463a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(prompt_data='Generate a story with 2 main characters: {A} and {B}.', system_instruction=You are a story writer. Write a short story in 2 sentences. Don't replace the words in the variables with their synnonyms.), model_name=projects/nghiale-demo-358818/locations/us-central1/publishers/google/models/gemini-2.0-flash-001), prompt_name=story-writer)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize vertexai\n",
    "vertexai.init(project=PROJECT, location=\"us-central1\")\n",
    "\n",
    "# Create local Prompt\n",
    "prompt = Prompt(\n",
    "    prompt_name=\"story-writer\",\n",
    "    prompt_data=\"Generate a story with 2 main characters: {A} and {B}.\",\n",
    "    model_name=MODEL,\n",
    "    system_instruction=\"You are a story writer. Write a short story in 2 sentences. Don't replace the words in the variables with their synnonyms.\",\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307ae9e-d1b7-496c-b4ae-cab4d6120158",
   "metadata": {},
   "source": [
    "After the creation of a Prompt object, the prompt data and properties representing various configurations can be used to generate content. Let's generate content for different variable sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7798223b-106c-424f-adef-197d268ad38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled prompt replacing: 1 instances of variable A, 1 instances of variable B\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"The cat, with a mischievous glint in its emerald eyes, plotted to swap the dog\\'s bone for a squeaky toy, but the dog, sensing the feline\\'s intentions, cleverly replaced the toy with a smelly sock.\\n\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  avg_logprobs: -0.4668608109156291\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 43\n",
      "  candidates_token_count: 48\n",
      "  total_token_count: 91\n",
      "  prompt_tokens_details {\n",
      "    modality: TEXT\n",
      "    token_count: 43\n",
      "  }\n",
      "  candidates_tokens_details {\n",
      "    modality: TEXT\n",
      "    token_count: 48\n",
      "  }\n",
      "}\n",
      "model_version: \"gemini-2.0-flash-001\"\n",
      "create_time {\n",
      "  seconds: 1745464148\n",
      "  nanos: 987746000\n",
      "}\n",
      "response_id: \"VKsJaOKkPLeTmecP6Nno-QM\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_1 = prompt.assemble_contents(A=\"cat\", B=\"dog\")\n",
    "\n",
    "response_1 = prompt.generate_content(contents=content_1)\n",
    "\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfe296a-fcfa-4321-8ad9-04cab9809595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled prompt replacing: 1 instances of variable A, 1 instances of variable B\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"King Alaric, bored with ruling his kingdom, wished for an escape, and a portal suddenly opened in his throne room, sucking him into detention with a bewildered high school student named Maya. Together, they had to navigate algebra tests and royal decrees to find a way back to their respective realities before either one of them failed senior year or lost a kingdom.\\n\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  avg_logprobs: -0.38257723384433323\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 47\n",
      "  candidates_token_count: 72\n",
      "  total_token_count: 119\n",
      "  prompt_tokens_details {\n",
      "    modality: TEXT\n",
      "    token_count: 47\n",
      "  }\n",
      "  candidates_tokens_details {\n",
      "    modality: TEXT\n",
      "    token_count: 72\n",
      "  }\n",
      "}\n",
      "model_version: \"gemini-2.0-flash-001\"\n",
      "create_time {\n",
      "  seconds: 1745464150\n",
      "  nanos: 347889000\n",
      "}\n",
      "response_id: \"VqsJaPGdFcuhmecP8vbe-Qc\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_2 = prompt.assemble_contents(\n",
    "    **{\"A\": \"a king\", \"B\": \"a high school student\"}\n",
    ")\n",
    "\n",
    "response_2 = prompt.generate_content(contents=content_2)\n",
    "\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb88194-2240-45b2-a28d-4a69495d770b",
   "metadata": {},
   "source": [
    "###Â Save, load and update a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4fe1f9-005c-48be-816e-a536e594038a",
   "metadata": {},
   "source": [
    "We can use the `vertexai.preview.prompts.create_version()` method to save a prompt online, making it accessible in the Google Cloud console. This method takes a Prompt object and creates a new version in the online store, returning an updated Prompt object linked to this resource. Remember that changes to a Prompt object are only saved online when `create_version()` is explicitly called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd31ffb-b708-47be-9c20-5c223d51a1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created prompt resource with id 8304504122208419840 with version number 1\n"
     ]
    }
   ],
   "source": [
    "prompt_v1 = prompts.create_version(prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb46078-b42f-43b0-975d-17a2ff20654d",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can go to Google Cloud Console to check your newly created prompt in [Prompt Management](https://console.cloud.google.com/vertex-ai/studio/saved-prompts). You can also retrieve a saved prompt from the online resource using the `vertexai.preview.prompts.get()` method. Simply provide the prompt's unique ID to this function, and it will return the associated Prompt object, as demonstrated in the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0cf98bc-6c93-418d-ba80-59cbd4e31915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(prompt_data='Generate a story with 2 main characters: {A} and {B}.', system_instruction=You are a story writer. Write a short story in 2 sentences. Don't replace the words in the variables with their synnonyms.), model_name=projects/nghiale-demo-358818/locations/us-central1/publishers/google/models/gemini-2.0-flash-001), prompt_id=8304504122208419840)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_prompt = prompts.get(prompt_id=prompt_v1.prompt_id)\n",
    "\n",
    "loaded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0808931-f454-45c6-8873-17d786b4a1eb",
   "metadata": {},
   "source": [
    "After retrieving a prompt using `get()`, you can modify its attributes and save these modifications as a new version. For instance, setting the new content to prompt_data updates the prompt locallyâthese changes are saved online only when create_version() is invoked. Because the prompt is associated with a prompt resource, `create_version()` generates a new version under the same prompt_id and returns a new `Prompt` object linked to the online resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c4fac2-19e3-4656-bd41-756a6b2a59db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated prompt resource with id 8304504122208419840 as version number 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prompt(prompt_data='Write a story with {A} as the protagonist and {B} as the antagonist.', system_instruction=You are a story writer. Write a short story in 2 sentences. Don't replace the words in the variables with their synnonyms.), model_name=projects/nghiale-demo-358818/locations/us-central1/publishers/google/models/gemini-2.0-flash-001), prompt_id=8304504122208419840, version_id=2, version_name=story-writer_2025_04_23_200923)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_prompt.prompt_data = (\n",
    "    \"Write a story with {A} as the protagonist and {B} as the antagonist.\"\n",
    ")\n",
    "\n",
    "prompt_v2 = prompts.create_version(prompt=loaded_prompt)\n",
    "\n",
    "prompt_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f30157-46fd-4a61-87bd-a0e80281604b",
   "metadata": {},
   "source": [
    "### List prompts and prompt versions\n",
    "\n",
    "To see the display names and prompt IDs of all prompts saved in the current Google Cloud project, use the `list_prompts()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d973d8-ecea-4f9f-be1a-c1253375b811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptMetadata(display_name='story-writer', prompt_id='8304504122208419840'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='2044500640163430400'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='4660938092037799936'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='3767817990934888448'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='1732472434340134912'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='3786113864421081088'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='893677003742380032'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='1425101759772098560'),\n",
       " PromptMetadata(display_name='story-writer', prompt_id='347615548923707392'),\n",
       " PromptMetadata(display_name='movie-critic', prompt_id='803745748683325440'),\n",
       " PromptMetadata(display_name='éè«', prompt_id='5764847766324903936'),\n",
       " PromptMetadata(display_name='untitled_1743733676692', prompt_id='6377337315647291392'),\n",
       " PromptMetadata(display_name='Car Customer Service', prompt_id='3468803604737949696'),\n",
       " PromptMetadata(display_name='Car Customer Service', prompt_id='7400446079432392704'),\n",
       " PromptMetadata(display_name='Test prompt', prompt_id='9042008143608938496'),\n",
       " PromptMetadata(display_name='untitled_1742363240770', prompt_id='6343226066907168768'),\n",
       " PromptMetadata(display_name='Video Upload', prompt_id='1008747492658905088'),\n",
       " PromptMetadata(display_name='Incomplete Prompt', prompt_id='7930780119927357440'),\n",
       " PromptMetadata(display_name='Google Pixel', prompt_id='7239477577125986304'),\n",
       " PromptMetadata(display_name='Barack Obama Biography', prompt_id='634948723587153920'),\n",
       " PromptMetadata(display_name='æ¥æ¬', prompt_id='6357546106347323392'),\n",
       " PromptMetadata(display_name='Information about Japan', prompt_id='8793993504754761728'),\n",
       " PromptMetadata(display_name='Name Correction', prompt_id='7607338983407747072'),\n",
       " PromptMetadata(display_name='Python to C# Code Conversion', prompt_id='7153038370996748288'),\n",
       " PromptMetadata(display_name='Python to C# Code Conversion', prompt_id='5254771128060084224'),\n",
       " PromptMetadata(display_name='ç»ååæ', prompt_id='7850533363285753856'),\n",
       " PromptMetadata(display_name='Information on Japan', prompt_id='1838227860746141696'),\n",
       " PromptMetadata(display_name='Dog and cat', prompt_id='7490245393096114176'),\n",
       " PromptMetadata(display_name='Popular National Parks in US and Canada', prompt_id='7743713609624059904'),\n",
       " PromptMetadata(display_name='Popular National Parks in US and Canada', prompt_id='7255073050054361088'),\n",
       " PromptMetadata(display_name='My Code Prompt', prompt_id='4631726267111047168'),\n",
       " PromptMetadata(display_name='Steve Jobs Biography and Summary', prompt_id='679817594093436928'),\n",
       " PromptMetadata(display_name='covertype', prompt_id='5167038896256581632'),\n",
       " PromptMetadata(display_name='Hawaii', prompt_id='2452760302672936960'),\n",
       " PromptMetadata(display_name='test', prompt_id='2509055298015068160'),\n",
       " PromptMetadata(display_name='departure board', prompt_id='747514525015605248'),\n",
       " PromptMetadata(display_name='untitled_1731649013286', prompt_id='3782940673863319552'),\n",
       " PromptMetadata(display_name='dictionary app', prompt_id='5350052606699896832'),\n",
       " PromptMetadata(display_name='untitled_1731293150297', prompt_id='5474077518313029632'),\n",
       " PromptMetadata(display_name='Hawaii', prompt_id='870038053501009920'),\n",
       " PromptMetadata(display_name='gs', prompt_id='4932443247063597056'),\n",
       " PromptMetadata(display_name='Hawaii', prompt_id='8350393889261092864'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='3735189433624821760'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='6944188886079766528'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='4636094077052387328'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='7554620149634957312'),\n",
       " PromptMetadata(display_name='Cloud Storage Bucket', prompt_id='5076021873465098240'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='428307058018746368'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='3935542442438164480'),\n",
       " PromptMetadata(display_name='Test', prompt_id='3496423886583496704'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='1613681747831029760'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='5804668229192253440'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='5179705819964375040'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='6278577531973009408'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='2675416355099901952'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='6317051642852147200'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='8229392634624344064'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='8055300361528803328'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='4178264032316227584'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='150920065540161536'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='7706834340361011200'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='8920730361797804032'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='1607025304436473856'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='478381016570920960'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='8092560611570876416'),\n",
       " PromptMetadata(display_name='Untitled prompt', prompt_id='9117692476751085568'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='470921929688088576'),\n",
       " PromptMetadata(display_name='ç¡é¡ã®ãã­ã³ãã', prompt_id='4511776695346266112'),\n",
       " PromptMetadata(display_name='untitled_1706855938297', prompt_id='5524039876434984960'),\n",
       " PromptMetadata(display_name='untitled_1706855898011', prompt_id='9125793678424539136'),\n",
       " PromptMetadata(display_name='untitled_1706854834311', prompt_id='6680620555739070464'),\n",
       " PromptMetadata(display_name='gs', prompt_id='3285064766376116224'),\n",
       " PromptMetadata(display_name='cloud storage', prompt_id='6964505661937811456'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='5775696097800355840'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='6627720852303511552'),\n",
       " PromptMetadata(display_name='cloud bucket', prompt_id='2982619903900516352'),\n",
       " PromptMetadata(display_name='gs', prompt_id='7954593892517543936'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='685221143988142080'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='5021096869611175936'),\n",
       " PromptMetadata(display_name='gs', prompt_id='5698044188600303616'),\n",
       " PromptMetadata(display_name='gs', prompt_id='2055195040010993664'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='8181497908118421504'),\n",
       " PromptMetadata(display_name='ãã¯ã¤', prompt_id='6725146378617487360'),\n",
       " PromptMetadata(display_name='jidousha', prompt_id='545785477400100864'),\n",
       " PromptMetadata(display_name='æ¥æ¬', prompt_id='4234233572216537088'),\n",
       " PromptMetadata(display_name='cloud storage', prompt_id='462455690154213376'),\n",
       " PromptMetadata(display_name='gs', prompt_id='647666224829825024'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='1676738739683983360'),\n",
       " PromptMetadata(display_name='æ¥æ¬', prompt_id='6791702016470024192'),\n",
       " PromptMetadata(display_name='compute engine', prompt_id='6760599031543496704'),\n",
       " PromptMetadata(display_name='bucket', prompt_id='7707269746965610496'),\n",
       " PromptMetadata(display_name='shellscript', prompt_id='8712416888799363072'),\n",
       " PromptMetadata(display_name='nihonjin', prompt_id='1449658802222661632'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='5266459486419156992'),\n",
       " PromptMetadata(display_name='cs-bucket', prompt_id='4037997134937587712'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='2469900039682523136'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='421888109135790080'),\n",
       " PromptMetadata(display_name='vertex ai gs', prompt_id='1336118833492000768'),\n",
       " PromptMetadata(display_name='Cloud storageã®prompt', prompt_id='571773534234214400'),\n",
       " PromptMetadata(display_name='storage bucket', prompt_id='6152155685011324928'),\n",
       " PromptMetadata(display_name='cloud storage prompt', prompt_id='4482023910698647552'),\n",
       " PromptMetadata(display_name='prompt1', prompt_id='3495172642351087616'),\n",
       " PromptMetadata(display_name='bucket', prompt_id='3449292221147250688'),\n",
       " PromptMetadata(display_name='bucket', prompt_id='1325844996842061824'),\n",
       " PromptMetadata(display_name='chat gs', prompt_id='5377395811615244288'),\n",
       " PromptMetadata(display_name='google cloud storage', prompt_id='3600514652385050624'),\n",
       " PromptMetadata(display_name='cloud storage', prompt_id='4137568907948982272'),\n",
       " PromptMetadata(display_name='cloud bucket', prompt_id='7067160465553489920'),\n",
       " PromptMetadata(display_name='Vertex AI chat', prompt_id='7722997161289318400'),\n",
       " PromptMetadata(display_name='gs', prompt_id='8023612436416299008'),\n",
       " PromptMetadata(display_name='gs', prompt_id='720321953193263104'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='2113482350422654976'),\n",
       " PromptMetadata(display_name='huntandfish', prompt_id='974441080604852224'),\n",
       " PromptMetadata(display_name='GH', prompt_id='6970139559518535680'),\n",
       " PromptMetadata(display_name='gs', prompt_id='2284707097192955904'),\n",
       " PromptMetadata(display_name='æ¥æ¬', prompt_id='6288407165925326848'),\n",
       " PromptMetadata(display_name='ggs ', prompt_id='1989862059088543744'),\n",
       " PromptMetadata(display_name='gs', prompt_id='9218139461018189824'),\n",
       " PromptMetadata(display_name='cgs', prompt_id='4583653969477238784'),\n",
       " PromptMetadata(display_name='gs', prompt_id='6653972791928291328'),\n",
       " PromptMetadata(display_name='kuruma', prompt_id='1506921367797235712'),\n",
       " PromptMetadata(display_name='Hawaii trip', prompt_id='3403218285896925184'),\n",
       " PromptMetadata(display_name='Hawaiichat', prompt_id='147678705261477888'),\n",
       " PromptMetadata(display_name='æ¥æ¬', prompt_id='5767185877801369600'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='2603407139573596160'),\n",
       " PromptMetadata(display_name='æè¡', prompt_id='8639532462017347584'),\n",
       " PromptMetadata(display_name='HuntandFish', prompt_id='6421791120514088960'),\n",
       " PromptMetadata(display_name='æ¥æ¬', prompt_id='6970104375146446848'),\n",
       " PromptMetadata(display_name='Hawaii', prompt_id='5016105086821072896'),\n",
       " PromptMetadata(display_name='ãã¯ã¤', prompt_id='8695686719871123456'),\n",
       " PromptMetadata(display_name='Grace Hopper important date', prompt_id='2553058303114477568'),\n",
       " PromptMetadata(display_name='Cloud storage', prompt_id='5998382386797084672'),\n",
       " PromptMetadata(display_name='Manga', prompt_id='3277223049446817792'),\n",
       " PromptMetadata(display_name='Doraemon', prompt_id='2149071342790508544'),\n",
       " PromptMetadata(display_name='gs', prompt_id='7875934830666448896'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='1277035476661829632'),\n",
       " PromptMetadata(display_name='Harry porter ', prompt_id='7098641682479972352'),\n",
       " PromptMetadata(display_name='bucket ', prompt_id='1859266465987821568'),\n",
       " PromptMetadata(display_name='Cloud Storage', prompt_id='7945388781169803264'),\n",
       " PromptMetadata(display_name='GH prompt', prompt_id='187692132419502080'),\n",
       " PromptMetadata(display_name='Grace Hopper test', prompt_id='4903400746927521792'),\n",
       " PromptMetadata(display_name='important dates', prompt_id='4727971467692605440'),\n",
       " PromptMetadata(display_name='GH', prompt_id='913879980146950144'),\n",
       " PromptMetadata(display_name='My wonderful prompt', prompt_id='3388045025433616384'),\n",
       " PromptMetadata(display_name='Grace Hopper', prompt_id='3238511444056080384'),\n",
       " PromptMetadata(display_name='Google prompt', prompt_id='6768204353472823296'),\n",
       " PromptMetadata(display_name='untitled_1666247981364', prompt_id='225129403833647104'),\n",
       " PromptMetadata(display_name='untitled_1666184991057', prompt_id='8642357107389104128')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_metadata = prompts.list()\n",
    "\n",
    "prompts_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc08941-827f-4afa-abc6-c4f3204d2c89",
   "metadata": {},
   "source": [
    "After checking the prompt list, you can specify the index to retriave a specific prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "231b1350-c92f-4ff4-8fa8-df8f23a38f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt(prompt_data='Write a story with {A} as the protagonist and {B} as the antagonist.', system_instruction=You are a story writer. Write a short story in 2 sentences. Don't replace the words in the variables with their synnonyms.), model_name=projects/nghiale-demo-358818/locations/us-central1/publishers/google/models/gemini-2.0-flash-001), prompt_id=8304504122208419840)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_prompt = prompts.get(prompt_id=prompts_metadata[0].prompt_id)\n",
    "\n",
    "retrieved_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27776abf-a694-458b-b4cb-4b48dd88d2d6",
   "metadata": {},
   "source": [
    "To see the display names and version IDs of all prompt versions saved within the prompt, use the `list_versions()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139d1022-26b3-44eb-af7e-0897c34b91dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptVersionMetadata(display_name='story-writer_2025_04_23_200914', prompt_id='8304504122208419840', version_id='1'),\n",
       " PromptVersionMetadata(display_name='story-writer_2025_04_23_200923', prompt_id='8304504122208419840', version_id='2')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_versions_metadata = prompts.list_versions(prompt_id=prompt_v1.prompt_id)\n",
    "\n",
    "prompt_versions_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268db51-97e7-4826-b141-652d96c53e7a",
   "metadata": {},
   "source": [
    "### Restore a prompt version\n",
    "\n",
    "Prompt resources keep a history of saved versions. To revert to a previous version, use the `restore_version()` method, which makes that older version the latest one. This method returns metadata you can use with `get()` to retrieve the newly restored version.\n",
    "\n",
    "For instance, the following code restores the prompt content to version id 1, the original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b87721-ffa4-4e55-8325-1be5fd7eeec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored prompt version 1 under prompt id 8304504122208419840 as version number 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prompt(prompt_data='Generate a story with 2 main characters: {A} and {B}.', system_instruction=You are a story writer. Write a short story in 2 sentences. Don't replace the words in the variables with their synnonyms.), model_name=projects/nghiale-demo-358818/locations/us-central1/publishers/google/models/gemini-2.0-flash-001), prompt_id=8304504122208419840)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_version_metadata = prompts.restore_version(\n",
    "    prompt_id=prompt_v1.prompt_id, version_id=\"1\"\n",
    ")\n",
    "\n",
    "# Fetch the newly restored latest version of the prompt\n",
    "restored_prompt = prompts.get(prompt_id=prompt_version_metadata.prompt_id)\n",
    "\n",
    "restored_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ea7a9-b17c-457c-ba41-716227bdd475",
   "metadata": {},
   "source": [
    "### Delete a prompt\n",
    "\n",
    "To delete the online resource associated with a prompt ID, use the `delete()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c1713b-5b9d-4cde-ba9e-a5070c9c22c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted prompt resource with id 8304504122208419840.\n"
     ]
    }
   ],
   "source": [
    "prompts.delete(prompt_id=prompt_v1.prompt_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a329286-4be9-45cc-b3e5-c0e9d1496fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Context caching\n",
    "\n",
    "The second section of this notebook demonstrates how to use context caching with Gemini models in Vertex AI. \n",
    "\n",
    "Context caching allows you to store the processed content, such as research papers, long videos or audios along with system instructions, so you don't have to re-process it every time. <br>\n",
    "When you query the model, it can leverage the stored context, leading to faster response times and reduced resource consumption. This is particularly useful when working with large documents or when using the same context across multiple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6270e-9506-4c43-bf52-d557431f842f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d81b19-c67c-40cd-9ddb-ebbd71547fb1",
   "metadata": {},
   "source": [
    "Here we define the contents variable as a list of `Part` objects, each containing a reference to a research paper in PDF format stored in Google Cloud Storage.<br>\n",
    "These are the papers that will be used for context caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "473eb6f0-0ebf-419b-b638-5132faccd2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert researcher. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "Now look at these research papers, and answer the following questions.\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    types.Part.from_uri(\n",
    "        file_uri=\"gs://asl-public/data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    types.Part.from_uri(\n",
    "        file_uri=\"gs://asl-public/data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79a0b6-d13c-44ad-968b-c2dd41cf614c",
   "metadata": {},
   "source": [
    "### Create context caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe710d-c4fe-4700-81cf-0229dac3e7fa",
   "metadata": {},
   "source": [
    "Let's create the cached content. It uses `client.caches.create` to set up a cache with specified parameters. The parameters are:\n",
    "\n",
    "*   `model`: Specifies the Gemini model to use (\"gemini-2.0-flash-001\" in this case).\n",
    "*   `config`: Basic configuration, which includes:\n",
    "    *   `system_instruction`: Sets the instructions for how the model should behave.\n",
    "    *   `contents`: The actual documents or other data you want to store in the cache.\n",
    "    *   `ttl`: The time-to-live of the cache (60 minutes in this case), after which the cache will expire.\n",
    "    *   `display_name`: A name for easy identification.\n",
    "\n",
    "The output of this cell is the unique identifier `cached_content.name` that is used to retrieve cached content later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b96b7c7-652b-490c-96cb-a94ff9538ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/684496754124/locations/us-central1/cachedContents/5362817386942562304\n"
     ]
    }
   ],
   "source": [
    "# cached_content = caching.CachedContent.create(\n",
    "cached_content = client.caches.create(\n",
    "    model=MODEL,\n",
    "    config=types.CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=contents,\n",
    "        ttl=\"3600s\",\n",
    "        display_name=\"example-cache\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27cd75-ada2-483a-9b18-2bbed9d57da5",
   "metadata": {},
   "source": [
    "Let's take a look at the created context cache!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72af9180-0442-4efb-a5f4-9a3beccbf9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='projects/684496754124/locations/us-central1/cachedContents/5362817386942562304' display_name='example-cache' model='projects/nghiale-demo-358818/locations/us-central1/publishers/google/models/gemini-2.0-flash-001' create_time=datetime.datetime(2025, 4, 24, 3, 9, 52, 640307, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 24, 3, 9, 52, 640307, tzinfo=TzInfo(UTC)) expire_time=datetime.datetime(2025, 4, 24, 4, 9, 52, 634948, tzinfo=TzInfo(UTC)) usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=167, text_count=153, total_token_count=43127, video_duration_seconds=None)\n",
      "name='projects/684496754124/locations/us-central1/cachedContents/505685188823482368' display_name='example-cache' model='projects/nghiale-demo-358818/locations/us-central1/publishers/google/models/gemini-2.0-flash-001' create_time=datetime.datetime(2025, 4, 24, 3, 3, 56, 907339, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 24, 3, 3, 56, 907339, tzinfo=TzInfo(UTC)) expire_time=datetime.datetime(2025, 4, 24, 4, 3, 56, 897688, tzinfo=TzInfo(UTC)) usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=167, text_count=153, total_token_count=43127, video_duration_seconds=None)\n"
     ]
    }
   ],
   "source": [
    "# caching.CachedContent.list()\n",
    "for cache in client.caches.list():\n",
    "    print(cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf99fa3-6123-4564-9f46-d9f8485e26eb",
   "metadata": {},
   "source": [
    "### Generate without cached context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeae236-de34-4459-939c-50733a9a3201",
   "metadata": {},
   "source": [
    "For comparison, let's first generate the answer **without** cached content and note the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0c8be69-cdbe-4003-abcf-ff3c6d5226da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The papers \"Gemini: A Family of Highly Capable Multimodal Models\" and \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\" are both about Google's Gemini family of language models. Here's a breakdown of what each paper covers:\n",
      "\n",
      "**1. \"Gemini: A Family of Highly Capable Multimodal Models\"**\n",
      "\n",
      "*   **Introduction of the Gemini Family:** This paper introduces the Gemini model family, emphasizing its multimodal capabilities (understanding and generating across image, audio, video, and text).\n",
      "*   **Model Sizes:** It describes three sizes of Gemini: Ultra (for complex tasks), Pro (for enhanced performance and scalability), and Nano (for on-device applications).\n",
      "*   **Performance:** It presents benchmarks showing state-of-the-art results on a wide range of language understanding, reasoning, coding, and multimodal tasks. One key highlight is Gemini Ultra being the first model to achieve human-expert performance on the MMLU benchmark.\n",
      "*   **Capabilities Showcase:** It showcases Gemini's impressive capabilities in cross-modal reasoning, like understanding messy handwritten solutions to physics problems and providing accurate feedback.\n",
      "*   **Responsible Deployment:** Discusses approaches to responsible deployment, including model policies and mitigations to reduce harm.\n",
      "\n",
      "**2. \"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\"**\n",
      "\n",
      "*   **Focus on Gemini 1.5 Pro:** It focuses on Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model.\n",
      "*   **Emphasis on Long Context:** The key focus is the model's ability to understand and reason over *millions* of tokens of context, including long documents, hours of video, and audio. It presents results showing near-perfect recall on long-context tasks and state-of-the-art long-document QA performance.\n",
      "*   **Surprising Capabilities:**Highlights the ability of Gemini 1.5 Pro to learn a new language (Kalamang) from a grammar manual provided in its input context.\n",
      "*   **Improvements Over Previous Gemini Models:** It emphasizes improvements in predictive performance, near-perfect retrieval, and new capabilities due to the increased context length.\n",
      "*   **Limited information on the full range of capabilities:**  A key caveat is that 1.5's long context abilities mean that much of the full range of capabilities of the new version has not been completely unlocked.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "*   The first paper (\"Gemini\") introduces the overall Gemini family and emphasizes multimodality.\n",
      "*   The second paper (\"Gemini 1.5\") dives deep into one specific model (Gemini 1.5 Pro) with an unprecedented long context window and its exciting implications and capabilities.\n",
      "CPU times: user 20.2 ms, sys: 9.69 ms, total: 29.9 ms\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL, contents=contents + [\"What are the papers about?\"]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86991481-6c6a-49c3-bb6f-3660c49f5f78",
   "metadata": {},
   "source": [
    "### Generate with cached context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6ffa7-3e79-43f4-a929-9faf0fec77d4",
   "metadata": {},
   "source": [
    "Now let's use the cached content to generate answers. The `cached_content` parameters refers to the created cached content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1603d54-50be-4d44-bf26-9eabb500e12e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The papers are about Gemini, which is a new family of multimodal models developed by Google. Gemini exhibits capabilities across image, audio, video, and text understanding. It advances the state-of-the-art in large-scale language modeling, image understanding, audio processing, and video understanding. It also has capabilities in areas such as coding and reasoning.\n",
      "\n",
      "One paper specifically discusses Gemini 1.5 Pro, the latest model of the Gemini family, and how it unlocks multimodal understanding across millions of tokens of context. This model is capable of recalling and reasoning over fine-grained information from millions of tokens of context and achieves near-perfect recall on long-context retrieval tasks across modalities.\n",
      "CPU times: user 17.7 ms, sys: 1.19 ms, total: 18.9 ms\n",
      "Wall time: 9.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# response = cached_model.generate_content(\"What are the papers about?\")\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=\"What are the papers about?\",\n",
    "    config=types.GenerateContentConfig(cached_content=cached_content.name),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35205f25-30d4-4015-86c0-47ea55816801",
   "metadata": {},
   "source": [
    "The output clearly demonstrates a substantial decrease in processing time. \n",
    "\n",
    "In conclusion, context caching markedly accelerates processing, with this performance gain amplifying as the volume of contextual information increases. By storing and reusing processed context, we achieve significant gains in efficiency, especially with larger contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7362ad8-8d9c-482a-bf56-482a7824ba22",
   "metadata": {},
   "source": [
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
