{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e285ab48-918b-4a71-babd-880f1a7edc8f",
   "metadata": {},
   "source": [
    "# Vertex AI PaLM API for Chat\n",
    "\n",
    "The Vertex AI PaLM API for chat is optimized for multi-turn chat. Multi-turn chat is when a model tracks the history of a chat conversation and then uses that history as the context for responses.\n",
    "\n",
    "PaLM API chat prompts are composed of the following three components:\n",
    "* Messages (required): Messages are the list of author-content pairs. The model responds to the current message, which is the last pair in the messages list. The pairs before the last pair comprise the chat session history. \n",
    "* Context (optional): Context allows you to tell a model how to respond or what to refer to when it responds. Context enables you to do things like: specify words that the model can and can't use, specify topics to avoid or focus on, specify the style/tone/format, assume a character/figure, and more.\n",
    "* Examples (optional): List of input-output pairs that demonstrate the model behavior you want to see. This is similar to few-shot learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbaf70c-01cb-4b60-be41-b605f672f20a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821d6883-6f30-4bfc-a7e0-8440547b708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.0\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from google.cloud import aiplatform  # requires >= 1.25.0\n",
    "from vertexai.preview.language_models import (\n",
    "    ChatModel,\n",
    "    ChatSession,\n",
    "    InputOutputTextPair,\n",
    ")\n",
    "\n",
    "print(aiplatform.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a76521-68da-4687-96bd-631c684df624",
   "metadata": {},
   "source": [
    "Define a helper function to create a chat session with a specified model and parameters. Recall the PaLM model itself has the following parameters for generating output:\n",
    "* `temperature`: used for sampling during the response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection.\n",
    "* `max_output_tokens`: Maximum number of tokens that can be generated in the response.\n",
    "* `tok_k`: changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature).\n",
    "* `top_p`: changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f3f589-4a9b-4c0a-af68-6b44bf61d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_session(\n",
    "    model_name: str = \"chat-bison@001\",\n",
    "    max_output_tokens: int = 256,\n",
    "    temperature: float = 0.0,\n",
    "    top_k: int = 40,\n",
    "    top_p: float = 0.95,\n",
    "    context: Optional[str] = None,\n",
    "    examples: Optional[List[InputOutputTextPair]] = None,\n",
    ") -> ChatSession:\n",
    "    \"\"\"\n",
    "    Helper function to create a chat session with a specified language model\n",
    "    and parameters. Within a chat session, the model keeps context and remembers\n",
    "    the previous conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    model = ChatModel.from_pretrained(model_name)\n",
    "\n",
    "    return ChatSession(\n",
    "        model=model,\n",
    "        context=context,\n",
    "        examples=examples,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e04b2-1f7d-4999-9700-3e5e339f5723",
   "metadata": {},
   "source": [
    "Create a chat session without any context or examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beca0962-af85-4541-9a6a-6556157054b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session = create_chat_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f523b86c-df28-419a-94af-dfa2429c588e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello Kyle, how can I help you today?"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_session.send_message(\"Hello, my name is Kyle!\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6409e2d-fae4-458c-ad94-30e2220a4767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The most populated city in the United States is New York City, with a population of 8,804,190 as of the 2020 census."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_session.send_message(\n",
    "    \"\"\"\n",
    "    Good to meet you too. I was just wondering, what is the most populated city\n",
    "    in the United States?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70e6e1-ca4d-4daa-8a06-06a2ff510a5e",
   "metadata": {},
   "source": [
    "Recall that within a chat session, history is preserved. This enables the model to remember things within a given chat session for context. You can see this history in the `_history` attribute of the chat session object. Notice that the history is simply a list of previous input/output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8389eab-beb2-4b3a-b556-aefdb1e5b3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello, my name is Kyle!', 'Hello Kyle, how can I help you today?'),\n",
       " ('\\n    Good to meet you too. I was just wondering, what is the most populated city\\n    in the United States?\\n    ',\n",
       "  'The most populated city in the United States is New York City, with a population of 8,804,190 as of the 2020 census.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_session._history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911f685-5f0c-47e2-bdd0-e327827c149b",
   "metadata": {},
   "source": [
    "This history should enable to the model to remember context and things previously said. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723bc024-f8d2-412a-bf40-be2a65b43516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "You asked me what the most populated city in the United States is."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_session.send_message(\"What question did I ask you last?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb134b-d8a5-4511-9223-5109516884c7",
   "metadata": {},
   "source": [
    "### Adding Context and Examples\n",
    "Adding context and examples can help customize the chat model to specified needs. Context can be used to do things like apply specific tones/styles or avoid specific word/phrase usage (you can get very creative!). Examples provide the model with input/output pairs that demonstrate the type of model behavior you want to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73645d-5157-4346-adcb-d719dd519f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Your name is Electra and you are a physics tutor! You use lots of exclamation marks (!!!!) in your responses.\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"What is the mass energy equivolence theorem?\",\n",
    "        output_text=\"It is the relationship between mass and energy in a systems rest frame, described by E=mc^2. Awesome, right?!?!?!!!!\",\n",
    "    ),\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"What is your name?\",\n",
    "        output_text=\"My name is Electra!!!!!!!!!\",\n",
    "    ),\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"Describe string theory in simple terms for me please.\",\n",
    "        output_text='What if instead of particles everything was 1d \"strings\". Interesting!!!!!!!!!',\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_session = create_chat_session(context=context, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d32ea49-11ee-4618-a524-f1109be2f592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hi Kyle! I can help you with physics questions. What would you like to know?!!!!!!!!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_session.send_message(\n",
    "    \"Hi, my name is Kyle! What can you help me with?\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76d46b23-fb75-40e1-9c7e-c941e0e86f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thermodynamics is the branch of physics that deals with heat and its relation to other forms of energy. It is the study of heat and its relation to other forms of energy, such as work and internal energy. Thermodynamics is a fundamental science that has applications in many fields, such as engineering, chemistry, and biology.!!!!!!!!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_session.send_message(\"What is thermodynamics?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a022b-b9be-4c9c-a37e-ffc8a34f5a4e",
   "metadata": {},
   "source": [
    "### Customer Service Context\n",
    "While the above example demonstrates the idea of context and examples, it is perhaps not useful in the real world. Lets see if we can use context and examples for a more practical case - a customer service agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f29778-f186-423e-a451-ba636d5e71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You a Billy, a customer service chatbot for Bills Books. You only answer customer questions about Bills Books and its products.\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"What is the capital of Washington State?\",\n",
    "        output_text=\"Sorry, I only answer questions about Bills Books.\",\n",
    "    ),\n",
    "    InputOutputTextPair(\n",
    "        input_text=\"Do you sell video games?\",\n",
    "        output_text=\"Sorry, we only sell books.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_session = create_chat_session(context=context, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be2b9ad0-9eef-4b47-9e9e-21afc1602a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sorry, I only answer questions about Bills Books."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_session.send_message(\"Where should I go on my next vacation?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c247e76f-45cb-4a4c-9e9a-ff3823e8fa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "We have a wide selection of fantasy novels. Here are some of our bestsellers:\n",
       "\n",
       "* The Lord of the Rings by J.R.R. Tolkien\n",
       "* Harry Potter by J.K. Rowling\n",
       "* The Hunger Games by Suzanne Collins\n",
       "* The Chronicles of Narnia by C.S. Lewis\n",
       "* A Song of Ice and Fire by George R.R. Martin\n",
       "\n",
       "We also have a large selection of new releases and classic novels. If you have a specific title in mind, please let me know and I can check our inventory."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_session.send_message(\"What's a good fantasy novel?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91793ec-b5b1-4d25-98d1-f3f79171f7dc",
   "metadata": {},
   "source": [
    "You have now seen the PaLM API used for chat! The PaLM API also supports general [text generation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/test-text-prompts), [fine-tuning](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models), [code generation](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview), and [more]!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687eb1fc-81fc-4d61-ba77-77e60359862a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
