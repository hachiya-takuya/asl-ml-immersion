{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02aa307c-a2b7-4927-98f5-51b27b908ba0",
   "metadata": {},
   "source": [
    "# Vertex AI PaLM API Introduction\n",
    "The Vertex AI PaLM API lets you test, customize, and deploy instances of Google's PaLM large language models (LLM) so that you can leverage the capabilities of PaLM in your applications. The PaLM family of models supports text completion, multi-turn chat, and text embeddings generation.\n",
    "\n",
    "This notebook will provide examples of accessing pre-trained PaLM models with the API for use cases like text classification, summarization, and extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30622e-7ae5-4092-bebf-80ecd3b874f6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ee081e-f7d1-44a8-8bee-57a3b6fbd750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.0\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform  # requires >=1.25.0\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "print(aiplatform.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da340d5-828a-48cf-a030-e0596a918050",
   "metadata": {},
   "source": [
    "### Input Parameters\n",
    "Below we specify a helper function to generate responses from the PaLM API. The input parameters are as follows:\n",
    "* `prompt`: Text input to generate model response. Prompts can include preamble, questions, suggestions, instructions, or examples.\n",
    "* `temperature`: The temperature is used for sampling during the response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic: the highest probability response is always selected. For most use cases, try starting with a temperature of 0.2.\n",
    "* `max_output_tokens`: Maximum number of tokens that can be generated in the response. Specify a lower value for shorter responses and a higher value for longer responses. A token may be smaller than a word. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n",
    "* `top_k`: Top-k changes how the model selects tokens for output. A top-k of 1 means the selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-k of 3 means that the next token is selected from among the 3 most probable tokens (using temperature). For each token selection step, the top K tokens with the highest probabilities are sampled. Then tokens are further filtered based on topP with the final token selected using temperature sampling. Specify a lower value for less random responses and a higher value for more random responses.\n",
    "* `top_p`: Top-p changes how the model selects tokens for output. Tokens are selected from most K (see topK parameter) probable to least until the sum of their probabilities equals the top-p value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5, then the model will select either A or B as the next token (using temperature) and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random responses and a higher value for more random responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ea6954a-a9ca-41dd-aa61-e6538e514f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt: str,\n",
    "    model_name: str = \"text-bison@001\",\n",
    "    temperature: float = 0.2,\n",
    "    max_output_tokens: int = 256,\n",
    "    top_p: float = 0.8,\n",
    "    top_k: int = 40,\n",
    "):\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1819d41-cb98-405d-aa8f-ff6244f5e805",
   "metadata": {},
   "source": [
    "Test out the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c96cff7e-8203-4ec8-8dbd-82dd14e3b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1. **Large language models are trained on massive datasets of text and code.** This gives them a vast knowledge base that they can draw on to generate text, translate languages, write different kinds of creative content, answer your questions, and more.\n",
       "2. **Large language models are still under development, and they have some limitations.** For example, they can sometimes be biased or inaccurate, and they may not always understand the nuances of human language.\n",
       "3. **Large language models are being used in a variety of applications,** such as customer service, content creation, and medical research. As they continue to develop, we can expect to see even more uses for these powerful tools.\n",
       "4. **The development of large language models raises some important ethical and societal issues.** For example, it's important to consider how these models can be used to amplify misinformation and hate speech.\n",
       "5. **The future of large language models is uncertain, but it's clear that they have the potential to revolutionize many aspects of our lives.** It will be important to continue to study and develop these models in a responsible way so that we can reap the benefits of their potential while mitigating their risks."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    \"What are five important things to understand about large language models?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e32f10-3419-4f9b-9be2-334b4e1b0f01",
   "metadata": {},
   "source": [
    "### Text Classification \n",
    "Now that we've tested our wrapper function, explore prompting for classification. Text classification is a common ML use-case and is frequently used for tasks like spam detection, sentiment analysis, topic classification, and more. \n",
    "\n",
    "Both zero-shot and few-shot prompting are common with text classification use cases. Zero-shot prompting is where you do not provide examples with labels in the input, and few-shot prompting is where you provide (a few) examples in the input. \n",
    "\n",
    "Additionally, for text classification use cases we can reduce max_output_tokens if we simply want the predicted class (because labels are typically a few words or less)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095984db-e3f1-431c-8464-6ed9c745b380",
   "metadata": {},
   "source": [
    "Zero-shot classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc027f3b-84ec-46ab-95b1-5d6afd6d1156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The text is about technology"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Classify the following: \n",
    "text: \"Zero-shot prompting is really easy with Google's PaLM API.\"\n",
    "label: technology, politics, sports \n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d73f1-11ce-4581-90da-d69e76b57247",
   "metadata": {},
   "source": [
    "Few-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4d9c371-dc6c-4ef7-98ad-dc8058b88bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cats"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the topic for a given text? \n",
    "- cats \n",
    "- dogs \n",
    "\n",
    "Text: They always sits on my lap and purr!\n",
    "The answer is: cats\n",
    "\n",
    "Text: They love to play fetch\n",
    "The answer is: dogs\n",
    "\n",
    "Text: I throw the frisbee in the water and they swim for hours! \n",
    "The answer is: dogs \n",
    "\n",
    "Text: They always knock things off my counter!\n",
    "The answer is:\n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff60e3-2e6d-4d70-988b-4940adb67c13",
   "metadata": {},
   "source": [
    "### Text Summarization \n",
    "PaLM can also be used for text summarization use cases. Text summarization produces a concise and fluent summary of a longer text document. There are two main text summarization types: extractive and abstractive. Extractive summarization involves selecting critical sentences from the original text and combining them to form a summary. Abstractive summarization involves generating new sentences representing the original text's main points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04163aa1-b750-4033-93b3-c08e572872c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A transformer is a deep learning model that processes sequential input data such as natural language. Unlike RNNs, transformers process the entire input at once, which allows for more parallelization and reduces training times."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a very short summary for the following:\n",
    "\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c16a-67cc-4bdb-ae66-4882d61279de",
   "metadata": {},
   "source": [
    "Slightly change the input prompt to see the impact it can have on the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "029d03ec-ae6c-4d47-8d2e-09ebec182d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "- A transformer is a deep learning model that is distinguished by its adoption of self-attention.\n",
       "- Transformers are designed to process sequential input data, such as natural language.\n",
       "- Unlike RNNs, transformers process the entire input all at once.\n",
       "- The attention mechanism provides context for any position in the input sequence."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide four bullet points summarizing the following:\n",
    "\n",
    "A transformer is a deep learning model. It is distinguished by its adoption of self-attention, \n",
    "differentially weighting the significance of each part of the input (which includes the \n",
    "recursive output) data. Like recurrent neural networks (RNNs), transformers are designed to \n",
    "process sequential input data, such as natural language, with applications towards tasks such \n",
    "as translation and text summarization. However, unlike RNNs, transformers process the \n",
    "entire input all at once. The attention mechanism provides context for any position in the \n",
    "input sequence. For example, if the input data is a natural language sentence, the transformer \n",
    "does not have to process one word at a time. This allows for more parallelization than RNNs \n",
    "and therefore reduces training times.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\"\"\"\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df5150-b6ff-499e-adad-6ae6d5cc7b0c",
   "metadata": {},
   "source": [
    "Dialog summarization falls under the category of text summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f594633f-ee1b-4a18-a11c-429b80e512e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kyle is having issues using the PaLM API and keeps getting an error. The error says: \"ResourceExhausted: 429 Quota exceeded for \n",
       "      aiplatform.googleapis.com/online_prediction_requests_per_base_model \n",
       "      with base model: text-bison\"\n",
       "The service rep informs Kyle that he has exceeded the quota for usage and provides a link to information about quotas and limits. Kyle asks if the service rep can increase his quota, but the service rep says that he cannot but will follow up with somebody who will be able to help.\n",
       "To-do's for the service rep:\n",
       "- Follow up with somebody who can increase Kyle's quota.\n",
       "- Provide Kyle with more information about quotas and limits."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a summary of the following converstaion and at the end, summarize to-do's for the service rep: \n",
    "\n",
    "Kyle: Hi! I'm reaching out to customer service because I am having issues.\n",
    "\n",
    "Service Rep: What seems to be the problem? \n",
    "\n",
    "Kyle: I am trying to use the PaLM API but I keep getting an error. \n",
    "\n",
    "Service Rep: Can you share the error with me? \n",
    "\n",
    "Kyle: Sure. The error says: \"ResourceExhausted: 429 Quota exceeded for \n",
    "      aiplatform.googleapis.com/online_prediction_requests_per_base_model \n",
    "      with base model: text-bison\"\n",
    "      \n",
    "Service Rep: It looks like you have exceeded the quota for usage. Please refer to \n",
    "             https://cloud.google.com/vertex-ai/docs/quotas for information about quotas\n",
    "             and limits. \n",
    "             \n",
    "Kyle: Can you increase my quota?\n",
    "\n",
    "Service Rep: I cannot, but let me follow up with somebody who will be able to help.\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd094d3-3026-4ff3-9306-357c50bc921d",
   "metadata": {},
   "source": [
    "### Text Extraction \n",
    "PaLM can be used to extract and structure text. Text extraction can be used for a variety of purposes. One common purpose is to convert documents into a machine-readable format. This can be useful for storing documents in a database or for processing documents with software. Another common purpose is to extract information from documents. This can be useful for finding specific information in a document or for summarizing the content of a document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003d56b-67cc-4dc5-9a29-288b5043af71",
   "metadata": {},
   "source": [
    "Zero-shot extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "776b19b1-79cb-400a-87c7-6b13fe756c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "```\n",
       "{\n",
       "  \"ingredient\": \"olive oil\",\n",
       "  \"quantity\": \"1 tablespoon\",\n",
       "  \"type\": \"oil\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"onion\",\n",
       "  \"quantity\": \"1\",\n",
       "  \"type\": \"vegetable\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"carrot\",\n",
       "  \"quantity\": \"2\",\n",
       "  \"type\": \"vegetable\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"celery\",\n",
       "  \"quantity\": \"2\",\n",
       "  \"type\": \"vegetable\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"ground cumin\",\n",
       "  \"quantity\": \"1 teaspoon\",\n",
       "  \"type\": \"spice\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"ground coriander\",\n",
       "  \"quantity\": \"1/2 teaspoon\",\n",
       "  \"type\": \"spice\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"turmeric powder\",\n",
       "  \"quantity\": \"1/4 teaspoon\",\n",
       "  \"type\": \"spice\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"cayenne pepper\",\n",
       "  \"quantity\": \"1/4 teaspoon\",\n",
       "  \"type\": \"spice\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"salt\",\n",
       "  \"quantity\": \"to taste\",\n",
       "  \"type\": \"seasoning\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"pepper\",\n",
       "  \"quantity\": \"to taste\",\n",
       "  \"type\": \"seasoning\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"black beans\",\n",
       "  \"quantity\": \"1 (15 ounce) can\",\n",
       "  \"type\": \"bean\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"kidney beans\",\n",
       "  \"quantity\": \"1 (15 ounce) can\",\n",
       "  \"type\": \"bean\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"diced tomatoes\",\n",
       "  \"quantity\": \"1 (14.5 ounce) can\",\n",
       "  \"type\": \"vegetable\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"diced tomatoes with green chilies\",\n",
       "  \"quantity\": \"1 (10 ounce) can\",\n",
       "  \"type\": \"vegetable\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"vegetable broth\",\n",
       "  \"quantity\": \"4 cups\",\n",
       "  \"type\": \"liquid\"\n",
       "},\n",
       "{\n",
       "  \"ingredient\": \"chopped fresh cilantro\",\n",
       "  \"quantity\": \"1 cup\",\n",
       "  \"type\": \"herb\"\n",
       "}\n",
       "```"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the ingredients from the following recipe. \n",
    "Return the ingredients in JSON format with keys: ingredient, quantity, type.\n",
    "\n",
    "Ingredients:\n",
    "* 1 tablespoon olive oil\n",
    "* 1 onion, chopped\n",
    "* 2 carrots, chopped\n",
    "* 2 celery stalks, chopped\n",
    "* 1 teaspoon ground cumin\n",
    "* 1/2 teaspoon ground coriander\n",
    "* 1/4 teaspoon turmeric powder\n",
    "* 1/4 teaspoon cayenne pepper (optional)\n",
    "* Salt and pepper to taste\n",
    "* 1 (15 ounce) can black beans, rinsed and drained\n",
    "* 1 (15 ounce) can kidney beans, rinsed and drained\n",
    "* 1 (14.5 ounce) can diced tomatoes, undrained\n",
    "* 1 (10 ounce) can diced tomatoes with green chilies, undrained\n",
    "* 4 cups vegetable broth\n",
    "* 1 cup chopped fresh cilantro\n",
    "\"\"\"\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669e27e-3bfe-4c95-8c11-dd424cc8a735",
   "metadata": {},
   "source": [
    "Few (single) shot extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78fc370e-b071-4377-b9c0-10e53c76c62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"product\":\"Google Pixel 7\",\n",
       "  \"network\":\"5G\",\n",
       "  \"RAM\":\"8GB\",\n",
       "  \"processor\":\"Tensor G2\",\n",
       "  \"storage\":\"128GB\",\n",
       "  \"color\":\"Lemongrass\"\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the technical specifications from the text below in JSON format.\n",
    "\n",
    "Text: Google Nest WiFi, network speed up to 1200Mpbs, 2.4GHz and 5GHz frequencies, WP3 protocol\n",
    "JSON: {\n",
    "  \"product\":\"Google Nest WiFi\",\n",
    "  \"speed\":\"1200Mpbs\",\n",
    "  \"frequencies\": [\"2.4GHz\", \"5GHz\"],\n",
    "  \"protocol\":\"WP3\"\n",
    "}\n",
    "\n",
    "Text: Google Pixel 7, 5G network, 8GB RAM, Tensor G2 processor, 128GB of storage, Lemongrass\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "generate(prompt, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec39913-16cf-4590-aa38-a4617f8985e0",
   "metadata": {},
   "source": [
    "You have now seen the PaLM API used for text classification, summarization, and extraction. The PaLM API also supports [fine-tuning](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models), [chatbots](https://cloud.google.com/vertex-ai/docs/generative-ai/chat/chat-prompts), [code generation](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-chat-prompts), and [more](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d486978-bcd5-4676-8a73-74611d5795dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
